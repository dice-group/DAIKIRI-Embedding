:py:mod:`dicee.dataset_classes`
===============================

.. py:module:: dicee.dataset_classes


Module Contents
---------------

Classes
~~~~~~~

.. autoapisummary::

   dicee.dataset_classes.BPE_NegativeSamplingDataset
   dicee.dataset_classes.MultiLabelDataset
   dicee.dataset_classes.MultiClassClassificationDataset
   dicee.dataset_classes.OnevsAllDataset
   dicee.dataset_classes.KvsAll
   dicee.dataset_classes.AllvsAll
   dicee.dataset_classes.KvsSampleDataset
   dicee.dataset_classes.NegSampleDataset
   dicee.dataset_classes.TriplePredictionDataset
   dicee.dataset_classes.CVDataModule



Functions
~~~~~~~~~

.. autoapisummary::

   dicee.dataset_classes.reload_dataset
   dicee.dataset_classes.construct_dataset



.. py:function:: reload_dataset(path: str, form_of_labelling: str, scoring_technique: str, neg_ratio: float, label_smoothing_rate: float) -> torch.utils.data.Dataset

   Reloads the dataset from disk and constructs a PyTorch dataset for training.

   :param path: The path to the directory where the dataset is stored.
   :type path: str
   :param form_of_labelling: The form of labelling used in the dataset. Determines how data points are represented.
   :type form_of_labelling: str
   :param scoring_technique: The scoring technique used for evaluating the embeddings.
   :type scoring_technique: str
   :param neg_ratio: The ratio of negative samples to positive samples in the dataset.
   :type neg_ratio: float
   :param label_smoothing_rate: The rate of label smoothing applied to the dataset.
   :type label_smoothing_rate: float

   :returns: A PyTorch dataset object ready for training.
   :rtype: torch.utils.data.Dataset


.. py:function:: construct_dataset(*, train_set: Union[numpy.ndarray, list], valid_set=None, test_set=None, ordered_bpe_entities=None, train_target_indices=None, target_dim: int = None, entity_to_idx: dict, relation_to_idx: dict, form_of_labelling: str, scoring_technique: str, neg_ratio: int, label_smoothing_rate: float, byte_pair_encoding=None, block_size: int = None) -> torch.utils.data.Dataset

   Constructs a dataset based on the specified parameters and returns a PyTorch Dataset object.

   :param train_set: The training set consisting of triples or tokens.
   :type train_set: Union[np.ndarray, list]
   :param valid_set: The validation set. Not currently used in dataset construction.
   :type valid_set: Optional
   :param test_set: The test set. Not currently used in dataset construction.
   :type test_set: Optional
   :param ordered_bpe_entities: Ordered byte pair encoding entities for the dataset.
   :type ordered_bpe_entities: Optional
   :param train_target_indices: Indices of target entities or relations for training.
   :type train_target_indices: Optional
   :param target_dim: The dimension of target entities or relations.
   :type target_dim: int, optional
   :param entity_to_idx: A dictionary mapping entity strings to indices.
   :type entity_to_idx: dict
   :param relation_to_idx: A dictionary mapping relation strings to indices.
   :type relation_to_idx: dict
   :param form_of_labelling: Specifies the form of labelling, such as 'EntityPrediction' or 'RelationPrediction'.
   :type form_of_labelling: str
   :param scoring_technique: The scoring technique used for generating negative samples or evaluating the model.
   :type scoring_technique: str
   :param neg_ratio: The ratio of negative samples to positive samples.
   :type neg_ratio: int
   :param label_smoothing_rate: The rate of label smoothing applied to labels.
   :type label_smoothing_rate: float
   :param byte_pair_encoding: Indicates if byte pair encoding is used.
   :type byte_pair_encoding: Optional
   :param block_size: The block size for transformer-based models.
   :type block_size: int, optional

   :returns: A PyTorch dataset object ready for model training.
   :rtype: torch.utils.data.Dataset


.. py:class:: BPE_NegativeSamplingDataset(train_set: torch.LongTensor, ordered_shaped_bpe_entities: torch.LongTensor, neg_ratio: int)


   Bases: :py:obj:`torch.utils.data.Dataset`

   A PyTorch Dataset for handling negative sampling with Byte Pair Encoding (BPE) entities.

   This dataset extends the PyTorch Dataset class to provide functionality for negative sampling
   in the context of knowledge graph embeddings. It uses byte pair encoding for entities
   to handle large vocabularies efficiently.

   :param train_set: A tensor containing the training set triples with byte pair encoded entities and relations.
                     The shape of the tensor is [N, 3], where N is the number of triples.
   :type train_set: torch.LongTensor
   :param ordered_shaped_bpe_entities: A tensor containing the ordered and shaped byte pair encoded entities.
   :type ordered_shaped_bpe_entities: torch.LongTensor
   :param neg_ratio: The ratio of negative samples to generate per positive sample.
   :type neg_ratio: int

   .. attribute:: num_bpe_entities

      The number of byte pair encoded entities.

      :type: int

   .. attribute:: num_datapoints

      The number of data points (triples) in the training set.

      :type: int

   .. py:method:: __len__() -> int

      Returns the total number of data points in the dataset.

      :returns: The number of data points.
      :rtype: int


   .. py:method:: __getitem__(idx: int) -> Tuple[torch.Tensor, torch.Tensor]

      Retrieves the BPE-encoded triple and its corresponding label at the specified index.

      :param idx: Index of the triple to retrieve.
      :type idx: int

      :returns: A tuple containing the following elements:
                - The BPE-encoded triple as a torch.Tensor of shape (3,).
                - The label for the triple, where positive examples have a label of 1 and negative examples have a label
                  of 0, as a torch.Tensor.
      :rtype: tuple


   .. py:method:: collate_fn(batch_shaped_bpe_triples: List[Tuple[torch.Tensor, torch.Tensor]]) -> Tuple[torch.Tensor, torch.Tensor]

      Collate function for the BPE_NegativeSamplingDataset. It processes a batch of byte pair encoded triples,
      performs negative sampling, and returns the batch along with corresponding labels.

      This function is designed to be used with a PyTorch DataLoader. It takes a list of byte pair encoded triples
      as input and generates negative samples according to the specified negative sampling ratio. The function
      ensures that the negative samples are combined with the original triples to form a single batch, which is
      suitable for training a knowledge graph embedding model.

      :param batch_shaped_bpe_triples: A list of tuples, where each tuple contains byte pair encoded representations of head entities, relations,
                                       and tail entities for a batch of triples.
      :type batch_shaped_bpe_triples: List[Tuple[torch.Tensor, torch.Tensor]]

      :returns: A tuple containing two elements:
                - The first element is a torch.Tensor of shape [N * (1 + neg_ratio), 3] that contains both the original
                byte pair encoded triples and the generated negative samples. N is the original number of triples in the
                batch, and neg_ratio is the negative sampling ratio.
                - The second element is a torch.Tensor of shape [N * (1 + neg_ratio)] that contains the labels for each
                triple in the batch. Positive samples are labeled as 1, and negative samples are labeled as 0.
      :rtype: Tuple[torch.Tensor, torch.Tensor]



.. py:class:: MultiLabelDataset(train_set: torch.LongTensor, train_indices_target: torch.LongTensor, target_dim: int, torch_ordered_shaped_bpe_entities: torch.LongTensor)


   Bases: :py:obj:`torch.utils.data.Dataset`

   A dataset class for multi-label knowledge graph embedding tasks. This dataset is designed for models where
   the output involves predicting multiple labels (entities or relations) for a given input (e.g., predicting all
   possible tail entities given a head entity and a relation).

   :param train_set: A tensor containing the training set triples with byte pair encoding, shaped as [num_triples, 3],
                     where each triple is [head, relation, tail].
   :type train_set: torch.LongTensor
   :param train_indices_target: A tensor where each row corresponds to the indices of the target labels for each training example.
                                The length of this tensor must match the number of triples in `train_set`.
   :type train_indices_target: torch.LongTensor
   :param target_dim: The dimensionality of the target space, typically the total number of possible labels (entities or relations).
   :type target_dim: int
   :param torch_ordered_shaped_bpe_entities: A tensor containing ordered byte pair encoded entities used for creating embeddings.
                                             This tensor is not directly used in generating targets but may be utilized for additional processing
                                             or embedding lookup.
   :type torch_ordered_shaped_bpe_entities: torch.LongTensor

   .. attribute:: num_datapoints

      The number of data points (triples) in the dataset.

      :type: int

   .. attribute:: collate_fn

      Optional custom collate function to be used with a PyTorch DataLoader.
      It's set to None by default and can be specified after initializing the dataset if needed.

      :type: None or callable

   .. note::

      This dataset is particularly suited for KvsAll (K entities vs. All entities) and AllvsAll training strategies
      in knowledge graph embedding, where a model predicts a set of possible tail entities given a head entity
      and a relation (or vice versa), and where each training example can have multiple correct labels.

   .. py:method:: __len__() -> int

      Returns the total number of data points in the dataset.

      :returns: The number of data points.
      :rtype: int


   .. py:method:: __getitem__(idx: int) -> Tuple[torch.Tensor, torch.Tensor]

      Retrieves the knowledge graph triple and its corresponding multi-label target vector at the specified index.

      :param idx: Index of the triple to retrieve.
      :type idx: int

      :returns: A tuple containing the following elements:
                - The triple as a torch.Tensor of shape (3,).
                - The multi-label target vector as a torch.Tensor of shape (`target_dim`,), where each element
                  indicates the presence (1) or absence (0) of a label for the given triple.
      :rtype: tuple



.. py:class:: MultiClassClassificationDataset(subword_units: numpy.ndarray, block_size: int = 8)


   Bases: :py:obj:`torch.utils.data.Dataset`

   A dataset class for multi-class classification tasks, specifically designed for the 1vsALL training strategy
   in knowledge graph embedding models. This dataset supports tasks where the model predicts a single correct
   label from all possible labels for a given input.

   :param subword_units: An array of subword unit indices representing the training data. Each row in the array corresponds to a
                         sequence of subword units (e.g., Byte Pair Encoding tokens) that have been converted to their respective
                         numeric indices.
   :type subword_units: np.ndarray
   :param block_size: The size of each sequence of subword units to be used as input to the model. This defines the length of
                      the sequences that the model will receive as input, by default 8.
   :type block_size: int, optional

   .. attribute:: num_of_data_points

      The number of sequences or data points available in the dataset, calculated based on the length of the
      `subword_units` array and the `block_size`.

      :type: int

   .. attribute:: collate_fn

      An optional custom collate function to be used with a PyTorch DataLoader. It's set to None by default
      and can be specified after initializing the dataset if needed.

      :type: None or callable

   .. note::

      This dataset is tailored for training knowledge graph embedding models on tasks where the output is a single
      label out of many possible labels (1vsALL strategy). It is especially suited for models trained with subword
      tokenization methods like Byte Pair Encoding (BPE), where inputs are sequences of subword unit indices.

   .. py:method:: __len__() -> int

      Returns the total number of sequences or data points available in the dataset.

      :returns: The number of sequences or data points.
      :rtype: int


   .. py:method:: __getitem__(idx: int) -> Tuple[torch.Tensor, torch.Tensor]

      Retrieves an input sequence and its subsequent target sequence for next token prediction.

      :param idx: The starting index for the sequence to be retrieved from the dataset.
      :type idx: int

      :returns: A tuple containing two elements:
                - `x`: The input sequence as a torch.Tensor of shape (`block_size`,).
                - `y`: The target sequence as a torch.Tensor of shape (`block_size`,), offset by one position
                  from the input sequence.
      :rtype: Tuple[torch.Tensor, torch.Tensor]



.. py:class:: OnevsAllDataset(train_set_idx: numpy.ndarray, entity_idxs)


   Bases: :py:obj:`torch.utils.data.Dataset`

   A dataset for the One-vs-All (1vsAll) training strategy designed for knowledge graph embedding tasks.
   This dataset structure is particularly suited for models predicting a single correct label (entity) out of
   all possible entities for a given pair of head entity and relation.

   :param train_set_idx: An array containing indexed triples from the knowledge graph. Each row represents a triple consisting of
                         indices for the head entity, relation, and tail entity, respectively.
   :type train_set_idx: np.ndarray
   :param entity_idxs: A dictionary mapping entity names to their corresponding unique integer indices. This is used to determine
                       the dimensionality of the target vector in the 1vsAll setting.
   :type entity_idxs: dict

   .. attribute:: train_data

      A tensor version of `train_set_idx`, prepared for use with PyTorch models.

      :type: torch.LongTensor

   .. attribute:: target_dim

      The dimensionality of the target vector, equivalent to the total number of unique entities in the dataset.

      :type: int

   .. attribute:: collate_fn

      An optional custom collate function for use with a PyTorch DataLoader. By default, it is set to None and can
      be specified after initializing the dataset.

      :type: None or callable

   .. note::

      This dataset is optimized for training knowledge graph embedding models using the 1vsAll strategy, where the
      model aims to correctly predict the tail entity from all possible entities given the head entity and relation.

   .. py:method:: __len__()

      Returns the total number of triples in the dataset.

      :returns: The total number of triples.
      :rtype: int


   .. py:method:: __getitem__(idx)

      Retrieves the input data and target vector for the triple at index `idx`.

      The input data consists of the indices for the head entity and relation, while the target vector is a
      one-hot encoded vector with a `1` at the position corresponding to the tail entity's index and `0`s elsewhere.

      :param idx: The index of the triple to retrieve.
      :type idx: int

      :returns: A tuple containing two elements:
                - The input data as a torch.Tensor of shape (2,), containing the indices of the head entity and relation.
                - The target vector as a torch.Tensor of shape (`target_dim`,), a one-hot encoded vector for the tail entity.
      :rtype: Tuple[torch.Tensor, torch.Tensor]



.. py:class:: KvsAll(train_set_idx: numpy.ndarray, entity_idxs, relation_idxs, form, store=None, label_smoothing_rate: float = 0.0)


   Bases: :py:obj:`torch.utils.data.Dataset`

   Creates a dataset for K-vs-All training strategy, inheriting from torch.utils.data.Dataset.
   This dataset is tailored for training scenarios where a model predicts all valid tail entities
   given a head entity and relation pair or vice versa. The labels are multi-hot encoded to represent
   the presence of multiple valid entities.

   Let \(D\) denote a dataset for KvsAll training and be defined as \(D := \{(x, y)_i\}_{i=1}^{N}\), where:
   \(x: (h, r)\) is a unique tuple of an entity \(h \in E\) and a relation \(r \in R\) that has been seen in the input graph.
   \(y\) denotes a multi-label vector \(\in [0, 1]^{|E|}\) is a binary label. For all \(y_i = 1\) s.t. \((h, r, E_i) \in KG\).

   :param train_set_idx: A numpy array of shape `(n, 3)` representing `n` triples, where each triple consists of
                         integer indices corresponding to a head entity, a relation, and a tail entity.
   :type train_set_idx: numpy.ndarray
   :param entity_idxs: A dictionary mapping entity names (strings) to their unique integer identifiers.
   :type entity_idxs: dict
   :param relation_idxs: A dictionary mapping relation names (strings) to their unique integer identifiers.
   :type relation_idxs: dict
   :param form: A string indicating the prediction form, either 'RelationPrediction' or 'EntityPrediction'.
   :type form: str
   :param store: A precomputed dictionary storing the training data points. If provided, it should map
                 tuples of entity and relation indices to lists of entity indices. If `None`, the store
                 will be constructed from `train_set_idx`.
   :type store: dict, optional
   :param label_smoothing_rate: A float representing the rate of label smoothing to be applied. A value of 0 means no
                                label smoothing is applied.
   :type label_smoothing_rate: float, default=0.0

   .. attribute:: train_data

      Tensor containing the input features for the model, typically consisting of pairs of
      entity and relation indices.

      :type: torch.LongTensor

   .. attribute:: train_target

      Tensor containing the target labels for the model, multi-hot encoded to indicate the
      presence of multiple valid entities.

      :type: torch.LongTensor

   .. attribute:: target_dim

      The dimensionality of the target labels, corresponding to the number of unique entities
      or relations, depending on the `form`.

      :type: int

   .. attribute:: collate_fn

      Placeholder for a custom collate function to be used with a PyTorch DataLoader. This is
      typically set to `None` and can be overridden as needed.

      :type: None

   .. note::

      The K-vs-All training strategy is used in scenarios where the task is to predict multiple
      valid entities given a single entity and relation pair. This dataset supports both predicting
      multiple valid tail entities given a head entity and relation (EntityPrediction) and predicting
      multiple valid relations given a pair of entities (RelationPrediction).

      The label smoothing rate can be adjusted to control the degree of smoothing applied to the
      target labels, which can help with regularization and model generalization.

   .. py:method:: __len__() -> int

      Returns the number of items in the dataset.

      :returns: The total number of items.
      :rtype: int


   .. py:method:: __getitem__(idx: int) -> Tuple[torch.Tensor, torch.Tensor]

      Retrieves the input pair (head entity, relation) and the corresponding multi-label target vector for the
      item at index `idx`.

      The target vector is a binary vector of length `target_dim`, where each element indicates the presence or
      absence of a tail entity for the given input pair.

      :param idx: The index of the item to retrieve.
      :type idx: int

      :returns: A tuple containing two elements:
                - The input pair as a torch.Tensor of shape (2,), containing the indices of the head entity and relation.
                - The multi-label target vector as a torch.Tensor of shape (`target_dim`,), indicating the presence or
                  absence of each possible tail entity.
      :rtype: Tuple[torch.Tensor, torch.Tensor]



.. py:class:: AllvsAll(train_set_idx: numpy.ndarray, entity_idxs, relation_idxs, label_smoothing_rate=0.0)


   Bases: :py:obj:`torch.utils.data.Dataset`

       A dataset class for the All-versus-All (AllvsAll) training strategy suitable for knowledge graph embedding models.
       This strategy considers all possible pairs of entities and relations, regardless of whether they exist in the
       knowledge graph, to predict the associated tail entities.

       Let D denote a dataset for AllvsAll training and be defined as D:= {(x,y)_i}_i ^N, where
       x: (h,r) is a possible unique tuple of an entity h \in E and a relation r \in R. Hence N = |E| x |R|
       y: denotes a multi-label vector \in [0,1]^{|E|} is a binary label.
   orall y_i =1 s.t. (h, r, E_i) \in KG.
       This setup extends beyond observed triples to include all possible combinations of entities and relations,
       marking non-existent combinations as negatives. It aims to enrich the training data with hard negatives.

       Parameters
       ----------
       train_set_idx : numpy.ndarray
           An array of shape `(n, 3)`, where each row represents a triple (head entity index, relation index,
           tail entity index).
       entity_idxs : dict
           A dictionary mapping entity names to their unique integer indices.
       relation_idxs : dict
           A dictionary mapping relation names to their unique integer indices.
       label_smoothing_rate : float, default=0.0
           A parameter for label smoothing to mitigate overfitting by softening the hard labels.

       Attributes
       ----------
       train_data : torch.LongTensor
           A tensor containing all possible pairs of entities and relations derived from the input triples.
       train_target : Union[np.ndarray, list]
           A target structure (either a Numpy array or a list) indicating the existence of a tail entity for
           each head entity and relation pair. It supports multi-label classification where a pair can have
           multiple correct tail entities.
       target_dim : int
           The dimension of the target vector, equal to the total number of unique entities.
       collate_fn : None or callable
           An optional function to merge a list of samples into a batch for loading. If not provided, the default
           collate function of PyTorch's DataLoader will be used.


   .. py:method:: __len__() -> int

      Returns the number of items in the dataset, including both existing and potential triples.

      :returns: The total number of items.
      :rtype: int


   .. py:method:: __getitem__(idx: int) -> Tuple[torch.Tensor, torch.Tensor]

      Retrieves the input pair (head entity, relation) and the corresponding multi-label target vector for the
      item at index `idx`. The target vector is a binary vector of length `target_dim`, where each element indicates
      the presence or absence of a tail entity for the given input pair, including negative samples.

      :param idx: The index of the item to retrieve.
      :type idx: int

      :returns: A tuple containing two elements:
                - The input pair as a torch.Tensor of shape (2,), containing the indices of the head entity and relation.
                - The multi-label target vector as a torch.Tensor of shape (`target_dim`,), indicating the presence or
                  absence of each possible tail entity, including negative samples.
      :rtype: Tuple[torch.Tensor, torch.Tensor]



.. py:class:: KvsSampleDataset(train_set: numpy.ndarray, num_entities, num_relations, neg_sample_ratio: int = None, label_smoothing_rate: float = 0.0)


   Bases: :py:obj:`torch.utils.data.Dataset`

   Constructs a dataset for KvsSample training strategy, specifically designed for knowledge graph embedding models.
   This dataset formulation is aimed at handling the imbalance between positive and negative examples for each
   (head, relation) pair by subsampling tail entities. The subsampling ensures a balanced representation of positive
   and negative examples in each training batch, according to the specified negative sampling ratio.

   The dataset is defined as \(D:= \{(x,y)_i\}_{i=1}^{N}\), where:
       - \(x: (h,r)\) is a unique head entity \(h \in E\) and a relation \(r \in R\).
       - \(y \in [0,1]^{|E|}\) is a binary label vector. For all \(y_i = 1\) such that \((h, r, E_i) \in KG\).

   At each mini-batch construction, we subsample \(y\), hence \(|new_y| \ll |E|\).
   The new \(y\) contains all 1's if \(sum(y) <\) neg_sample_ratio, otherwise, it contains a balanced mix of 1's and 0's.

   :param train_set: An array of shape \((n, 3)\), where \(n\) is the number of triples in the dataset. Each row in the array
                     represents a triple \((h, r, t)\), consisting of head entity index \(h\), relation index \(r\), and
                     tail entity index \(t\).
   :type train_set: np.ndarray
   :param num_entities: The total number of unique entities in the dataset.
   :type num_entities: int
   :param num_relations: The total number of unique relations in the dataset.
   :type num_relations: int
   :param neg_sample_ratio: The ratio of negative samples to positive samples for each (head, relation) pair. If the number of
                            available positive samples is less than this ratio, additional negative samples are generated to meet the ratio.
   :type neg_sample_ratio: int
   :param label_smoothing_rate: A parameter for label smoothing, aiming to mitigate overfitting by softening the hard labels. The labels
                                are adjusted towards a uniform distribution, with the smoothing rate determining the degree of softening.
   :type label_smoothing_rate: float, default=0.0

   .. attribute:: train_data

      A tensor containing the (head, relation) pairs derived from the input triples, used to index the training set.

      :type: torch.IntTensor

   .. attribute:: train_target

      A list where each element corresponds to the tail entity indices associated with a given (head, relation) pair.

      :type: list of numpy.ndarray

   .. attribute:: collate_fn

      A function to merge a list of samples to form a batch. If None, PyTorch's default collate function is used.

      :type: None or callable

   .. py:method:: __len__()

      Returns the total number of unique (head, relation) pairs in the dataset.

      :returns: The number of unique (head, relation) pairs.
      :rtype: int


   .. py:method:: __getitem__(idx)

      Retrieves the data for the given index, including the (head, relation) pair, selected tail entity indices,
      and their labels. Positive examples are sampled from the training set, and negative examples are generated
      by randomly selecting tail entities not associated with the (head, relation) pair.

      :param idx: The index of the (head, relation) pair in the dataset.
      :type idx: int

      :returns: A tuple containing the following elements:
                - x: The (head, relation) pair as a torch.Tensor.
                - y_idx: The indices of selected tail entities, both positive and negative, as a torch.IntTensor.
                - y_vec: The labels for the selected tail entities, with 1s indicating positive and 0s indicating negative
                         examples, as a torch.Tensor.
      :rtype: tuple



.. py:class:: NegSampleDataset(train_set: numpy.ndarray, num_entities: int, num_relations: int, neg_sample_ratio: int = 1)


   Bases: :py:obj:`torch.utils.data.Dataset`

   A dataset for training knowledge graph embedding models using negative sampling.
   For each positive triple from the knowledge graph, a negative triple is generated by corrupting either
   the head or the tail entity with a randomly selected entity.

   :param train_set: The training set of triples, where each triple consists of indices of the head entity, relation, and tail entity.
   :type train_set: np.ndarray
   :param num_entities: The total number of unique entities in the knowledge graph.
   :type num_entities: int
   :param num_relations: The total number of unique relations in the knowledge graph.
   :type num_relations: int
   :param neg_sample_ratio: The ratio of negative samples to positive samples. Currently, it generates one negative sample per positive sample.
   :type neg_sample_ratio: int, default=1

   .. attribute:: train_set

      The training set converted to a PyTorch tensor and expanded to include a batch dimension.

      :type: torch.Tensor

   .. attribute:: length

      The total number of triples in the training set.

      :type: int

   .. attribute:: num_entities

      A tensor containing the total number of entities.

      :type: torch.tensor

   .. attribute:: num_relations

      A tensor containing the total number of relations.

      :type: torch.tensor

   .. attribute:: neg_sample_ratio

      A tensor containing the ratio of negative to positive samples.

      :type: torch.tensor

   .. py:method:: __len__() -> int

      Returns the total number of triples in the dataset.

      :returns: The total number of triples.
      :rtype: int


   .. py:method:: __getitem__(idx: int) -> Tuple[torch.Tensor, torch.Tensor]

      Retrieves a pair consisting of a positive triple and a generated negative triple along with their labels.

      :param idx: The index of the triple to retrieve.
      :type idx: int

      :returns: A tuple where the first element is a tensor containing a pair of positive and negative triples,
                and the second element is a tensor containing their respective labels (1 for positive, 0 for negative).
      :rtype: Tuple[torch.Tensor, torch.Tensor]



.. py:class:: TriplePredictionDataset(train_set: numpy.ndarray, num_entities: int, num_relations: int, neg_sample_ratio: int = 1, label_smoothing_rate: float = 0.0)


   Bases: :py:obj:`torch.utils.data.Dataset`

       A dataset for triple prediction using negative sampling and label smoothing.

       D:= {(x)_i}_i ^N, where
       - x:(h,r, t) \in KG is a unique h \in E and a relation r \in R and
       - collact_fn => Generates negative triples

       collect_fn:
   orall (h,r,t) \in G obtain, create negative triples{(h,r,x),(,r,t),(h,m,t)}

       y: labels are represented in torch.float16

       This dataset generates negative triples by corrupting either the head or the tail of each positive triple
       from the training set. The corruption is performed by randomly replacing the head or the tail with another entity
       from the entity set. The dataset supports label smoothing to soften the target labels, which can help improve
       generalization.

       Parameters
       ----------
       train_set : np.ndarray
           The training set consisting of triples in the form of (head, relation, tail) indices.
       num_entities : int
           The total number of unique entities in the knowledge graph.
       num_relations : int
           The total number of unique relations in the knowledge graph.
       neg_sample_ratio : int, optional
           The ratio of negative samples to generate for each positive sample. Default is 1.
       label_smoothing_rate : float, optional
           The rate of label smoothing to apply to the target labels. Default is 0.0.

       Notes
       -----
       The `collate_fn` should be passed to the DataLoader's `collate_fn` argument to ensure proper
       batch processing and negative sample generation.


   .. py:method:: __len__() -> int

      Returns the total number of triples in the dataset.

      :returns: The total number of triples.
      :rtype: int


   .. py:method:: __getitem__(idx: int) -> torch.Tensor

      Retrieves a triple for the given index.

      :param idx: The index of the triple to retrieve.
      :type idx: int

      :returns: The triple at the specified index.
      :rtype: torch.Tensor


   .. py:method:: collate_fn(batch: List[torch.Tensor]) -> Tuple[torch.Tensor, torch.Tensor]

      Custom collate function to generate a batch of positive and negative triples along with their labels.

      :param batch: A list of tensors representing triples.
      :type batch: List[torch.Tensor]

      :returns: A tuple containing a tensor of triples and a tensor of corresponding labels.
      :rtype: Tuple[torch.Tensor, torch.Tensor]



.. py:class:: CVDataModule(train_set_idx: numpy.ndarray, num_entities: int, num_relations: int, neg_sample_ratio: int, batch_size: int, num_workers: int)


   Bases: :py:obj:`pytorch_lightning.LightningDataModule`

   A LightningDataModule for setting up data loaders for cross-validation training of knowledge graph embedding models.

   :param train_set_idx: An array of indexed triples for training, where each triple consists of indices of the head entity, relation,
                         and tail entity.
   :type train_set_idx: np.ndarray
   :param num_entities: The total number of unique entities in the knowledge graph.
   :type num_entities: int
   :param num_relations: The total number of unique relations in the knowledge graph.
   :type num_relations: int
   :param neg_sample_ratio: The ratio of negative samples to positive samples for each positive triple.
   :type neg_sample_ratio: int
   :param batch_size: The number of samples in each batch of data.
   :type batch_size: int
   :param num_workers: The number of subprocesses to use for data loading. https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader
   :type num_workers: int

   :returns: A PyTorch DataLoader for the training dataset.
   :rtype: DataLoader

   .. py:method:: train_dataloader() -> torch.utils.data.DataLoader

      Creates a DataLoader for the training dataset.

      :returns: A DataLoader object that loads the training data.
      :rtype: DataLoader


   .. py:method:: setup(*args, **kwargs)

      Called at the beginning of fit (train + validate), validate, test, or predict. This is a good hook when you
      need to build models dynamically or adjust something about them. This hook is called on every process when
      using DDP.

      :param stage: either ``'fit'``, ``'validate'``, ``'test'``, or ``'predict'``

      Example::

          class LitModel(...):
              def __init__(self):
                  self.l1 = None

              def prepare_data(self):
                  download_data()
                  tokenize()

                  # don't do this
                  self.something = else

              def setup(self, stage):
                  data = load_data(...)
                  self.l1 = nn.Linear(28, data.num_classes)



   .. py:method:: transfer_batch_to_device(*args, **kwargs)

      Override this hook if your :class:`~torch.utils.data.DataLoader` returns tensors wrapped in a custom data
      structure.

      The data types listed below (and any arbitrary nesting of them) are supported out of the box:

      - :class:`torch.Tensor` or anything that implements `.to(...)`
      - :class:`list`
      - :class:`dict`
      - :class:`tuple`

      For anything else, you need to define how the data is moved to the target device (CPU, GPU, TPU, ...).

      .. note::

         This hook should only transfer the data and not modify it, nor should it move the data to
         any other device than the one passed in as argument (unless you know what you are doing).
         To check the current state of execution of this hook you can use
         ``self.trainer.training/testing/validating/predicting`` so that you can
         add different logic as per your requirement.

      :param batch: A batch of data that needs to be transferred to a new device.
      :param device: The target device as defined in PyTorch.
      :param dataloader_idx: The index of the dataloader to which the batch belongs.

      :returns: A reference to the data on the new device.

      Example::

          def transfer_batch_to_device(self, batch, device, dataloader_idx):
              if isinstance(batch, CustomBatch):
                  # move all tensors in your custom data structure to the device
                  batch.samples = batch.samples.to(device)
                  batch.targets = batch.targets.to(device)
              elif dataloader_idx == 0:
                  # skip device transfer for the first dataloader or anything you wish
                  pass
              else:
                  batch = super().transfer_batch_to_device(batch, device, dataloader_idx)
              return batch

      :raises MisconfigurationException: If using IPUs, ``Trainer(accelerator='ipu')``.

      .. seealso::

         - :meth:`move_data_to_device`
         - :meth:`apply_to_collection`


   .. py:method:: prepare_data(*args, **kwargs)

      Use this to download and prepare data. Downloading and saving data with multiple processes (distributed
      settings) will result in corrupted data. Lightning ensures this method is called only within a single process,
      so you can safely add your downloading logic within.

      .. warning:: DO NOT set state to the model (use ``setup`` instead)
          since this is NOT called on every device

      Example::

          def prepare_data(self):
              # good
              download_data()
              tokenize()
              etc()

              # bad
              self.split = data_split
              self.some_state = some_other_state()

      In a distributed environment, ``prepare_data`` can be called in two ways
      (using :ref:`prepare_data_per_node<common/lightning_module:prepare_data_per_node>`)

      1. Once per node. This is the default and is only called on LOCAL_RANK=0.
      2. Once in total. Only called on GLOBAL_RANK=0.

      Example::

          # DEFAULT
          # called once per node on LOCAL_RANK=0 of that node
          class LitDataModule(LightningDataModule):
              def __init__(self):
                  super().__init__()
                  self.prepare_data_per_node = True


          # call on GLOBAL_RANK=0 (great for shared file systems)
          class LitDataModule(LightningDataModule):
              def __init__(self):
                  super().__init__()
                  self.prepare_data_per_node = False

      This is called before requesting the dataloaders:

      .. code-block:: python

          model.prepare_data()
          initialize_distributed()
          model.setup(stage)
          model.train_dataloader()
          model.val_dataloader()
          model.test_dataloader()
          model.predict_dataloader()




