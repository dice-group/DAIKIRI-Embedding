:py:mod:`dicee`
===============

.. py:module:: dicee


Subpackages
-----------
.. toctree::
   :titlesonly:
   :maxdepth: 3

   models/index.rst
   read_preprocess_save_load_kg/index.rst
   scripts/index.rst
   trainer/index.rst


Submodules
----------
.. toctree::
   :titlesonly:
   :maxdepth: 1

   abstracts/index.rst
   analyse_experiments/index.rst
   callbacks/index.rst
   config/index.rst
   dataset_classes/index.rst
   eval_static_funcs/index.rst
   evaluator/index.rst
   executer/index.rst
   knowledge_graph/index.rst
   knowledge_graph_embeddings/index.rst
   query_generator/index.rst
   sanity_checkers/index.rst
   static_funcs/index.rst
   static_funcs_training/index.rst
   static_preprocess_funcs/index.rst


Package Contents
----------------

Classes
~~~~~~~

.. autoapisummary::

   dicee.CMult
   dicee.Pyke
   dicee.DistMult
   dicee.KeciBase
   dicee.Keci
   dicee.TransE
   dicee.DeCaL
   dicee.ComplEx
   dicee.AConEx
   dicee.AConvO
   dicee.AConvQ
   dicee.ConvQ
   dicee.ConvO
   dicee.ConEx
   dicee.QMult
   dicee.OMult
   dicee.Shallom
   dicee.LFMult
   dicee.PykeenKGE
   dicee.BytE
   dicee.BaseKGE
   dicee.DICE_Trainer
   dicee.KGE
   dicee.Execute
   dicee.BPE_NegativeSamplingDataset
   dicee.MultiLabelDataset
   dicee.MultiClassClassificationDataset
   dicee.OnevsAllDataset
   dicee.KvsAll
   dicee.AllvsAll
   dicee.KvsSampleDataset
   dicee.NegSampleDataset
   dicee.TriplePredictionDataset
   dicee.CVDataModule
   dicee.QueryGenerator



Functions
~~~~~~~~~

.. autoapisummary::

   dicee.create_recipriocal_triples
   dicee.get_er_vocab
   dicee.get_re_vocab
   dicee.get_ee_vocab
   dicee.timeit
   dicee.save_pickle
   dicee.load_pickle
   dicee.select_model
   dicee.load_model
   dicee.load_model_ensemble
   dicee.save_numpy_ndarray
   dicee.numpy_data_type_changer
   dicee.save_checkpoint_model
   dicee.store
   dicee.add_noisy_triples
   dicee.read_or_load_kg
   dicee.intialize_model
   dicee.load_json
   dicee.save_embeddings
   dicee.random_prediction
   dicee.deploy_triple_prediction
   dicee.deploy_tail_entity_prediction
   dicee.deploy_head_entity_prediction
   dicee.deploy_relation_prediction
   dicee.vocab_to_parquet
   dicee.create_experiment_folder
   dicee.continual_training_setup_executor
   dicee.exponential_function
   dicee.load_numpy
   dicee.evaluate
   dicee.download_file
   dicee.download_files_from_url
   dicee.download_pretrained_model
   dicee.mapping_from_first_two_cols_to_third
   dicee.timeit
   dicee.load_pickle
   dicee.reload_dataset
   dicee.construct_dataset



Attributes
~~~~~~~~~~

.. autoapisummary::

   dicee.__version__


.. py:class:: CMult(args)


   Bases: :py:obj:`dicee.models.base_model.BaseKGE`

   The CMult class represents a specific kind of mathematical object used in knowledge graph embeddings,
   involving Clifford algebra multiplication. It defines several algebraic structures based on the signature (p, q),
   such as Real Numbers, Complex Numbers, Quaternions, and others. The class provides functionality for
   performing Clifford multiplication, a generalization of the geometric product for vectors in a Clifford algebra.

   TODO: Add mathematical format for sphinx.

   Cl_(0,0) => Real Numbers


   Cl_(0,1) =>
               A multivector \mathbf{a} = a_0 + a_1 e_1
               A multivector \mathbf{b} = b_0 + b_1 e_1

               multiplication is isomorphic to the product of two complex numbers

               \mathbf{a}      imes \mathbf{b} = a_0 b_0 + a_0b_1 e1 + a_1 b_1 e_1 e_1
                                            = (a_0 b_0 - a_1 b_1) + (a_0 b_1 + a_1 b_0) e_1
   Cl_(2,0) =>
               A multivector \mathbf{a} = a_0 + a_1 e_1 + a_2 e_2 + a_{12} e_1 e_2
               A multivector \mathbf{b} = b_0 + b_1 e_1 + b_2 e_2 + b_{12} e_1 e_2

               \mathbf{a}      imes \mathbf{b} = a_0b_0 + a_0b_1 e_1 + a_0b_2e_2 + a_0 b_12 e_1 e_2
                                           + a_1 b_0 e_1 + a_1b_1 e_1_e1 ..

   Cl_(0,2) => Quaternions

   .. attribute:: name

      The name identifier for the CMult class.

      :type: str

   .. attribute:: entity_embeddings

      Embedding layer for entities in the knowledge graph.

      :type: torch.nn.Embedding

   .. attribute:: relation_embeddings

      Embedding layer for relations in the knowledge graph.

      :type: torch.nn.Embedding

   .. attribute:: p

      Non-negative integer representing the number of positive square terms in the Clifford algebra.

      :type: int

   .. attribute:: q

      Non-negative integer representing the number of negative square terms in the Clifford algebra.

      :type: int

   .. method:: clifford_mul(x: torch.FloatTensor, y: torch.FloatTensor, p: int, q: int) -> tuple

      Performs Clifford multiplication based on the given signature (p, q).

   .. method:: score(head_ent_emb, rel_ent_emb, tail_ent_emb) -> torch.FloatTensor

      Computes a scoring function for a head entity, relation, and tail entity embeddings.

   .. method:: forward_triples(x: torch.LongTensor) -> torch.FloatTensor

      Computes scores for a batch of triples.

   .. method:: forward_k_vs_all(x: torch.Tensor) -> torch.FloatTensor

      Computes scores for a batch of triples against all entities in the knowledge graph.


   .. py:method:: clifford_mul(x: torch.FloatTensor, y: torch.FloatTensor, p: int, q: int) -> tuple

              Performs Clifford multiplication in the Clifford algebra Cl_{p,q}. This method generalizes the geometric product
              of vectors in a Clifford algebra, handling different algebraic structures like real numbers, complex numbers,
              quaternions, etc., based on the signature (p, q).

              Clifford multiplication Cl_{p,q} (\mathbb{R})

              ei ^2 = +1     for i =< i =< p
              ej ^2 = -1     for p < j =< p+q
              ei ej = -eje1  for i
      eq j

              Parameters
              ----------
              x : torch.FloatTensor
                  The first multivector operand with shape (n, d).
              y : torch.FloatTensor
                  The second multivector operand with shape (n, d).
              p : int
                  A non-negative integer representing the number of positive square terms in the Clifford algebra.
              q : int
                  A non-negative integer representing the number of negative square terms in the Clifford algebra.

              Returns
              -------
              tuple
                  The result of Clifford multiplication, a tuple of tensors representing the components of the resulting multivector.



   .. py:method:: score(head_ent_emb: torch.FloatTensor, rel_ent_emb: torch.FloatTensor, tail_ent_emb: torch.FloatTensor) -> torch.FloatTensor

      Computes a scoring function for a given triple of head entity, relation, and tail entity embeddings.
      The method involves Clifford multiplication of the head entity and relation embeddings, followed by
      a calculation of the score with the tail entity embedding.

      :param head_ent_emb: Embedding of the head entity.
      :type head_ent_emb: torch.FloatTensor
      :param rel_ent_emb: Embedding of the relation.
      :type rel_ent_emb: torch.FloatTensor
      :param tail_ent_emb: Embedding of the tail entity.
      :type tail_ent_emb: torch.FloatTensor

      :returns: A tensor representing the score of the given triple.
      :rtype: torch.FloatTensor


   .. py:method:: forward_triples(x: torch.LongTensor) -> torch.FloatTensor

      Computes scores for a batch of triples. This method is typically used in training or evaluation
      of knowledge graph embedding models. It applies Clifford multiplication to the embeddings of head
      entities and relations and then calculates the score with respect to the tail entity embeddings.

      :param x: A tensor with shape (n, 3) representing a batch of triples, where each triple consists of indices
                for a head entity, a relation, and a tail entity.
      :type x: torch.LongTensor

      :returns: A tensor with shape (n,) containing the scores for each triple in the batch.
      :rtype: torch.FloatTensor


   .. py:method:: forward_k_vs_all(x: torch.Tensor) -> torch.FloatTensor

      Computes scores for a batch of triples against all entities in the knowledge graph, often used in KvsAll evaluation.
      This method retrieves embeddings for heads and relations, performs Clifford multiplication, and then computes the
      inner product with all entity embeddings to get scores for every possible triple involving the given heads and relations.

      :param x: A tensor with shape (n, 3) representing a batch of triples, where each triple consists of indices
                for a head entity and a relation. The tail entity is to be compared against all possible entities.
      :type x: torch.Tensor

      :returns: A tensor with shape (n,) containing scores for each triple against all possible tail entities.
      :rtype: torch.FloatTensor



.. py:class:: Pyke(args: dict)


   Bases: :py:obj:`dicee.models.base_model.BaseKGE`

   Pyke is a physical embedding model for knowledge graphs, emphasizing the geometric relationships
   in the embedding space. The model aims to represent entities and relations in a way that captures
   the underlying structure of the knowledge graph.

   .. attribute:: name

      The name identifier for the Pyke model.

      :type: str

   .. attribute:: dist_func

      A pairwise distance function to compute distances in the embedding space.

      :type: torch.nn.PairwiseDistance

   .. attribute:: margin

      The margin value used in the scoring function.

      :type: float

   .. method:: forward_triples(x: torch.LongTensor) -> torch.FloatTensor

      Computes scores for a batch of triples based on the physical embedding approach.


   .. py:method:: forward_triples(x: torch.LongTensor) -> torch.FloatTensor

      Computes scores for a batch of triples based on the physical embedding approach.

      The method calculates the Euclidean distance between the head and relation embeddings,
      and between the relation and tail embeddings. The average of these distances is subtracted
      from the margin to compute the score for each triple.

      :param x: A tensor containing indices for head entities, relations, and tail entities.
      :type x: torch.LongTensor

      :returns: Scores for the given batch of triples. Lower scores indicate more likely triples
                according to the geometric arrangement of embeddings.
      :rtype: torch.FloatTensor



.. py:class:: DistMult(args)


   Bases: :py:obj:`dicee.models.base_model.BaseKGE`

   DistMult model for learning and inference in knowledge bases. It represents both entities
   and relations using embeddings and uses a simple bilinear form to compute scores for triples.

   This implementation of the DistMult model is based on the paper:
   'Embedding Entities and Relations for Learning and Inference in Knowledge Bases'
   (https://arxiv.org/abs/1412.6575).

   .. attribute:: name

      The name identifier for the DistMult model.

      :type: str

   .. method:: k_vs_all_score(emb_h: torch.FloatTensor, emb_r: torch.FloatTensor, emb_E: torch.FloatTensor) -> torch.FloatTensor

      Computes scores in a K-vs-All setting using embeddings for a batch of head entities and relations.


   .. method:: forward_k_vs_all(x: torch.LongTensor) -> torch.FloatTensor

      Computes scores for all entities given a batch of head entities and relations.


   .. method:: forward_k_vs_sample(x: torch.LongTensor, target_entity_idx: torch.LongTensor) -> torch.FloatTensor

      Computes scores for a sampled subset of entities given a batch of head entities and relations.


   .. method:: score(h: torch.FloatTensor, r: torch.FloatTensor, t: torch.FloatTensor) -> torch.FloatTensor

      Computes the score of triples using DistMult's scoring function.


   .. py:method:: k_vs_all_score(emb_h: torch.FloatTensor, emb_r: torch.FloatTensor, emb_E: torch.FloatTensor) -> torch.FloatTensor

      Computes scores in a K-vs-All setting using embeddings for a batch of head entities and relations.

      This method multiplies the head entity and relation embeddings, applies a dropout and a normalization,
      and then computes the dot product with all tail entity embeddings.

      :param emb_h: Embeddings of head entities.
      :type emb_h: torch.FloatTensor
      :param emb_r: Embeddings of relations.
      :type emb_r: torch.FloatTensor
      :param emb_E: Embeddings of all entities.
      :type emb_E: torch.FloatTensor

      :returns: Scores for all possible triples formed with the given head entities and relations against all entities.
      :rtype: torch.FloatTensor


   .. py:method:: forward_k_vs_all(x: torch.LongTensor) -> torch.FloatTensor

      Computes scores for all entities given a batch of head entities and relations.

      This method is used for K-vs-All scoring, where the model predicts the likelihood of each entity
      being the tail entity in a triple with each head entity and relation pair in the batch.

      :param x: Tensor containing indices for head entities and relations.
      :type x: torch.LongTensor

      :returns: Scores for all entities for each head entity and relation pair in the batch.
      :rtype: torch.FloatTensor


   .. py:method:: forward_k_vs_sample(x: torch.LongTensor, target_entity_idx: torch.LongTensor) -> torch.FloatTensor

      Computes scores for a sampled subset of entities given a batch of head entities and relations.

      This method is particularly useful when the full set of entities is too large to score
      with every batch and only a subset of entities is required.

      :param x: Tensor containing indices for head entities and relations.
      :type x: torch.LongTensor
      :param target_entity_idx: Indices of the target entities against which the scores are to be computed.
      :type target_entity_idx: torch.LongTensor

      :returns: Scores for each head entity and relation pair against the sampled subset of entities.
      :rtype: torch.FloatTensor


   .. py:method:: score(h: torch.FloatTensor, r: torch.FloatTensor, t: torch.FloatTensor) -> torch.FloatTensor

      Computes the score of triples using DistMult's scoring function.

      The scoring function multiplies head entity and relation embeddings, applies dropout and normalization,
      and computes the dot product with the tail entity embeddings.

      :param h: Embedding of the head entity.
      :type h: torch.FloatTensor
      :param r: Embedding of the relation.
      :type r: torch.FloatTensor
      :param t: Embedding of the tail entity.
      :type t: torch.FloatTensor

      :returns: The score of the triple.
      :rtype: torch.FloatTensor



.. py:class:: KeciBase(args)


   Bases: :py:obj:`Keci`

   Without learning dimension scaling


.. py:class:: Keci(args: dict)


   Bases: :py:obj:`dicee.models.base_model.BaseKGE`

   The Keci class is a knowledge graph embedding model that incorporates Clifford algebra for embeddings.
   It supports different dimensions of Clifford algebra by setting the parameters p and q. The class
   utilizes Clifford multiplication for embedding interactions and computes scores for knowledge graph triples.

   :param args: A dictionary of arguments containing hyperparameters and settings for the model.
   :type args: dict

   .. attribute:: name

      The name identifier for the Keci class.

      :type: str

   .. attribute:: p

      The parameter 'p' in Clifford algebra, representing the number of positive square terms.

      :type: int

   .. attribute:: q

      The parameter 'q' in Clifford algebra, representing the number of negative square terms.

      :type: int

   .. attribute:: r

      A derived attribute for dimension scaling based on 'p' and 'q'.

      :type: int

   .. attribute:: p_coefficients

      Embedding for scaling coefficients of 'p' terms, if 'p' > 0.

      :type: torch.nn.Embedding (optional)

   .. attribute:: q_coefficients

      Embedding for scaling coefficients of 'q' terms, if 'q' > 0.

      :type: torch.nn.Embedding (optional)

   .. method:: compute_sigma_pp(hp: torch.Tensor, rp: torch.Tensor) -> torch.Tensor

      Computes the sigma_pp component in Clifford multiplication.

   .. method:: compute_sigma_qq(hq: torch.Tensor, rq: torch.Tensor) -> torch.Tensor

      Computes the sigma_qq component in Clifford multiplication.

   .. method:: compute_sigma_pq(hp: torch.Tensor, hq: torch.Tensor, rp: torch.Tensor, rq: torch.Tensor) -> torch.Tensor

      Computes the sigma_pq component in Clifford multiplication.

   .. method:: apply_coefficients(h0: torch.Tensor, hp: torch.Tensor, hq: torch.Tensor, r0: torch.Tensor, rp: torch.Tensor, rq: torch.Tensor) -> tuple

      Applies scaling coefficients to the base vectors in Clifford algebra.

   .. method:: clifford_multiplication(h0: torch.Tensor, hp: torch.Tensor, hq: torch.Tensor, r0: torch.Tensor, rp: torch.Tensor, rq: torch.Tensor) -> tuple

      Performs Clifford multiplication of head and relation embeddings.

   .. method:: construct_cl_multivector(x: torch.FloatTensor, r: int, p: int, q: int) -> tuple

      Constructs a multivector in Clifford algebra Cl_{p,q}(\mathbb{R}^d).

   .. method:: forward_k_vs_with_explicit(x: torch.Tensor) -> torch.FloatTensor

      Computes scores for a batch of triples against all entities using explicit Clifford multiplication.

   .. method:: k_vs_all_score(bpe_head_ent_emb: torch.Tensor, bpe_rel_ent_emb: torch.Tensor, E: torch.Tensor) -> torch.FloatTensor

      Computes scores for all triples using Clifford multiplication in a K-vs-All setup.

   .. method:: forward_k_vs_all(x: torch.Tensor) -> torch.FloatTensor

      Wrapper function for K-vs-All scoring.

   .. method:: forward_k_vs_sample(x: torch.LongTensor, target_entity_idx: torch.LongTensor) -> torch.FloatTensor

      Computes scores for a sampled subset of entities.

   .. method:: score(h: torch.Tensor, r: torch.Tensor, t: torch.Tensor) -> torch.FloatTensor

      Computes the score for a given triple using Clifford multiplication.

   .. method:: forward_triples(x: torch.Tensor) -> torch.FloatTensor

      Computes scores for a batch of triples.


   .. rubric:: Notes

   The class is designed to work with embeddings in the context of knowledge graph completion tasks,
   leveraging the properties of Clifford algebra for embedding interactions.

   .. py:method:: compute_sigma_pp(hp: torch.Tensor, rp: torch.Tensor) -> torch.Tensor

      Computes the sigma_pp component in Clifford multiplication, representing the interactions
      between the positive square terms in the Clifford algebra.

      sigma_{pp} = \sum_{i=1}^{p-1} \sum_{k=i+1}^p (h_i r_k - h_k r_i) e_i e_k, TODO: Add mathematical format for sphinx.

      sigma_{pp} captures the interactions between along p bases
      For instance, let p e_1, e_2, e_3, we compute interactions between e_1 e_2, e_1 e_3 , and e_2 e_3
      This can be implemented with a nested two for loops

                      results = []
                      for i in range(p - 1):
                          for k in range(i + 1, p):
                              results.append(hp[:, :, i] * rp[:, :, k] - hp[:, :, k] * rp[:, :, i])
                      sigma_pp = torch.stack(results, dim=2)
                      assert sigma_pp.shape == (b, r, int((p * (p - 1)) / 2))

      Yet, this computation would be quite inefficient. Instead, we compute interactions along all p,
      e.g., e1e1, e1e2, e1e3,
            e2e1, e2e2, e2e3,
            e3e1, e3e2, e3e3
      Then select the triangular matrix without diagonals: e1e2, e1e3, e2e3.

      :param hp: The 'p' part of the head entity embedding in Clifford algebra.
      :type hp: torch.Tensor
      :param rp: The 'p' part of the relation embedding in Clifford algebra.
      :type rp: torch.Tensor

      :returns: **sigma_pp** -- The sigma_pp component of the Clifford multiplication.
      :rtype: torch.Tensor


   .. py:method:: compute_sigma_qq(hq: torch.Tensor, rq: torch.Tensor) -> torch.Tensor

      Computes the sigma_qq component in Clifford multiplication, representing the interactions
      between the negative square terms in the Clifford algebra.

      TODO: Add mathematical format for sphinx.

      sigma_{qq} = \sum_{j=1}^{p+q-1} \sum_{k=j+1}^{p+q} (h_j r_k - h_k r_j) e_j e_k
      sigma_{q} captures the interactions between along q bases
      For instance, let q e_1, e_2, e_3, we compute interactions between e_1 e_2, e_1 e_3 , and e_2 e_3
      This can be implemented with a nested two for loops

                      results = []
                      for j in range(q - 1):
                          for k in range(j + 1, q):
                              results.append(hq[:, :, j] * rq[:, :, k] - hq[:, :, k] * rq[:, :, j])
                      sigma_qq = torch.stack(results, dim=2)
                      assert sigma_qq.shape == (b, r, int((q * (q - 1)) / 2))

      Yet, this computation would be quite inefficient. Instead, we compute interactions along all p,
      e.g., e1e1, e1e2, e1e3,
            e2e1, e2e2, e2e3,
            e3e1, e3e2, e3e3
      Then select the triangular matrix without diagonals: e1e2, e1e3, e2e3.

      :param hq: The 'q' part of the head entity embedding in Clifford algebra.
      :type hq: torch.Tensor
      :param rq: The 'q' part of the relation embedding in Clifford algebra.
      :type rq: torch.Tensor

      :returns: **sigma_qq** -- The sigma_qq component of the Clifford multiplication.
      :rtype: torch.Tensor


   .. py:method:: compute_sigma_pq(*, hp: torch.Tensor, hq: torch.Tensor, rp: torch.Tensor, rq: torch.Tensor) -> torch.Tensor

      Computes the sigma_pq component in Clifford multiplication, representing the interactions
      between the positive and negative square terms in the Clifford algebra.

      TODO: Add mathematical format for sphinx.

      \sum_{i=1}^{p} \sum_{j=p+1}^{p+q} (h_i r_j - h_j r_i) e_i e_j

      # results = []
      # sigma_pq = torch.zeros(b, r, p, q)
      # for i in range(p):
      #     for j in range(q):
      #         sigma_pq[:, :, i, j] = hp[:, :, i] * rq[:, :, j] - hq[:, :, j] * rp[:, :, i]
      # print(sigma_pq.shape)

      :param hp: The 'p' part of the head entity embedding in Clifford algebra.
      :type hp: torch.Tensor
      :param hq: The 'q' part of the head entity embedding in Clifford algebra.
      :type hq: torch.Tensor
      :param rp: The 'p' part of the relation embedding in Clifford algebra.
      :type rp: torch.Tensor
      :param rq: The 'q' part of the relation embedding in Clifford algebra.
      :type rq: torch.Tensor

      :returns: **sigma_pq** -- The sigma_pq component of the Clifford multiplication.
      :rtype: torch.Tensor


   .. py:method:: apply_coefficients(h0: torch.Tensor, hp: torch.Tensor, hq: torch.Tensor, r0: torch.Tensor, rp: torch.Tensor, rq: torch.Tensor) -> tuple[torch.FloatTensor, torch.FloatTensor, torch.FloatTensor, torch.FloatTensor, torch.FloatTensor, torch.FloatTensor]

      Applies scaling coefficients to the base vectors in the Clifford algebra.
      This method is used for adjusting the contributions of different components in the algebra.

      :param h0: The scalar part of the head entity embedding.
      :type h0: torch.Tensor
      :param hp: The 'p' part of the head entity embedding.
      :type hp: torch.Tensor
      :param hq: The 'q' part of the head entity embedding.
      :type hq: torch.Tensor
      :param r0: The scalar part of the relation embedding.
      :type r0: torch.Tensor
      :param rp: The 'p' part of the relation embedding.
      :type rp: torch.Tensor
      :param rq: The 'q' part of the relation embedding.
      :type rq: torch.Tensor

      :returns: Tuple containing the scaled components of the head and relation embeddings.
      :rtype: tuple


   .. py:method:: clifford_multiplication(h0: torch.Tensor, hp: torch.Tensor, hq: torch.Tensor, r0: torch.Tensor, rp: torch.Tensor, rq: torch.Tensor) -> tuple[torch.FloatTensor, torch.FloatTensor, torch.FloatTensor, torch.FloatTensor, torch.FloatTensor, torch.FloatTensor]

              Performs Clifford multiplication of head and relation embeddings. This method computes the
              various components of the Clifford product, combining the scalar, 'p', and 'q' parts of the embeddings.

              TODO: Add mathematical format for sphinx.

              h = h_0 + \sum_{i=1}^p h_i e_i + \sum_{j=p+1}^{p+q} h_j e_j
              r = r_0 + \sum_{i=1}^p r_i e_i + \sum_{j=p+1}^{p+q} r_j e_j

              ei ^2 = +1     for i =< i =< p
              ej ^2 = -1     for p < j =< p+q
              ei ej = -eje1  for i
      eq j

              h r =   sigma_0 + sigma_p + sigma_q + sigma_{pp} + sigma_{q}+ sigma_{pq}
              where
                      (1) sigma_0 = h_0 r_0 + \sum_{i=1}^p (h_0 r_i) e_i - \sum_{j=p+1}^{p+q} (h_j r_j) e_j

                      (2) sigma_p = \sum_{i=1}^p (h_0 r_i + h_i r_0) e_i

                      (3) sigma_q = \sum_{j=p+1}^{p+q} (h_0 r_j + h_j r_0) e_j

                      (4) sigma_{pp} = \sum_{i=1}^{p-1} \sum_{k=i+1}^p (h_i r_k - h_k r_i) e_i e_k

                      (5) sigma_{qq} = \sum_{j=1}^{p+q-1} \sum_{k=j+1}^{p+q} (h_j r_k - h_k r_j) e_j e_k

                      (6) sigma_{pq} = \sum_{i=1}^{p} \sum_{j=p+1}^{p+q} (h_i r_j - h_j r_i) e_i e_j

              Parameters
              ----------
              h0 : torch.Tensor
                  The scalar part of the head entity embedding.
              hp : torch.Tensor
                  The 'p' part of the head entity embedding.
              hq : torch.Tensor
                  The 'q' part of the head entity embedding.
              r0 : torch.Tensor
                  The scalar part of the relation embedding.
              rp : torch.Tensor
                  The 'p' part of the relation embedding.
              rq : torch.Tensor
                  The 'q' part of the relation embedding.

              Returns
              -------
              tuple
                  Tuple containing the components of the Clifford product.




   .. py:method:: construct_cl_multivector(x: torch.FloatTensor, r: int, p: int, q: int) -> tuple[torch.FloatTensor, torch.FloatTensor, torch.FloatTensor]

      Construct a batch of multivectors Cl_{p,q}(\mathbb{R}^d)

      Parameter
      ---------
      x : torch.FloatTensor
          The embedding vector with shape (n, d).
      r : int
          The dimension of the scalar part.
      p : int
          The number of positive square terms.
      q : int
          The number of negative square terms.

      :returns: * **a0** (*torch.FloatTensor*) -- Tensor with (n,r) shape
                * **ap** (*torch.FloatTensor*) -- Tensor with (n,r,p) shape
                * **aq** (*torch.FloatTensor*) -- Tensor with (n,r,q) shape


   .. py:method:: forward_k_vs_with_explicit(x: torch.Tensor) -> torch.FloatTensor

      Computes scores for a batch of triples against all entities using explicit Clifford multiplication.
      This method is used for K-vs-All training and evaluation.

      :param x: Tensor representing a batch of head entities and relations.
      :type x: torch.Tensor

      :returns: A tensor containing scores for each triple against all entities.
      :rtype: torch.FloatTensor


   .. py:method:: k_vs_all_score(bpe_head_ent_emb: torch.Tensor, bpe_rel_ent_emb: torch.Tensor, E: torch.Tensor) -> torch.FloatTensor

      Computes scores for all triples using Clifford multiplication in a K-vs-All setup. This method involves constructing
      multivectors for head entities and relations in Clifford algebra, applying coefficients, and computing interaction
      scores based on different components of the Clifford algebra.

      :param bpe_head_ent_emb: Batch of head entity embeddings in BPE (Byte Pair Encoding) format. Tensor shape: (batch_size, embedding_dim).
      :type bpe_head_ent_emb: torch.Tensor
      :param bpe_rel_ent_emb: Batch of relation embeddings in BPE format. Tensor shape: (batch_size, embedding_dim).
      :type bpe_rel_ent_emb: torch.Tensor
      :param E: Tensor containing all entity embeddings. Tensor shape: (num_entities, embedding_dim).
      :type E: torch.Tensor

      :returns: Tensor containing the scores for each triple in the K-vs-All setting. Tensor shape: (batch_size, num_entities).
      :rtype: torch.FloatTensor

      .. rubric:: Notes

      The method computes scores based on the basis of 1 (scalar part), the bases of 'p' (positive square terms),
      and the bases of 'q' (negative square terms). Additional computations involve sigma_pp, sigma_qq, and sigma_pq
      components in Clifford multiplication, corresponding to different interaction terms.


   .. py:method:: forward_k_vs_all(x: torch.Tensor) -> torch.FloatTensor

      TODO: Add mathematical format for sphinx.
      Performs the forward pass for K-vs-All training and evaluation in knowledge graph embeddings.
      This method involves retrieving real-valued embedding vectors for head entities and relations \mathbb{R}^d,
      constructing Clifford algebra multivectors for these embeddings according to Cl_{p,q}(\mathbb{R}^d), performing Clifford multiplication,
      and computing the inner product with all entity embeddings.

      :param x: A tensor representing a batch of head entities and relations for the K-vs-All evaluation.
                Expected tensor shape: (n, 2), where 'n' is the batch size and '2' represents head entity
                and relation pairs.
      :type x: torch.Tensor

      :returns: A tensor containing the scores for each head entity and relation pair against all possible
                tail entities in the knowledge graph. Tensor shape: (n, |E|), where '|E|' is the number
                of entities in the knowledge graph.
      :rtype: torch.FloatTensor

      .. rubric:: Notes

      This method is similar to the 'forward_k_vs_with_explicit' function in functionality. It is
      typically used in scenarios where every possible combination of a head entity and a relation
      is scored against all tail entities, commonly used in knowledge graph completion tasks.


   .. py:method:: forward_k_vs_sample(x: torch.LongTensor, target_entity_idx: torch.LongTensor) -> torch.FloatTensor

      TODO: Add mathematical format for sphinx.

      Performs the forward pass for K-vs-Sample training in knowledge graph embeddings. This method involves
      retrieving real-valued embedding vectors for head entities and relations \mathbb{R}^d, constructing Clifford algebra
      multivectors for these embeddings according to Cl_{p,q}(\mathbb{R}^d), performing Clifford multiplication,
      and computing the inner product with a sampled subset of entity embeddings.

      :param x: A tensor representing a batch of head entities and relations for the K-vs-Sample evaluation.
                Expected tensor shape: (n, 2), where 'n' is the batch size and '2' represents head entity
                and relation pairs.
      :type x: torch.LongTensor
      :param target_entity_idx: A tensor of target entity indices for sampling in the K-vs-Sample evaluation.
                                Tensor shape: (n, sample_size), where 'sample_size' is the number of entities sampled.
      :type target_entity_idx: torch.LongTensor

      :returns: A tensor containing the scores for each head entity and relation pair against the sampled
                subset of tail entities. Tensor shape: (n, sample_size).
      :rtype: torch.FloatTensor

      .. rubric:: Notes

      This method is used in scenarios where every possible combination of a head entity and a relation
      is scored against a sampled subset of tail entities, commonly used in knowledge graph completion tasks
      with a large number of entities.


   .. py:method:: score(h: torch.Tensor, r: torch.Tensor, t: torch.Tensor) -> torch.FloatTensor

      Computes the score for a given triple using Clifford multiplication in the context of knowledge graph embeddings.
      This method involves constructing Clifford algebra multivectors for head entities, relations, and tail entities,
      applying coefficients, and computing interaction scores based on different components of the Clifford algebra.

      :param h: Tensor representing the embeddings of head entities. Expected shape: (n, d), where 'n' is the number of triples
                and 'd' is the embedding dimension.
      :type h: torch.Tensor
      :param r: Tensor representing the embeddings of relations. Expected shape: (n, d).
      :type r: torch.Tensor
      :param t: Tensor representing the embeddings of tail entities. Expected shape: (n, d).
      :type t: torch.Tensor

      :returns: Tensor containing the scores for each triple. Tensor shape: (n,).
      :rtype: torch.FloatTensor

      .. rubric:: Notes

      The method computes scores based on the scalar part, the bases of 'p' (positive square terms),
      and the bases of 'q' (negative square terms) in Clifford algebra. It includes additional computations
      involving sigma_pp, sigma_qq, and sigma_pq components, which correspond to different interaction terms
      in the Clifford product.


   .. py:method:: forward_triples(x: torch.Tensor) -> torch.FloatTensor

      Computes scores for a batch of triples using Clifford multiplication.
      This method is involved in the forward pass of the model during training or evaluation.
      It retrieves embeddings for head entities, relations, and tail entities, constructs Clifford algebra multivectors,
      applies coefficients, and computes interaction scores based on different components of Clifford algebra.

      :param x: A tensor representing a batch of triples. Each triple consists of indices for a head entity, a relation, and a tail entity.
                Expected tensor shape: (n, 3), where 'n' is the number of triples.
      :type x: torch.Tensor

      :returns: A tensor containing the scores for each triple in the batch. Tensor shape: (n,), where 'n' is the number of triples.
      :rtype: torch.FloatTensor

      .. rubric:: Notes

      The method computes scores based on the scalar part, the bases of 'p' (positive square terms), and the bases of 'q' (negative square terms) in Clifford algebra.
      It includes additional computations involving sigma_pp, sigma_qq, and sigma_pq components, corresponding to different interaction terms in the Clifford product.



.. py:class:: TransE(args)


   Bases: :py:obj:`dicee.models.base_model.BaseKGE`

   TransE model for learning embeddings in multi-relational data. It is based on the idea of translating
   embeddings for head entities by the relation vector to approach the tail entity embeddings in the embedding space.

   This implementation of TransE is based on the paper:
   'Translating Embeddings for Modeling Multi-relational Data'
   (https://proceedings.neurips.cc/paper/2013/file/1cecc7a77928ca8133fa24680a88d2f9-Paper.pdf).

   .. attribute:: name

      The name identifier for the TransE model.

      :type: str

   .. attribute:: _norm

      The norm used for computing pairwise distances in the embedding space.

      :type: int

   .. attribute:: margin

      The margin value used in the scoring function.

      :type: int

   .. method:: score(head_ent_emb: torch.Tensor, rel_ent_emb: torch.Tensor, tail_ent_emb: torch.Tensor) -> torch.Tensor

      Computes the score of triples using the TransE scoring function.


   .. method:: forward_k_vs_all(x: torch.Tensor) -> torch.FloatTensor

      Computes scores for all entities given a head entity and a relation.


   .. py:method:: score(head_ent_emb: torch.Tensor, rel_ent_emb: torch.Tensor, tail_ent_emb: torch.Tensor) -> torch.Tensor

      Computes the score of triples using the TransE scoring function.

      The scoring function computes the L2 distance between the translated head entity
      and the tail entity embeddings and subtracts this distance from the margin.

      :param head_ent_emb: Embedding of the head entity.
      :type head_ent_emb: torch.Tensor
      :param rel_ent_emb: Embedding of the relation.
      :type rel_ent_emb: torch.Tensor
      :param tail_ent_emb: Embedding of the tail entity.
      :type tail_ent_emb: torch.Tensor

      :returns: The score of the triple.
      :rtype: torch.Tensor


   .. py:method:: forward_k_vs_all(x: torch.Tensor) -> torch.FloatTensor

      Computes scores for all entities given a head entity and a relation.

      This method is used for K-vs-All scoring, where the model predicts the likelihood of each entity
      being the tail entity in a triple with each head entity and relation.

      :param x: Tensor containing indices for head entities and relations.
      :type x: torch.Tensor

      :returns: Scores for all entities for each head entity and relation pair.
      :rtype: torch.FloatTensor



.. py:class:: DeCaL(args)


   Bases: :py:obj:`dicee.models.base_model.BaseKGE`

   Base class for all neural network modules.

   Your models should also subclass this class.

   Modules can also contain other Modules, allowing to nest them in
   a tree structure. You can assign the submodules as regular attributes::

       import torch.nn as nn
       import torch.nn.functional as F

       class Model(nn.Module):
           def __init__(self):
               super().__init__()
               self.conv1 = nn.Conv2d(1, 20, 5)
               self.conv2 = nn.Conv2d(20, 20, 5)

           def forward(self, x):
               x = F.relu(self.conv1(x))
               return F.relu(self.conv2(x))

   Submodules assigned in this way will be registered, and will have their
   parameters converted too when you call :meth:`to`, etc.

   .. note::
       As per the example above, an ``__init__()`` call to the parent class
       must be made before assignment on the child.

   :ivar training: Boolean represents whether this module is in training or
                   evaluation mode.
   :vartype training: bool

   .. py:method:: forward_triples(x: torch.Tensor) -> torch.FloatTensor

      Parameter
      ---------
      x: torch.LongTensor with (n,3) shape

      :rtype: torch.FloatTensor with (n) shape


   .. py:method:: cl_pqr(a)

      Input: tensor(batch_size, emb_dim) ----> output: tensor with 1+p+q+r components with size (batch_size, emb_dim/(1+p+q+r)) each.

      1) takes a tensor of size (batch_size, emb_dim), split it into 1 + p + q +r components, hence 1+p+q+r must be a divisor
      of the emb_dim.
      2) Return a list of the 1+p+q+r components vectors, each are tensors of size (batch_size, emb_dim/(1+p+q+r))


   .. py:method:: compute_sigmas_single(list_h_emb, list_r_emb, list_t_emb)

      here we compute all the sums with no others vectors interaction taken with the scalar product with t, that is,
      1) s0 = h_0r_0t_0
      2) s1 = \sum_{i=1}^{p}h_ir_it_0
      3) s2 = \sum_{j=p+1}^{p+q}h_jr_jt_0
      4) s3 = \sum_{i=1}^{q}(h_0r_it_i + h_ir_0t_i)
      5) s4 = \sum_{i=p+1}^{p+q}(h_0r_it_i + h_ir_0t_i)
      5) s5 = \sum_{i=p+q+1}^{p+q+r}(h_0r_it_i + h_ir_0t_i)

      and return:

      *) sigma_0t = \sigma_0 \cdot t_0 = s0 + s1 -s2
      *) s3, s4 and s5


   .. py:method:: compute_sigmas_multivect(list_h_emb, list_r_emb)

      Here we compute and return all the sums with vectors interaction for the same and different bases.

      For same bases vectors interaction we have

      1) \sigma_pp = \sum_{i=1}^{p-1}\sum_{i'=i+1}^{p}(h_ir_{i'}-h_{i'}r_i) (models the interactions between e_i and e_i' for 1 <= i, i' <= p)
      2) \sigma_qq = \sum_{j=p+1}^{p+q-1}\sum_{j'=j+1}^{p+q}(h_jr_{j'}-h_{j'} (models the interactions between e_j and e_j' for p+1 <= j, j' <= p+q)
      3) \sigma_rr = \sum_{k=p+q+1}^{p+q+r-1}\sum_{k'=k+1}^{p}(h_kr_{k'}-h_{k'}r_k) (models the interactions between e_k and e_k' for p+q+1 <= k, k' <= p+q+r)

      For different base vector interactions, we have

      4) \sigma_pq = \sum_{i=1}^{p}\sum_{j=p+1}^{p+q}(h_ir_j - h_jr_i) (interactionsn between e_i and e_j for 1<=i <=p and p+1<= j <= p+q)
      5) \sigma_pr = \sum_{i=1}^{p}\sum_{k=p+q+1}^{p+q+r}(h_ir_k - h_kr_i) (interactionsn between e_i and e_k for 1<=i <=p and p+q+1<= k <= p+q+r)
      6) \sigma_qr = \sum_{j=p+1}^{p+q}\sum_{j=p+q+1}^{p+q+r}(h_jr_k - h_kr_j) (interactionsn between e_j and e_k for p+1 <= j <=p+q and p+q+1<= j <= p+q+r)



   .. py:method:: forward_k_vs_all(x: torch.Tensor) -> torch.FloatTensor

      Kvsall training

      (1) Retrieve real-valued embedding vectors for heads and relations \mathbb{R}^d .
      (2) Construct head entity and relation embeddings according to Cl_{p,q}(\mathbb{R}^d) .
      (3) Perform Cl multiplication
      (4) Inner product of (3) and all entity embeddings

      forward_k_vs_with_explicit and this funcitons are identical
      Parameter
      ---------
      x: torch.LongTensor with (n,2) shape
      :rtype: torch.FloatTensor with (n, |E|) shape


   .. py:method:: apply_coefficients(h0, hp, hq, hk, r0, rp, rq, rk)

      Multiplying a base vector with its scalar coefficient


   .. py:method:: construct_cl_multivector(x: torch.FloatTensor, re: int, p: int, q: int, r: int) -> tuple[torch.FloatTensor, torch.FloatTensor, torch.FloatTensor]

      Construct a batch of multivectors Cl_{p,q,r}(\mathbb{R}^d)

      Parameter
      ---------
      x: torch.FloatTensor with (n,d) shape

      :returns: * **a0** (*torch.FloatTensor*)
                * **ap** (*torch.FloatTensor*)
                * **aq** (*torch.FloatTensor*)
                * **ar** (*torch.FloatTensor*)


   .. py:method:: compute_sigma_pp(hp, rp)

      \sigma_{p,p}^* = \sum_{i=1}^{p-1}\sum_{i'=i+1}^{p}(x_iy_{i'}-x_{i'}y_i)

      sigma_{pp} captures the interactions between along p bases
      For instance, let p e_1, e_2, e_3, we compute interactions between e_1 e_2, e_1 e_3 , and e_2 e_3
      This can be implemented with a nested two for loops

                      results = []
                      for i in range(p - 1):
                          for k in range(i + 1, p):
                              results.append(hp[:, :, i] * rp[:, :, k] - hp[:, :, k] * rp[:, :, i])
                      sigma_pp = torch.stack(results, dim=2)
                      assert sigma_pp.shape == (b, r, int((p * (p - 1)) / 2))

      Yet, this computation would be quite inefficient. Instead, we compute interactions along all p,
      e.g., e1e1, e1e2, e1e3,
            e2e1, e2e2, e2e3,
            e3e1, e3e2, e3e3
      Then select the triangular matrix without diagonals: e1e2, e1e3, e2e3.


   .. py:method:: compute_sigma_qq(hq, rq)

      Compute  \sigma_{q,q}^* = \sum_{j=p+1}^{p+q-1}\sum_{j'=j+1}^{p+q}(x_jy_{j'}-x_{j'}y_j) Eq. 16
      sigma_{q} captures the interactions between along q bases
      For instance, let q e_1, e_2, e_3, we compute interactions between e_1 e_2, e_1 e_3 , and e_2 e_3
      This can be implemented with a nested two for loops

                      results = []
                      for j in range(q - 1):
                          for k in range(j + 1, q):
                              results.append(hq[:, :, j] * rq[:, :, k] - hq[:, :, k] * rq[:, :, j])
                      sigma_qq = torch.stack(results, dim=2)
                      assert sigma_qq.shape == (b, r, int((q * (q - 1)) / 2))

      Yet, this computation would be quite inefficient. Instead, we compute interactions along all p,
      e.g., e1e1, e1e2, e1e3,
            e2e1, e2e2, e2e3,
            e3e1, e3e2, e3e3
      Then select the triangular matrix without diagonals: e1e2, e1e3, e2e3.


   .. py:method:: compute_sigma_rr(hk, rk)

      \sigma_{r,r}^* = \sum_{k=p+q+1}^{p+q+r-1}\sum_{k'=k+1}^{p}(x_ky_{k'}-x_{k'}y_k)



   .. py:method:: compute_sigma_pq(*, hp, hq, rp, rq)

      \sum_{i=1}^{p} \sum_{j=p+1}^{p+q} (h_i r_j - h_j r_i) e_i e_j

      results = []
      sigma_pq = torch.zeros(b, r, p, q)
      for i in range(p):
          for j in range(q):
              sigma_pq[:, :, i, j] = hp[:, :, i] * rq[:, :, j] - hq[:, :, j] * rp[:, :, i]
      print(sigma_pq.shape)



   .. py:method:: compute_sigma_pr(*, hp, hk, rp, rk)

      \sum_{i=1}^{p} \sum_{j=p+1}^{p+q} (h_i r_j - h_j r_i) e_i e_j

      results = []
      sigma_pq = torch.zeros(b, r, p, q)
      for i in range(p):
          for j in range(q):
              sigma_pq[:, :, i, j] = hp[:, :, i] * rq[:, :, j] - hq[:, :, j] * rp[:, :, i]
      print(sigma_pq.shape)



   .. py:method:: compute_sigma_qr(*, hq, hk, rq, rk)

      \sum_{i=1}^{p} \sum_{j=p+1}^{p+q} (h_i r_j - h_j r_i) e_i e_j

      results = []
      sigma_pq = torch.zeros(b, r, p, q)
      for i in range(p):
          for j in range(q):
              sigma_pq[:, :, i, j] = hp[:, :, i] * rq[:, :, j] - hq[:, :, j] * rp[:, :, i]
      print(sigma_pq.shape)




.. py:class:: ComplEx(args: dict)


   Bases: :py:obj:`dicee.models.base_model.BaseKGE`

   ComplEx (Complex Embeddings for Knowledge Graphs) is a model that extends
   the base knowledge graph embedding approach by using complex-valued embeddings.
   It emphasizes the interaction of real and imaginary components of embeddings
   to capture the asymmetric relationships often found in knowledge graphs.

   :param args: A dictionary of arguments containing hyperparameters and settings for the model,
                such as embedding dimensions, learning rate, and regularization methods.
   :type args: dict

   .. attribute:: name

      The name identifier for the ComplEx model.

      :type: str

   .. method:: score(head_ent_emb: torch.FloatTensor, rel_ent_emb: torch.FloatTensor,

        tail_ent_emb: torch.FloatTensor) -> torch.FloatTensor
      Computes the score of a triple using the ComplEx scoring function.


   .. method:: k_vs_all_score(emb_h: torch.FloatTensor, emb_r: torch.FloatTensor,

                 emb_E: torch.FloatTensor) -> torch.FloatTensor
      Computes scores in a K-vs-All setting using complex-valued embeddings.


   .. method:: forward_k_vs_all(x: torch.LongTensor) -> torch.FloatTensor

      Performs a forward pass for K-vs-All scoring, returning scores for all entities.


   .. rubric:: Notes

   ComplEx is particularly suited for modeling asymmetric relations and has been
   shown to perform well on various knowledge graph benchmarks. The use of complex
   numbers allows the model to encode additional information compared to real-valued models.

   .. py:method:: score(head_ent_emb: torch.FloatTensor, rel_ent_emb: torch.FloatTensor, tail_ent_emb: torch.FloatTensor) -> torch.FloatTensor
      :staticmethod:

      Compute the scoring function for a given triple using complex-valued embeddings.

      :param head_ent_emb: The complex embedding of the head entity.
      :type head_ent_emb: torch.FloatTensor
      :param rel_ent_emb: The complex embedding of the relation.
      :type rel_ent_emb: torch.FloatTensor
      :param tail_ent_emb: The complex embedding of the tail entity.
      :type tail_ent_emb: torch.FloatTensor

      :returns: The score of the triple calculated using the Hermitian dot product of complex embeddings.
      :rtype: torch.FloatTensor

      .. rubric:: Notes

      The scoring function exploits the complex vector space to model the interactions
      between entities and relations. It involves element-wise multiplication and
      summation of real and imaginary parts.


   .. py:method:: k_vs_all_score(emb_h: torch.FloatTensor, emb_r: torch.FloatTensor, emb_E: torch.FloatTensor) -> torch.FloatTensor
      :staticmethod:

      Compute scores for a head entity and relation against all entities in a K-vs-All scenario.

      :param emb_h: The complex embedding of the head entity.
      :type emb_h: torch.FloatTensor
      :param emb_r: The complex embedding of the relation.
      :type emb_r: torch.FloatTensor
      :param emb_E: The complex embeddings of all possible tail entities.
      :type emb_E: torch.FloatTensor

      :returns: Scores for all possible triples formed with the given head entity and relation.
      :rtype: torch.FloatTensor

      .. rubric:: Notes

      This method is useful for tasks like link prediction where the model predicts
      the likelihood of a relation between a given entity pair.


   .. py:method:: forward_k_vs_all(x: torch.LongTensor) -> torch.FloatTensor

      Perform a forward pass for K-vs-all scoring using complex-valued embeddings.

      :param x: Tensor containing indices for head entities and relations.
      :type x: torch.LongTensor

      :returns: Scores for all triples formed with the given head entities and relations against all entities.
      :rtype: torch.FloatTensor

      .. rubric:: Notes

      This method is typically used in training and evaluation of the model in a
      link prediction setting, where the goal is to rank all possible tail entities
      for a given head entity and relation.



.. py:class:: AConEx(args)


   Bases: :py:obj:`dicee.models.base_model.BaseKGE`

   AConEx (Additive Convolutional ComplEx) extends the ConEx model by incorporating
   additive connections in the convolutional operations. This model integrates
   convolutional neural networks with complex-valued embeddings, emphasizing
   additive feature interactions for knowledge graph embeddings.

   :param args: A dictionary of arguments containing hyperparameters and settings for the model,
                such as embedding dimensions, kernel size, number of output channels, and dropout rates.
   :type args: dict

   .. attribute:: name

      The name identifier for the AConEx model.

      :type: str

   .. attribute:: conv2d

      A 2D convolutional layer used for processing complex-valued embeddings.

      :type: torch.nn.Conv2d

   .. attribute:: fc_num_input

      The number of input features for the fully connected layer.

      :type: int

   .. attribute:: fc1

      A fully connected linear layer for compressing the output of the
      convolutional layer.

      :type: torch.nn.Linear

   .. attribute:: norm_fc1

      Normalization layer applied after the fully connected layer.

      :type: Normalizer

   .. attribute:: bn_conv2d

      Batch normalization layer applied after the convolutional operation.

      :type: torch.nn.BatchNorm2d

   .. attribute:: feature_map_dropout

      Dropout layer applied to the output of the convolutional layer.

      :type: torch.nn.Dropout2d

   .. method:: residual_convolution(C_1: Tuple[torch.Tensor, torch.Tensor],

                       C_2: Tuple[torch.Tensor, torch.Tensor]) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]
      Performs a residual convolution operation on two complex-valued embeddings.

   .. method:: forward_k_vs_all(x: torch.Tensor) -> torch.FloatTensor

      Computes scores in a K-vs-All setting using convolutional operations on embeddings.

   .. method:: forward_triples(x: torch.Tensor) -> torch.FloatTensor

      Computes scores for a batch of triples using convolutional operations.

   .. method:: forward_k_vs_sample(x: torch.Tensor, target_entity_idx: torch.Tensor)

      Computes scores against a sampled subset of entities using convolutional operations.


   .. rubric:: Notes

   AConEx aims to enhance the modeling capabilities of knowledge graph embeddings
   by adding more complex interaction patterns through convolutional layers, potentially
   improving performance on tasks like link prediction.

   .. py:method:: residual_convolution(C_1: Tuple[torch.Tensor, torch.Tensor], C_2: Tuple[torch.Tensor, torch.Tensor]) -> Tuple[torch.FloatTensor, torch.FloatTensor, torch.FloatTensor, torch.FloatTensor]

      Computes the residual convolution of two complex-valued embeddings. This method
      is a core part of the AConEx model, applying convolutional neural network techniques
      to complex-valued embeddings to capture intricate relationships in the data.

      :param C_1: A tuple of two PyTorch tensors representing the real and imaginary components
                  of the first complex-valued embedding.
      :type C_1: Tuple[torch.Tensor, torch.Tensor]
      :param C_2: A tuple of two PyTorch tensors representing the real and imaginary components
                  of the second complex-valued embedding.
      :type C_2: Tuple[torch.Tensor, torch.Tensor]

      :returns: A tuple of four tensors, each representing a component of the convolutionally
                transformed embeddings. These components correspond to the modified real
                and imaginary parts of the input embeddings.
      :rtype: Tuple[torch.FloatTensor, torch.FloatTensor, torch.FloatTensor, torch.FloatTensor]

      .. rubric:: Notes

      The method concatenates the real and imaginary components of the embeddings and
      applies a 2D convolution, followed by batch normalization, ReLU activation, dropout,
      and a fully connected layer. This convolutional process is designed to enhance
      the model's ability to capture complex patterns in knowledge graph embeddings.


   .. py:method:: forward_k_vs_all(x: torch.Tensor) -> torch.FloatTensor

      Computes scores in a K-vs-All setting using convolutional and additive operations on
      complex-valued embeddings. This method evaluates the performance of the model by computing
      scores for each head entity and relation pair against all possible tail entities.

      :param x: A tensor representing a batch of head entities and relations. Expected tensor shape:
                (batch_size, 2), where 'batch_size' is the number of head entity and relation pairs.
      :type x: torch.Tensor

      :returns: A tensor containing the scores for each head entity and relation pair against all possible
                tail entities. Tensor shape: (batch_size, |E|), where '|E|' is the number of entities
                in the knowledge graph.
      :rtype: torch.FloatTensor

      .. rubric:: Notes

      The method first retrieves embeddings for head entities and relations, splits them into real
      and imaginary parts, and applies a convolutional operation. It then computes the Hermitian
      inner product with all tail entity embeddings, using an additive approach that combines the
      convolutional results with the original embeddings. This technique aims to capture complex
      relational patterns in the knowledge graph.


   .. py:method:: forward_triples(x: torch.Tensor) -> torch.FloatTensor

      Computes scores for a batch of triples using convolutional operations and additive connections
      on complex-valued embeddings. This method is key for evaluating the model's performance on
      individual triples within the knowledge graph.

      :param x: A tensor representing a batch of triples. Each triple consists of indices for a head entity,
                a relation, and a tail entity. Expected tensor shape: (n, 3), where 'n' is the number of triples.
      :type x: torch.Tensor

      :returns: A tensor containing the scores for each triple in the batch. Tensor shape: (n,), where 'n'
                is the number of triples.
      :rtype: torch.FloatTensor

      .. rubric:: Notes

      The method retrieves embeddings for head entities, relations, and tail entities, and splits them
      into real and imaginary parts. It then applies a convolution operation on these embeddings and
      computes the Hermitian inner product, enhanced with an additive connection. This approach allows
      the model to capture complex relational patterns within the knowledge graph, potentially improving
      prediction accuracy and interpretability.


   .. py:method:: forward_k_vs_sample(x: torch.Tensor, target_entity_idx: torch.Tensor) -> torch.FloatTensor

      Computes scores for a batch of samples (entity pairs) given a batch of queries. This method is used
      to predict the scores for different tail entities for a set of query triples.

      :param x: A tensor representing a batch of query triples. Each triple consists of indices for a head entity,
                a relation, and a dummy tail entity (used for scoring). Expected tensor shape: (n, 3), where 'n' is
                the number of query triples.
      :type x: torch.Tensor
      :param target_entity_idx: A tensor containing the indices of the target tail entities for which scores are to be predicted.
                                Expected tensor shape: (n, m), where 'n' is the number of queries and 'm' is the number of target
                                entities.
      :type target_entity_idx: torch.Tensor

      :returns: A tensor containing the scores for each query-triple and target-entity pair. Tensor shape: (n, m),
                where 'n' is the number of queries and 'm' is the number of target entities.
      :rtype: torch.FloatTensor

      .. rubric:: Notes

      This method retrieves embeddings for the head entities and relations in the query triples, splits them
      into real and imaginary parts, and applies convolutional operations with additive connections to capture
      complex patterns. It also retrieves embeddings for the target tail entities and computes Hermitian inner
      products to obtain scores, allowing the model to rank the tail entities based on their relevance to the queries.



.. py:class:: AConvO(args: dict)


   Bases: :py:obj:`dicee.models.base_model.BaseKGE`

   Additive Convolutional Octonion(AConvO) extends the base knowledge graph embedding model by integrating additive convolutional
   operations with octonion algebra. This model applies convolutional neural networks to octonion-based
   embeddings, capturing complex interactions in knowledge graphs.

   :param args: A dictionary of arguments containing hyperparameters and settings for the model,
                such as embedding dimensions, number of output channels, kernel size, and dropout rates.
   :type args: dict

   .. attribute:: name

      The name identifier for the AConvO model.

      :type: str

   .. attribute:: conv2d

      A 2D convolutional layer used for processing octonion-based embeddings.

      :type: torch.nn.Conv2d

   .. attribute:: fc_num_input

      The number of input features for the fully connected layer.

      :type: int

   .. attribute:: fc1

      A fully connected linear layer for compressing the output of the convolutional layer.

      :type: torch.nn.Linear

   .. attribute:: bn_conv2d

      Batch normalization layer applied after the convolutional operation.

      :type: torch.nn.BatchNorm2d

   .. attribute:: norm_fc1

      Normalization layer applied after the fully connected layer.

      :type: Normalizer

   .. attribute:: feature_map_dropout

      Dropout layer applied to the output of the convolutional layer.

      :type: torch.nn.Dropout2d

   .. method:: octonion_normalizer(emb_rel_e0: torch.Tensor, emb_rel_e1: torch.Tensor, ..., emb_rel_e7: torch.Tensor) -> Tuple[torch.Tensor, ...]

      Normalizes octonion components to unit length.


   .. method:: residual_convolution(self, O_1: Tuple[torch.Tensor, ...], O_2: Tuple[torch.Tensor, ...]) -> Tuple[torch.Tensor, ...]

      Performs a residual convolution operation on two octonion embeddings.


   .. method:: forward_triples(x: torch.Tensor) -> torch.Tensor

      Computes scores for a batch of triples using convolutional operations.


   .. method:: forward_k_vs_all(x: torch.Tensor)

      Computes scores against a sampled subset of entities using convolutional operations.


   .. rubric:: Notes

   AConvO aims to enhance the modeling capabilities of knowledge graph embeddings by
   adding more complex interaction patterns through convolutional layers, potentially
   improving performance on tasks like link prediction.

   .. py:method:: octonion_normalizer(emb_rel_e0: torch.Tensor, emb_rel_e1: torch.Tensor, emb_rel_e2: torch.Tensor, emb_rel_e3: torch.Tensor, emb_rel_e4: torch.Tensor, emb_rel_e5: torch.Tensor, emb_rel_e6: torch.Tensor, emb_rel_e7: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]
      :staticmethod:

      Normalizes the components of an octonion to unit length.

      Each component of the octonion is divided by the square root of the sum of
      the squares of all components.

      :param emb_rel_e0: The eight components of an octonion.
      :type emb_rel_e0: torch.Tensor
      :param emb_rel_e1: The eight components of an octonion.
      :type emb_rel_e1: torch.Tensor
      :param ...: The eight components of an octonion.
      :type ...: torch.Tensor
      :param emb_rel_e7: The eight components of an octonion.
      :type emb_rel_e7: torch.Tensor

      :returns: The normalized components of the octonion.
      :rtype: Tuple[torch.Tensor, ...]


   .. py:method:: residual_convolution(O_1: Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor], O_2: Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]

      Performs a residual convolution operation on two sets of octonion embeddings.

      The method combines two octonion embeddings and applies a convolutional operation
      followed by batch normalization, dropout, and a fully connected layer.

      :param O_1: The first set of octonion embeddings.
      :type O_1: Tuple[torch.Tensor, ...]
      :param O_2: The second set of octonion embeddings.
      :type O_2: Tuple[torch.Tensor, ...]

      :returns: The resulting octonion embeddings after the convolutional operation.
      :rtype: Tuple[torch.Tensor, ...]


   .. py:method:: forward_triples(x: torch.Tensor) -> torch.Tensor

      Computes scores for a batch of triples using convolutional operations.

      The method processes head, relation, and tail embeddings using convolutional
      layers and computes the scores of the triples.

      :param x: Tensor containing indices for head entities, relations, and tail entities.
      :type x: torch.Tensor

      :returns: Scores for the given batch of triples.
      :rtype: torch.Tensor


   .. py:method:: forward_k_vs_all(x: torch.Tensor) -> torch.Tensor

      Compute scores for a head entity and a relation (h,r) against all entities in the knowledge graph.

      Given a head entity and a relation (h, r), this method computes scores for (h, r, x) for all entities x in the knowledge graph.

      :param x: A tensor containing indices for head entities and relations.
      :type x: torch.Tensor

      :returns: A tensor of scores representing the compatibility of (h, r, x) for all entities x in the knowledge graph.
      :rtype: torch.Tensor

      .. rubric:: Notes

      This method supports batch processing, allowing the input tensor `x` to contain multiple head entities and relations.

      The scores indicate how well each entity x in the knowledge graph fits the (h, r) pattern, with higher scores indicating better compatibility.



.. py:class:: AConvQ(args)


   Bases: :py:obj:`dicee.models.base_model.BaseKGE`

   Additive Convolutional Quaternion Knowledge Graph Embeddings (AConvQ) model integrates
   quaternion algebra with convolutional neural networks for knowledge graph embeddings.
   This model is designed to capture complex interactions in knowledge graphs by applying
   additive convolutions to quaternion-based entity and relation embeddings.

   .. attribute:: name

      The name identifier for the AConvQ model.

      :type: str

   .. attribute:: entity_embeddings

      Embedding layer for entities in the knowledge graph.

      :type: torch.nn.Embedding

   .. attribute:: relation_embeddings

      Embedding layer for relations in the knowledge graph.

      :type: torch.nn.Embedding

   .. attribute:: conv2d

      A 2D convolutional layer used for processing quaternion embeddings.

      :type: torch.nn.Conv2d

   .. attribute:: fc_num_input

      The number of input features for the fully connected layer.

      :type: int

   .. attribute:: fc1

      A fully connected linear layer for compressing the output of the convolutional layer.

      :type: torch.nn.Linear

   .. attribute:: bn_conv1

      Batch normalization layer applied after the convolutional operation.

      :type: torch.nn.BatchNorm2d

   .. attribute:: bn_conv2

      Normalization layer applied after the fully connected layer.

      :type: Normalizer

   .. attribute:: feature_map_dropout

      Dropout layer applied to the output of the convolutional layer.

      :type: torch.nn.Dropout2d

   .. method:: residual_convolution(Q_1, Q_2)

      Performs an additive residual convolution operation on two sets of quaternion embeddings.


   .. method:: forward_triples(indexed_triple: torch.FloatTensor) -> torch.FloatTensor

      Computes scores for a batch of triples using additive convolutional operations on quaternion embeddings.


   .. method:: forward_k_vs_all(x: torch.FloatTensor) -> torch.FloatTensor

      Computes scores for all entities in a K-vs-All setting given a batch of head entities and relations.


   .. py:method:: residual_convolution(Q_1: Tuple[torch.FloatTensor, torch.FloatTensor, torch.FloatTensor, torch.FloatTensor], Q_2: Tuple[torch.FloatTensor, torch.FloatTensor, torch.FloatTensor, torch.FloatTensor]) -> Tuple[torch.FloatTensor, torch.FloatTensor, torch.FloatTensor, torch.FloatTensor]

      Performs a residual convolution operation on two sets of quaternion embeddings.

      The method combines two quaternion embeddings and applies a convolutional operation
      followed by batch normalization, dropout, and a fully connected layer.

      :param Q_1: The first set of quaternion embeddings.
      :type Q_1: Tuple[torch.FloatTensor, torch.FloatTensor, torch.FloatTensor, torch.FloatTensor]
      :param Q_2: The second set of quaternion embeddings.
      :type Q_2: Tuple[torch.FloatTensor, torch.FloatTensor, torch.FloatTensor, torch.FloatTensor]

      :returns: The resulting quaternion embeddings after the convolutional operation.
      :rtype: Tuple[torch.FloatTensor, torch.FloatTensor, torch.FloatTensor, torch.FloatTensor]


   .. py:method:: forward_triples(indexed_triple: torch.FloatTensor) -> torch.FloatTensor

      Computes scores for a batch of triples using convolutional operations on quaternion embeddings.

      The method processes head, relation, and tail embeddings using quaternion algebra and
      convolutional layers and computes the scores of the triples.

      :param indexed_triple: Tensor containing indices for head entities, relations, and tail entities.
      :type indexed_triple: torch.FloatTensor

      :returns: Scores for the given batch of triples.
      :rtype: torch.FloatTensor


   .. py:method:: forward_k_vs_all(x: torch.FloatTensor) -> torch.FloatTensor

      Computes scores for all entities in a K-vs-All setting given a batch of head entities and relations.

      This method retrieves embeddings for the head entities and relations from the input tensor `x`,
      applies necessary dropout and normalization, and then computes scores against all entities in
      the knowledge graph.

      :param x: A tensor containing indices for head entities and relations.
      :type x: torch.FloatTensor

      :returns: Scores for all entities for the given batch of head entities and relations.
      :rtype: torch.FloatTensor



.. py:class:: ConvQ(args)


   Bases: :py:obj:`dicee.models.base_model.BaseKGE`

   Convolutional Quaternion Knowledge Graph Embeddings (ConvQ) is a model that extends
   the base knowledge graph embedding approach by using quaternion algebra and convolutional
   neural networks. This model aims to capture complex interactions in knowledge graphs
   by applying convolutions to quaternion-based entity and relation embeddings.

   :param args: A dictionary of arguments containing hyperparameters and settings for the model,
                such as embedding dimensions, number of output channels, kernel size, and dropout rates.
   :type args: dict

   .. attribute:: name

      The name identifier for the ConvQ model.

      :type: str

   .. attribute:: entity_embeddings

      Embedding layer for entities in the knowledge graph.

      :type: torch.nn.Embedding

   .. attribute:: relation_embeddings

      Embedding layer for relations in the knowledge graph.

      :type: torch.nn.Embedding

   .. attribute:: conv2d

      A 2D convolutional layer used for processing quaternion embeddings.

      :type: torch.nn.Conv2d

   .. attribute:: fc_num_input

      The number of input features for the fully connected layer.

      :type: int

   .. attribute:: fc1

      A fully connected linear layer for compressing the output of the convolutional layer.

      :type: torch.nn.Linear

   .. attribute:: bn_conv1

      First batch normalization layer applied after the convolutional operation.

      :type: torch.nn.BatchNorm2d

   .. attribute:: bn_conv2

      Second normalization layer applied after the fully connected layer.

      :type: Normalizer

   .. attribute:: feature_map_dropout

      Dropout layer applied to the output of the convolutional layer.

      :type: torch.nn.Dropout2d

   .. method:: residual_convolution(Q_1, Q_2)

      Performs a residual convolution operation on two sets of quaternion embeddings.


   .. method:: forward_triples(indexed_triple: torch.FloatTensor) -> torch.FloatTensor

      Computes scores for a batch of triples using convolutional operations on quaternion embeddings.


   .. method:: forward_k_vs_all(x: torch.FloatTensor) -> torch.FloatTensor

      Computes scores for all entities in a K-vs-All setting given a batch of head entities and relations.


   .. rubric:: Notes

   ConvQ leverages the properties of quaternions, a number system that extends complex numbers,
   to represent and process the embeddings of entities and relations. The convolutional layers
   aim to capture spatial relationships and complex patterns in the embeddings.

   .. py:method:: residual_convolution(Q_1: Tuple[torch.FloatTensor, torch.FloatTensor, torch.FloatTensor, torch.FloatTensor], Q_2: Tuple[torch.FloatTensor, torch.FloatTensor, torch.FloatTensor, torch.FloatTensor]) -> Tuple[torch.FloatTensor, torch.FloatTensor, torch.FloatTensor, torch.FloatTensor]

      Performs a residual convolution operation on two sets of quaternion embeddings.

      The method combines two quaternion embeddings and applies a convolutional operation
      followed by batch normalization, dropout, and a fully connected layer.

      :param Q_1: The first set of quaternion embeddings.
      :type Q_1: Tuple[torch.FloatTensor, torch.FloatTensor, torch.FloatTensor, torch.FloatTensor]
      :param Q_2: The second set of quaternion embeddings.
      :type Q_2: Tuple[torch.FloatTensor, torch.FloatTensor, torch.FloatTensor, torch.FloatTensor]

      :returns: The resulting quaternion embeddings after the convolutional operation.
      :rtype: Tuple[torch.FloatTensor, torch.FloatTensor, torch.FloatTensor, torch.FloatTensor]


   .. py:method:: forward_triples(indexed_triple: torch.FloatTensor) -> torch.FloatTensor

      Computes scores for a batch of triples using convolutional operations on quaternion embeddings.

      The method processes head, relation, and tail embeddings using quaternion algebra and
      convolutional layers and computes the scores of the triples.

      :param indexed_triple: Tensor containing indices for head entities, relations, and tail entities.
      :type indexed_triple: torch.FloatTensor

      :returns: Scores for the given batch of triples.
      :rtype: torch.FloatTensor


   .. py:method:: forward_k_vs_all(x: torch.FloatTensor) -> torch.FloatTensor

      Computes scores for all entities in a K-vs-All setting given a batch of head entities and relations.

      This method retrieves embeddings for the head entities and relations from the input tensor `x`,
      applies necessary dropout and normalization, and then computes scores against all entities in
      the knowledge graph.

      :param x: A tensor containing indices for head entities and relations.
      :type x: torch.FloatTensor

      :returns: Scores for all entities for the given batch of head entities and relations.
      :rtype: torch.FloatTensor



.. py:class:: ConvO(args: dict)


   Bases: :py:obj:`dicee.models.base_model.BaseKGE`

   ConvO extends the base knowledge graph embedding model by integrating convolutional
   operations with octonion algebra. This model applies convolutional neural networks
   to octonion-based embeddings, capturing complex interactions in knowledge graphs.

   :param args: A dictionary of arguments containing hyperparameters and settings for the model,
                such as embedding dimensions, number of output channels, kernel size, and dropout rates.
   :type args: dict

   .. attribute:: name

      The name identifier for the ConvO model.

      :type: str

   .. attribute:: conv2d

      A 2D convolutional layer used for processing octonion-based embeddings.

      :type: torch.nn.Conv2d

   .. attribute:: fc_num_input

      The number of input features for the fully connected layer.

      :type: int

   .. attribute:: fc1

      A fully connected linear layer for compressing the output of the convolutional layer.

      :type: torch.nn.Linear

   .. attribute:: bn_conv2d

      Batch normalization layer applied after the convolutional operation.

      :type: torch.nn.BatchNorm2d

   .. attribute:: norm_fc1

      Normalization layer applied after the fully connected layer.

      :type: Normalizer

   .. attribute:: feature_map_dropout

      Dropout layer applied to the output of the convolutional layer.

      :type: torch.nn.Dropout2d

   .. method:: octonion_normalizer(emb_rel_e0, emb_rel_e1, ..., emb_rel_e7)

      Normalizes octonion components to unit length.


   .. method:: residual_convolution(O_1, O_2)

      Performs a residual convolution operation on two octonion embeddings.


   .. method:: forward_triples(x: torch.Tensor) -> torch.Tensor

      Computes scores for a batch of triples using convolutional operations.


   .. method:: forward_k_vs_all(x: torch.Tensor)

      Computes scores against a sampled subset of entities using convolutional operations.


   .. rubric:: Notes

   ConvO aims to enhance the modeling capabilities of knowledge graph embeddings by
   adding more complex interaction patterns through convolutional layers, potentially
   improving performance on tasks like link prediction.

   .. py:method:: octonion_normalizer(emb_rel_e0: torch.Tensor, emb_rel_e1: torch.Tensor, emb_rel_e2: torch.Tensor, emb_rel_e3: torch.Tensor, emb_rel_e4: torch.Tensor, emb_rel_e5: torch.Tensor, emb_rel_e6: torch.Tensor, emb_rel_e7: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]
      :staticmethod:

      Normalizes the components of an octonion to unit length.

      Each component of the octonion is divided by the square root of the sum of
      the squares of all components.

      :param emb_rel_e0: The eight components of an octonion.
      :type emb_rel_e0: torch.Tensor
      :param emb_rel_e1: The eight components of an octonion.
      :type emb_rel_e1: torch.Tensor
      :param ...: The eight components of an octonion.
      :type ...: torch.Tensor
      :param emb_rel_e7: The eight components of an octonion.
      :type emb_rel_e7: torch.Tensor

      :returns: The normalized components of the octonion.
      :rtype: Tuple[torch.Tensor, ...]


   .. py:method:: residual_convolution(O_1: Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor], O_2: Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]

      Performs a residual convolution operation on two sets of octonion embeddings.

      The method combines two octonion embeddings and applies a convolutional operation
      followed by batch normalization, dropout, and a fully connected layer.

      :param O_1: The first set of octonion embeddings.
      :type O_1: Tuple[torch.Tensor, ...]
      :param O_2: The second set of octonion embeddings.
      :type O_2: Tuple[torch.Tensor, ...]

      :returns: The resulting octonion embeddings after the convolutional operation.
      :rtype: Tuple[torch.Tensor, ...]


   .. py:method:: forward_triples(x: torch.Tensor) -> torch.Tensor

      Computes scores for a batch of triples using convolutional operations.

      The method processes head, relation, and tail embeddings using convolutional
      layers and computes the scores of the triples.

      :param x: Tensor containing indices for head entities, relations, and tail entities.
      :type x: torch.Tensor

      :returns: Scores for the given batch of triples.
      :rtype: torch.Tensor


   .. py:method:: forward_k_vs_all(x: torch.Tensor) -> torch.Tensor

      Given a batch of head entities and relations (h,r), this method computes scores for all entities.
      [score(h,r,x)|x \in Entities] => [0.0,0.1,...,0.8], shape=> (1, |Entities|)
      Given a batch of head entities and relations => shape (size of batch,| Entities|)

      :param x: A tensor representing a batch of input triples in the form of (head entities, relations).
      :type x: torch.Tensor

      :returns: Scores for the input triples against all possible tail entities.
      :rtype: torch.Tensor

      .. rubric:: Notes

      - The input `x` is a tensor of shape (batch_size, 2), where each row represents a pair of head entities and relations.
      - The method follows the following steps:
          (1) Retrieve embeddings & Apply Dropout & Normalization.
          (2) Split the embeddings into real and imaginary parts.
          (3) Apply convolution operation on the real and imaginary parts.
          (4) Perform quaternion multiplication.
          (5) Compute scores for all entities.

      The method returns a tensor of shape (batch_size, num_entities) where each row contains scores for each entity in the knowledge graph.



.. py:class:: ConEx(args)


   Bases: :py:obj:`dicee.models.base_model.BaseKGE`

   ConEx (Convolutional ComplEx) is a Knowledge Graph Embedding model that extends ComplEx embeddings with convolutional layers.
   It integrates convolutional neural networks into the embedding process to capture complex patterns in the data.

   :param args: A dictionary of arguments containing hyperparameters and settings for the model, such as embedding dimensions,
                kernel size, number of output channels, and dropout rates.
   :type args: dict

   .. attribute:: name

      The name identifier for the ConEx model.

      :type: str

   .. attribute:: conv2d

      A 2D convolutional layer used for processing complex-valued embeddings.

      :type: torch.nn.Conv2d

   .. attribute:: fc1

      A fully connected linear layer for compressing the output of the convolutional layer.

      :type: torch.nn.Linear

   .. attribute:: norm_fc1

      Normalization layer applied after the fully connected layer.

      :type: Normalizer

   .. attribute:: bn_conv2d

      Batch normalization layer applied after the convolutional operation.

      :type: torch.nn.BatchNorm2d

   .. attribute:: feature_map_dropout

      Dropout layer applied to the output of the convolutional layer.

      :type: torch.nn.Dropout2d

   .. method:: residual_convolution(C_1: Tuple[torch.Tensor, torch.Tensor], C_2: Tuple[torch.Tensor, torch.Tensor]) -> Tuple[torch.Tensor, torch.Tensor]

      Performs a residual convolution operation on two complex-valued embeddings.

   .. method:: forward_k_vs_all(x: torch.Tensor) -> torch.FloatTensor

      Computes scores in a K-vs-All setting using convolutional operations on embeddings.

   .. method:: forward_triples(x: torch.Tensor) -> torch.FloatTensor

      Computes scores for a batch of triples using convolutional operations.

   .. method:: forward_k_vs_sample(x: torch.Tensor, target_entity_idx: torch.Tensor) -> torch.Tensor

      Computes scores against a sampled subset of entities using convolutional operations.


   .. rubric:: Notes

   ConEx combines complex-valued embeddings with convolutional neural networks to capture intricate patterns and interactions
   in the knowledge graph, potentially leading to improved performance on tasks like link prediction.

   .. py:method:: residual_convolution(C_1: Tuple[torch.Tensor, torch.Tensor], C_2: Tuple[torch.Tensor, torch.Tensor]) -> Tuple[torch.FloatTensor, torch.FloatTensor]

      Computes the residual score of two complex-valued embeddings by applying convolutional operations.
      This method is a key component of the ConEx model, combining complex embeddings with convolutional neural networks.

      :param C_1: A tuple consisting of two PyTorch tensors representing the real and imaginary components of the first complex-valued embedding.
      :type C_1: Tuple[torch.Tensor, torch.Tensor]
      :param C_2: A tuple consisting of two PyTorch tensors representing the real and imaginary components of the second complex-valued embedding.
      :type C_2: Tuple[torch.Tensor, torch.Tensor]

      :returns: A tuple of two tensors, representing the real and imaginary parts of the convolutionally transformed embeddings.
      :rtype: Tuple[torch.FloatTensor, torch.FloatTensor]

      .. rubric:: Notes

      The method involves concatenating the real and imaginary components of the embeddings, applying a 2D convolution,
      followed by batch normalization, ReLU activation, dropout, and a fully connected layer. This process is intended to
      capture complex interactions between the embeddings in a convolutional manner.


   .. py:method:: forward_k_vs_all(x: torch.Tensor) -> torch.FloatTensor

      Computes scores in a K-vs-All setting using convolutional operations on complex-valued embeddings.
      This method is used for evaluating the performance of the model by computing scores for each head entity
      and relation pair against all possible tail entities.

      :param x: A tensor representing a batch of head entities and relations. Expected tensor shape: (n, 2),
                where 'n' is the batch size and '2' represents head entity and relation pairs.
      :type x: torch.Tensor

      :returns: A tensor containing the scores for each head entity and relation pair against all possible tail entities.
                Tensor shape: (n, |E|), where '|E|' is the number of entities in the knowledge graph.
      :rtype: torch.FloatTensor

      .. rubric:: Notes

      The method retrieves embeddings for head entities and relations, splits them into real and imaginary parts,
      and applies a convolution operation. It then computes the Hermitian product of the transformed embeddings
      with all tail entity embeddings to generate scores. This approach allows for capturing complex relational patterns
      in the knowledge graph.


   .. py:method:: forward_triples(x: torch.Tensor) -> torch.FloatTensor

      Computes scores for a batch of triples using convolutional operations on complex-valued embeddings.
      This method is crucial for evaluating the performance of the model on individual triples in the
      knowledge graph.

      :param x: A tensor representing a batch of triples. Each triple consists of indices for a head entity,
                a relation, and a tail entity. Expected tensor shape: (n, 3), where 'n' is the number of triples.
      :type x: torch.Tensor

      :returns: A tensor containing the scores for each triple in the batch. Tensor shape: (n,), where 'n'
                is the number of triples.
      :rtype: torch.FloatTensor

      .. rubric:: Notes

      The method retrieves embeddings for head entities, relations, and tail entities, and splits them
      into real and imaginary parts. It then applies a convolution operation on these embeddings and
      computes the Hermitian inner product, which involves a combination of real and imaginary parts
      of the embeddings. This process is designed to capture complex relational patterns and interactions
      within the knowledge graph, leveraging the power of convolutional neural networks.


   .. py:method:: forward_k_vs_sample(x: torch.Tensor, target_entity_idx: torch.Tensor) -> torch.Tensor

      Computes scores against a sampled subset of entities using convolutional operations
      on complex-valued embeddings. This method is particularly useful for large knowledge graphs
      where computing scores against all entities is computationally expensive.

      :param x: A tensor representing a batch of head entities and relations. Expected tensor shape:
                (batch_size, 2), where 'batch_size' is the number of head entity and relation pairs.
      :type x: torch.Tensor
      :param target_entity_idx: A tensor of target entity indices for sampling. Tensor shape:
                                (batch_size, num_selected_entities).
      :type target_entity_idx: torch.Tensor

      :returns: A tensor containing the scores for each head entity and relation pair against the sampled
                subset of tail entities. Tensor shape: (batch_size, num_selected_entities).
      :rtype: torch.Tensor

      .. rubric:: Notes

      The method first retrieves and processes the embeddings for head entities and relations. It then
      applies a convolution operation and computes the Hermitian inner product with the embeddings of
      the sampled tail entities. This process enables capturing complex relational patterns in a
      computationally efficient manner.



.. py:class:: QMult(args: dict)


   Bases: :py:obj:`dicee.models.base_model.BaseKGE`

   QMult extends the base knowledge graph embedding model by integrating quaternion
   algebra. This model leverages the properties of quaternions to represent and process
   the embeddings of entities and relations in a knowledge graph, aiming to capture
   complex interactions and patterns.

   :param args: A dictionary of arguments containing hyperparameters and settings for the model,
                such as embedding dimensions and learning rate.
   :type args: dict

   .. attribute:: name

      The name identifier for the QMult model.

      :type: str

   .. method:: quaternion_normalizer(x: torch.FloatTensor) -> torch.FloatTensor

      Normalizes the length of relation vectors.


   .. method:: score(head_ent_emb: torch.FloatTensor, rel_ent_emb: torch.FloatTensor, tail_ent_emb: torch.FloatTensor) -> torch.FloatTensor

      Computes the score of a triple using quaternion multiplication.


   .. method:: k_vs_all_score(bpe_head_ent_emb: torch.FloatTensor, bpe_rel_ent_emb: torch.FloatTensor, E: torch.FloatTensor) -> torch.FloatTensor

      Computes scores in a K-vs-All setting using quaternion embeddings.


   .. method:: forward_k_vs_all(x: torch.FloatTensor) -> torch.FloatTensor

      Performs a forward pass for K-vs-All scoring, returning scores for all entities.


   .. method:: forward_k_vs_sample(x: torch.FloatTensor, target_entity_idx: int) -> torch.FloatTensor

      Performs a forward pass for K-vs-Sample scoring, returning scores for the specified entities.


   .. method:: quaternion_multiplication_followed_by_inner_product(h: torch.FloatTensor, r: torch.FloatTensor, t: torch.FloatTensor) -> torch.FloatTensor

      Performs quaternion multiplication followed by inner product, returning triple scores.


   .. py:method:: quaternion_multiplication_followed_by_inner_product(h: torch.FloatTensor, r: torch.FloatTensor, t: torch.FloatTensor) -> torch.FloatTensor

      Performs quaternion multiplication followed by inner product.

      :param h: The head representations. Shape: (`*batch_dims`, dim)
      :type h: torch.FloatTensor
      :param r: The relation representations. Shape: (`*batch_dims`, dim)
      :type r: torch.FloatTensor
      :param t: The tail representations. Shape: (`*batch_dims`, dim)
      :type t: torch.FloatTensor

      :returns: Triple scores.
      :rtype: torch.FloatTensor


   .. py:method:: quaternion_normalizer(x: torch.FloatTensor) -> torch.FloatTensor
      :staticmethod:

      TODO: Add mathematical format for sphinx.
      Normalize the length of relation vectors, if the forward constraint has not been applied yet.

      The absolute value of a quaternion is calculated as follows:
      .. math::

          |a + bi + cj + dk| = \sqrt{a^2 + b^2 + c^2 + d^2}

      The L2 norm of a quaternion vector is computed as:
      .. math::
          \|x\|^2 = \sum_{i=1}^d |x_i|^2
                   = \sum_{i=1}^d (x_i.re^2 + x_i.im_1^2 + x_i.im_2^2 + x_i.im_3^2)
      :param x: The vector containing quaternion values.
      :type x: torch.FloatTensor

      :returns: The normalized vector.
      :rtype: torch.FloatTensor

      .. rubric:: Notes

      This function normalizes the length of relation vectors represented as quaternions. It ensures that
      the absolute value of each quaternion in the vector is equal to 1, preserving the unit length.


   .. py:method:: score(head_ent_emb: torch.FloatTensor, rel_ent_emb: torch.FloatTensor, tail_ent_emb: torch.FloatTensor) -> torch.FloatTensor

      Compute scores for a batch of triples using octonion-based embeddings.

      This method computes scores for a batch of triples using octonion-based embeddings of head entities,
      relation embeddings, and tail entities. It supports both explicit and non-explicit scoring methods.

      :param head_ent_emb: Tensor containing the octonion-based embeddings of head entities.
      :type head_ent_emb: torch.FloatTensor
      :param rel_ent_emb: Tensor containing the octonion-based embeddings of relations.
      :type rel_ent_emb: torch.FloatTensor
      :param tail_ent_emb: Tensor containing the octonion-based embeddings of tail entities.
      :type tail_ent_emb: torch.FloatTensor

      :returns: Scores for the given batch of triples.
      :rtype: torch.FloatTensor

      .. rubric:: Notes

      If no normalization is set, this method applies quaternion normalization to relation embeddings.

      If the scoring method is explicit, it computes the scores using quaternion multiplication followed by
      an inner product of the real and imaginary parts of the resulting quaternions.

      If the scoring method is non-explicit, it directly computes the inner product of the real and
      imaginary parts of the octonion-based embeddings.


   .. py:method:: k_vs_all_score(bpe_head_ent_emb: torch.FloatTensor, bpe_rel_ent_emb: torch.FloatTensor, E: torch.FloatTensor) -> torch.FloatTensor

      Computes scores in a K-vs-All setting using quaternion embeddings for a batch of head entities and relations.

      This method involves splitting the head entity and relation embeddings into quaternion components,
      optionally normalizing the relation embeddings, performing quaternion multiplication, and then
      calculating the score by performing an inner product with all tail entity embeddings.

      :param bpe_head_ent_emb: Batched embeddings of head entities, each represented as a quaternion.
      :type bpe_head_ent_emb: torch.FloatTensor
      :param bpe_rel_ent_emb: Batched embeddings of relations, each represented as a quaternion.
      :type bpe_rel_ent_emb: torch.FloatTensor
      :param E: Embeddings of all possible tail entities.
      :type E: torch.FloatTensor

      :returns: Scores for all possible triples formed with the given head entities and relations against all entities.
                The shape of the output is (size of batch, number of entities).
      :rtype: torch.FloatTensor

      .. rubric:: Notes

      The method is particularly useful in scenarios like link prediction, where the goal is to rank all possible
      tail entities for a given head entity and relation. Quaternion algebra is used to enhance the interaction
      modeling between entities and relations.


   .. py:method:: forward_k_vs_all(x: torch.FloatTensor) -> torch.FloatTensor

      Computes scores for all entities in a K-vs-All setting given a batch of head entities and relations.

      This method retrieves embeddings for the head entities and relations from the input tensor `x`,
      applies necessary dropout and normalization, and then uses the `k_vs_all_score` method to compute
      the scores against all possible tail entities in the knowledge graph.

      :param x: A tensor containing indices for head entities and relations. The tensor is expected to have
                a specific format suitable for the model's embedding retrieval process.
      :type x: torch.FloatTensor

      :returns: A tensor of scores, where each row corresponds to the scores of all tail entities for a
                single head entity and relation pair. The shape of the tensor is (size of the batch, number of entities).
      :rtype: torch.FloatTensor

      .. rubric:: Notes

      This method is typically used in evaluating the model's performance in link prediction tasks,
      where it's important to rank the likelihood of every possible tail entity for a given head entity
      and relation.


   .. py:method:: forward_k_vs_sample(x: torch.FloatTensor, target_entity_idx: int) -> torch.FloatTensor

      Computes scores for a batch of triples against a sampled subset of entities in a K-vs-Sample setting.

      Given a batch of head entities and relations (h,r), this method computes the scores for all possible triples
      formed with these head entities and relations against a subset of entities, i.e., [score(h,r,x)|x \in Entities] => [0.0,0.1,...,0.8], shape=> (1, |Entities|). TODO: Add mathematical format for sphinx.
      The subset of entities is specified by the `target_entity_idx`, which is an integer index representing a specific entity.
      Given a batch of head entities and relations => shape (size of batch,| Entities|).

      :param x: A tensor containing indices for head entities and relations. The tensor is expected to have
                a specific format suitable for the model's embedding retrieval process.
      :type x: torch.FloatTensor
      :param target_entity_idx: Index of the target entity against which the scores are to be computed.
      :type target_entity_idx: int

      :returns: A tensor of scores where each element corresponds to the score of the target entity
                for a single head entity and relation pair. The shape of the tensor is (size of the batch, 1).
      :rtype: torch.FloatTensor

      .. rubric:: Notes

      This method is particularly useful in scenarios like link prediction, where it's necessary to
      evaluate the likelihood of a specific relationship between a given head entity and a particular
      target entity.



.. py:class:: OMult(args: dict)


   Bases: :py:obj:`dicee.models.base_model.BaseKGE`

   OMult extends the base knowledge graph embedding model by integrating octonion
   algebra. This model leverages the properties of octonions to represent and process
   the embeddings of entities and relations in a knowledge graph, aiming to capture
   complex interactions and patterns.

   :param args: A dictionary of arguments containing hyperparameters and settings for the model,
                such as embedding dimensions and learning rate.
   :type args: dict

   .. attribute:: name

      The name identifier for the OMult model.

      :type: str

   .. method:: octonion_normalizer(emb_rel_e0: torch.Tensor, emb_rel_e1: torch.Tensor, ..., emb_rel_e7: torch.Tensor) -> Tuple[torch.Tensor, ...]

      Normalizes octonion components to unit length.


   .. method:: score(head_ent_emb: torch.FloatTensor, rel_ent_emb: torch.FloatTensor, tail_ent_emb: torch.FloatTensor) -> torch.FloatTensor

      Computes the score of a triple using octonion multiplication.


   .. method:: k_vs_all_score(bpe_head_ent_emb, bpe_rel_ent_emb, E) -> torch.FloatTensor

      Computes scores in a K-vs-All setting using octonion embeddings.


   .. method:: forward_k_vs_all(x) -> torch.FloatTensor

      Performs a forward pass for K-vs-All scoring, returning scores for all entities.


   .. py:method:: octonion_normalizer(emb_rel_e0: torch.Tensor, emb_rel_e1: torch.Tensor, emb_rel_e2: torch.Tensor, emb_rel_e3: torch.Tensor, emb_rel_e4: torch.Tensor, emb_rel_e5: torch.Tensor, emb_rel_e6: torch.Tensor, emb_rel_e7: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]
      :staticmethod:

      Normalizes the components of an octonion.

      Each component of the octonion is divided by the square root of the sum of
      the squares of all components, normalizing it to unit length.

      :param emb_rel_e0: The eight components of an octonion.
      :type emb_rel_e0: torch.Tensor
      :param emb_rel_e1: The eight components of an octonion.
      :type emb_rel_e1: torch.Tensor
      :param ...: The eight components of an octonion.
      :type ...: torch.Tensor
      :param emb_rel_e7: The eight components of an octonion.
      :type emb_rel_e7: torch.Tensor

      :returns: The normalized components of the octonion.
      :rtype: Tuple[torch.Tensor, ...]


   .. py:method:: score(head_ent_emb: torch.FloatTensor, rel_ent_emb: torch.FloatTensor, tail_ent_emb: torch.FloatTensor) -> torch.FloatTensor

      Computes the score of a triple using octonion multiplication.

      The method involves splitting the embeddings into real and imaginary parts,
      normalizing the relation embeddings, performing octonion multiplication,
      and then calculating the score based on the inner product.

      :param head_ent_emb: Embedding of the head entity.
      :type head_ent_emb: torch.FloatTensor
      :param rel_ent_emb: Embedding of the relation.
      :type rel_ent_emb: torch.FloatTensor
      :param tail_ent_emb: Embedding of the tail entity.
      :type tail_ent_emb: torch.FloatTensor

      :returns: The score of the triple.
      :rtype: torch.FloatTensor


   .. py:method:: k_vs_all_score(bpe_head_ent_emb: torch.FloatTensor, bpe_rel_ent_emb: torch.FloatTensor, E: torch.FloatTensor) -> torch.FloatTensor

      Computes scores in a K-vs-All setting using octonion embeddings for a batch of head entities and relations.

      This method splits the head entity and relation embeddings into their octonion components, normalizes
      the relation embeddings if necessary, and then applies octonion multiplication. It computes the score
      by performing an inner product with all tail entity embeddings.

      :param bpe_head_ent_emb: Batched embeddings of head entities, each represented as an octonion.
      :type bpe_head_ent_emb: torch.FloatTensor
      :param bpe_rel_ent_emb: Batched embeddings of relations, each represented as an octonion.
      :type bpe_rel_ent_emb: torch.FloatTensor
      :param E: Embeddings of all possible tail entities.
      :type E: torch.FloatTensor

      :returns: Scores for all possible triples formed with the given head entities and relations against all entities.
                The shape of the output is (size of batch, number of entities).
      :rtype: torch.FloatTensor

      .. rubric:: Notes

      The method is particularly useful in scenarios like link prediction, where the goal is to rank all possible
      tail entities for a given head entity and relation.


   .. py:method:: forward_k_vs_all(x)

      Performs a forward pass for K-vs-All scoring.

      TODO: Add mathematical format for sphinx.

      Given a head entity and a relation (h,r), this method computes scores for all
      possible triples, i.e., [score(h,r,x)|x \in Entities] => [0.0,0.1,...,0.8], shape=> (1, |Entities|), returning a score for each entity in the knowledge graph.

      :param x: Tensor containing indices for head entities and relations.
      :type x: Tensor

      :returns: Scores for all triples formed with the given head entities and relations against all entities.
      :rtype: torch.FloatTensor



.. py:class:: Shallom(args: dict)


   Bases: :py:obj:`dicee.models.base_model.BaseKGE`

   Shallom is a shallow neural model designed for relation prediction in knowledge graphs.
   The model combines entity embeddings and passes them through a neural network to predict
   the likelihood of different relations. It's based on the paper:
   'A Shallow Neural Model for Relation Prediction'
   (https://arxiv.org/abs/2101.09090).

   .. attribute:: name

      The name identifier for the Shallom model.

      :type: str

   .. attribute:: shallom

      A sequential neural network model used for predicting relations.

      :type: torch.nn.Sequential

   .. method:: get_embeddings() -> Tuple[np.ndarray, None]

      Retrieves the entity embeddings.


   .. method:: forward_k_vs_all(x) -> torch.FloatTensor

      Computes relation scores for all pairs of entities in the batch.


   .. method:: forward_triples(x) -> torch.FloatTensor

      Computes relation scores for a batch of triples.


   .. py:method:: get_embeddings() -> Tuple[numpy.ndarray, None]

      Retrieves the entity embeddings from the model.

      :returns: A tuple containing the entity embeddings as a NumPy array and None for the relation embeddings.
      :rtype: Tuple[np.ndarray, None]


   .. py:method:: forward_k_vs_all(x: torch.Tensor) -> torch.FloatTensor

      Computes relation scores for all pairs of entities in the batch.

      Each pair of entities is passed through the Shallom neural network to predict
      the likelihood of various relations between them.

      :param x: A tensor of entity pairs.
      :type x: torch.Tensor

      :returns: A tensor of relation scores for each pair of entities in the batch.
      :rtype: torch.FloatTensor


   .. py:method:: forward_triples(x: torch.Tensor) -> torch.FloatTensor

      Computes relation scores for a batch of triples.

      This method first computes relation scores for all possible relations for each pair of entities
      and then selects the scores corresponding to the actual relations in the triples.

      :param x: A tensor containing a batch of triples.
      :type x: torch.Tensor

      :returns: A flattened tensor of relation scores for the given batch of triples.
      :rtype: torch.FloatTensor



.. py:class:: LFMult(args)


   Bases: :py:obj:`dicee.models.base_model.BaseKGE`

   Embedding with polynomial functions. We represent all entities and relations in the polynomial space as:
   f(x) = \sum_{i=0}^{d-1} a_k x^{i%d} and use the three differents scoring function as in the paper to evaluate the score.
   We also consider combining with Neural Networks.

   .. py:method:: forward_triples(idx_triple)

      Perform the forward pass for triples.

      :param x: The input tensor containing the indexes of head, relation, and tail entities.
      :type x: torch.LongTensor

      :returns: The output tensor containing the scores for the input triples.
      :rtype: torch.Tensor


   .. py:method:: construct_multi_coeff(x)


   .. py:method:: poly_NN(x, coefh, coefr, coeft)

      Constructing a 2 layers NN to represent the embeddings.
      h = \sigma(wh^T x + bh ),  r = \sigma(wr^T x + br ),  t = \sigma(wt^T x + bt )


   .. py:method:: linear(x, w, b)


   .. py:method:: scalar_batch_NN(a, b, c)

      element wise multiplication between a,b and c:
      Inputs : a, b, c ====> torch.tensor of size batch_size x m x d
      Output : a tensor of size batch_size x d


   .. py:method:: tri_score(coeff_h, coeff_r, coeff_t)

      this part implement the trilinear scoring techniques:

      score(h,r,t) = \int_{0}{1} h(x)r(x)t(x) dx = \sum_{i,j,k = 0}^{d-1} \dfrac{a_i*b_j*c_k}{1+(i+j+k)%d}

      1. generate the range for i,j and k from [0 d-1]

      2. perform
      \dfrac{a_i*b_j*c_k}{1+(i+j+k)%d} in parallel for every batch

      3. take the sum over each batch



   .. py:method:: vtp_score(h, r, t)

      this part implement the vector triple product scoring techniques:

      score(h,r,t) = \int_{0}{1} h(x)r(x)t(x) dx = \sum_{i,j,k = 0}^{d-1} \dfrac{a_i*c_j*b_k - b_i*c_j*a_k}{(1+(i+j)%d)(1+k)}

      1. generate the range for i,j and k from [0 d-1]

      2. Compute the first and second terms of the sum

      3.  Multiply with then denominator and take the sum

      4. take the sum over each batch



   .. py:method:: comp_func(h, r, t)

      this part implement the function composition scoring techniques: i.e. score = <hor, t>


   .. py:method:: polynomial(coeff, x, degree)

      This function takes a matrix tensor of coefficients (coeff), a tensor vector of points x  and range of integer [0,1,...d]
      and return a vector tensor (coeff[0][0] + coeff[0][1]x +...+ coeff[0][d]x^d,
                          coeff[1][0] + coeff[1][1]x +...+ coeff[1][d]x^d)
                                  ....


   .. py:method:: pop(coeff, x, degree)

      This function allow us to evaluate the composition of two polynomes without for loops :)
      it takes a matrix tensor of coefficients (coeff), a matrix tensor of points x  and range of integer [0,1,...d]
          and return a tensor (coeff[0][0] + coeff[0][1]x +...+ coeff[0][d]x^d,
                              coeff[1][0] + coeff[1][1]x +...+ coeff[1][d]x^d)
                                      ....



.. py:class:: PykeenKGE(args: dict)


   Bases: :py:obj:`dicee.models.base_model.BaseKGE`

   A class for using knowledge graph embedding models implemented in Pykeen.

   :param args: A dictionary of arguments containing hyperparameters and settings for the model,
                such as embedding dimensions, random seed, and model-specific kwargs.
   :type args: dict

   .. attribute:: name

      The name identifier for the PykeenKGE model.

      :type: str

   .. attribute:: model

      The Pykeen model instance.

      :type: pykeen.models.base.Model

   .. attribute:: loss_history

      A list to store the training loss history.

      :type: list

   .. attribute:: args

      The arguments used to initialize the model.

      :type: dict

   .. attribute:: entity_embeddings

      Entity embeddings learned by the model.

      :type: torch.nn.Embedding

   .. attribute:: relation_embeddings

      Relation embeddings learned by the model.

      :type: torch.nn.Embedding

   .. attribute:: interaction

      Interaction module used by the Pykeen model.

      :type: pykeen.nn.modules.Interaction

   .. method:: forward_k_vs_all(x: torch.LongTensor) -> torch.FloatTensor

      Compute scores for all entities given a batch of head entities and relations.

   .. method:: forward_triples(x: torch.LongTensor) -> torch.FloatTensor

      Compute scores for a batch of triples.

   .. method:: forward_k_vs_sample(x: torch.LongTensor, target_entity_idx: int)

      Compute scores against a sampled subset of entities.


   .. rubric:: Notes

   This class provides an interface for using knowledge graph embedding models implemented
   in Pykeen. It initializes Pykeen models based on the provided arguments and allows for
   scoring triples and conducting knowledge graph embedding experiments.

   .. py:method:: forward_k_vs_all(x: torch.LongTensor)

      TODO: Format in Numpy-style documentation

      # => Explicit version by this we can apply bn and dropout

      # (1) Retrieve embeddings of heads and relations +  apply Dropout & Normalization if given.
      h, r = self.get_head_relation_representation(x)
      # (2) Reshape (1).
      if self.last_dim > 0:
          h = h.reshape(len(x), self.embedding_dim, self.last_dim)
          r = r.reshape(len(x), self.embedding_dim, self.last_dim)
      # (3) Reshape all entities.
      if self.last_dim > 0:
          t = self.entity_embeddings.weight.reshape(self.num_entities, self.embedding_dim, self.last_dim)
      else:
          t = self.entity_embeddings.weight
      # (4) Call the score_t from interactions to generate triple scores.
      return self.interaction.score_t(h=h, r=r, all_entities=t, slice_size=1)


   .. py:method:: forward_triples(x: torch.LongTensor) -> torch.FloatTensor

      TODO: Format in Numpy-style documentation

      # => Explicit version by this we can apply bn and dropout

      # (1) Retrieve embeddings of heads, relations and tails and apply Dropout & Normalization if given.
      h, r, t = self.get_triple_representation(x)
      # (2) Reshape (1).
      if self.last_dim > 0:
          h = h.reshape(len(x), self.embedding_dim, self.last_dim)
          r = r.reshape(len(x), self.embedding_dim, self.last_dim)
          t = t.reshape(len(x), self.embedding_dim, self.last_dim)
      # (3) Compute the triple score
      return self.interaction.score(h=h, r=r, t=t, slice_size=None, slice_dim=0)


   .. py:method:: forward_k_vs_sample(x: torch.LongTensor, target_entity_idx: int)
      :abstractmethod:

      Forward pass for K vs. Sample.

      :raises ValueError: This function is not implemented in the current model.



.. py:class:: BytE(*args, **kwargs)


   Bases: :py:obj:`dicee.models.base_model.BaseKGE`

   Base class for all neural network modules.

   Your models should also subclass this class.

   Modules can also contain other Modules, allowing to nest them in
   a tree structure. You can assign the submodules as regular attributes::

       import torch.nn as nn
       import torch.nn.functional as F

       class Model(nn.Module):
           def __init__(self):
               super().__init__()
               self.conv1 = nn.Conv2d(1, 20, 5)
               self.conv2 = nn.Conv2d(20, 20, 5)

           def forward(self, x):
               x = F.relu(self.conv1(x))
               return F.relu(self.conv2(x))

   Submodules assigned in this way will be registered, and will have their
   parameters converted too when you call :meth:`to`, etc.

   .. note::
       As per the example above, an ``__init__()`` call to the parent class
       must be made before assignment on the child.

   :ivar training: Boolean represents whether this module is in training or
                   evaluation mode.
   :vartype training: bool

   .. py:method:: loss_function(yhat_batch, y_batch)

      :param yhat_batch:
      :param y_batch:


   .. py:method:: forward(x: torch.LongTensor)

      :param x:
      :type x: B by T tensor


   .. py:method:: generate(idx, max_new_tokens, temperature=1.0, top_k=None)

      Take a conditioning sequence of indices idx (LongTensor of shape (b,t)) and complete
      the sequence max_new_tokens times, feeding the predictions back into the model each time.
      Most likely you'll want to make sure to be in model.eval() mode of operation for this.


   .. py:method:: training_step(batch, batch_idx=None)

      Here you compute and return the training loss and some additional metrics for e.g. the progress bar or
      logger.

      :param batch: The output of your data iterable, normally a :class:`~torch.utils.data.DataLoader`.
      :param batch_idx: The index of this batch.
      :param dataloader_idx: The index of the dataloader that produced this batch.
                             (only if multiple dataloaders used)

      :returns:

                - :class:`~torch.Tensor` - The loss tensor
                - ``dict`` - A dictionary which can include any keys, but must include the key ``'loss'`` in the case of
                  automatic optimization.
                - ``None`` - In automatic optimization, this will skip to the next batch (but is not supported for
                  multi-GPU, TPU, or DeepSpeed). For manual optimization, this has no special meaning, as returning
                  the loss is not required.

      In this step you'd normally do the forward pass and calculate the loss for a batch.
      You can also do fancier things like multiple forward passes or something model specific.

      Example::

          def training_step(self, batch, batch_idx):
              x, y, z = batch
              out = self.encoder(x)
              loss = self.loss(out, x)
              return loss

      To use multiple optimizers, you can switch to 'manual optimization' and control their stepping:

      .. code-block:: python

          def __init__(self):
              super().__init__()
              self.automatic_optimization = False


          # Multiple optimizers (e.g.: GANs)
          def training_step(self, batch, batch_idx):
              opt1, opt2 = self.optimizers()

              # do training_step with encoder
              ...
              opt1.step()
              # do training_step with decoder
              ...
              opt2.step()

      .. note::

         When ``accumulate_grad_batches`` > 1, the loss returned here will be automatically
         normalized by ``accumulate_grad_batches`` internally.



.. py:class:: BaseKGE(args: dict)


   Bases: :py:obj:`BaseKGELightning`

   Base class for all neural network modules.

   Your models should also subclass this class.

   Modules can also contain other Modules, allowing to nest them in
   a tree structure. You can assign the submodules as regular attributes::

       import torch.nn as nn
       import torch.nn.functional as F

       class Model(nn.Module):
           def __init__(self):
               super().__init__()
               self.conv1 = nn.Conv2d(1, 20, 5)
               self.conv2 = nn.Conv2d(20, 20, 5)

           def forward(self, x):
               x = F.relu(self.conv1(x))
               return F.relu(self.conv2(x))

   Submodules assigned in this way will be registered, and will have their
   parameters converted too when you call :meth:`to`, etc.

   .. note::
       As per the example above, an ``__init__()`` call to the parent class
       must be made before assignment on the child.

   :ivar training: Boolean represents whether this module is in training or
                   evaluation mode.
   :vartype training: bool

   .. py:method:: forward_byte_pair_encoded_k_vs_all(x: torch.LongTensor)

      :param x:
      :type x: B x 2 x T


   .. py:method:: forward_byte_pair_encoded_triple(x: Tuple[torch.LongTensor, torch.LongTensor])

      Perform the forward pass for byte pair encoded triples.

      :param x: The input tuple containing byte pair encoded entities and relations.
      :type x: Tuple[torch.LongTensor, torch.LongTensor]

      :returns: The output tensor containing the scores for the byte pair encoded triples.
      :rtype: torch.Tensor


   .. py:method:: init_params_with_sanity_checking()


   .. py:method:: forward(x: Union[torch.LongTensor, Tuple[torch.LongTensor, torch.LongTensor]], y_idx: torch.LongTensor = None)

      Perform the forward pass of the model.

      :param x: The input tensor or a tuple containing the input tensor and target entity indexes.
      :type x: Union[torch.LongTensor, Tuple[torch.LongTensor, torch.LongTensor]]
      :param y_idx: The target entity indexes (default is None).
      :type y_idx: torch.LongTensor, optional

      :returns: The output of the forward pass.
      :rtype: Any


   .. py:method:: forward_triples(x: torch.LongTensor) -> torch.Tensor

      Perform the forward pass for triples.

      :param x: The input tensor containing the indexes of head, relation, and tail entities.
      :type x: torch.LongTensor

      :returns: The output tensor containing the scores for the input triples.
      :rtype: torch.Tensor


   .. py:method:: forward_k_vs_all(*args, **kwargs)

      Forward pass for K vs. All.

      :raises ValueError: This function is not implemented in the current model.


   .. py:method:: forward_k_vs_sample(*args, **kwargs)

      Forward pass for K vs. Sample.

      :raises ValueError: This function is not implemented in the current model.


   .. py:method:: get_triple_representation(idx_hrt)


   .. py:method:: get_head_relation_representation(indexed_triple: torch.LongTensor) -> Tuple[torch.FloatTensor, torch.FloatTensor]

      Get the representation for the head and relation entities.

      :param indexed_triple: The indexes of the head and relation entities.
      :type indexed_triple: torch.LongTensor

      :returns: The representation for the head and relation entities.
      :rtype: Tuple[torch.FloatTensor, torch.FloatTensor]


   .. py:method:: get_sentence_representation(x: torch.LongTensor) -> Tuple[torch.FloatTensor, torch.FloatTensor, torch.FloatTensor]

      Get the representation for a sentence.

      :param x: The input tensor containing the indexes of head, relation, and tail entities.
      :type x: torch.LongTensor

      :returns: The representation for the input sentence.
      :rtype: Tuple[torch.FloatTensor, torch.FloatTensor, torch.FloatTensor]


   .. py:method:: get_bpe_head_and_relation_representation(x: torch.LongTensor) -> Tuple[torch.FloatTensor, torch.FloatTensor]

      Get the representation for BPE head and relation entities.

      :param x:
      :type x: B x 2 x T

      :returns: The representation for BPE head and relation entities.
      :rtype: Tuple[torch.FloatTensor, torch.FloatTensor]


   .. py:method:: get_embeddings() -> Tuple[numpy.ndarray, numpy.ndarray]

      Get the entity and relation embeddings.

      :returns: The entity and relation embeddings.
      :rtype: Tuple[np.ndarray, np.ndarray]



.. py:function:: create_recipriocal_triples(x)

   Add inverse triples into dask dataframe
   :param x:
   :return:


.. py:function:: get_er_vocab(data, file_path: str = None)


.. py:function:: get_re_vocab(data, file_path: str = None)


.. py:function:: get_ee_vocab(data, file_path: str = None)


.. py:function:: timeit(func)


.. py:function:: save_pickle(*, data: object = None, file_path=str)


.. py:function:: load_pickle(file_path=str)


.. py:function:: select_model(args: dict, is_continual_training: bool = None, storage_path: str = None)


.. py:function:: load_model(path_of_experiment_folder: str, model_name='model.pt', verbose=0) -> Tuple[object, Tuple[dict, dict]]

   Load weights and initialize pytorch module from namespace arguments


.. py:function:: load_model_ensemble(path_of_experiment_folder: str) -> Tuple[dicee.models.base_model.BaseKGE, Tuple[pandas.DataFrame, pandas.DataFrame]]

   Construct Ensemble Of weights and initialize pytorch module from namespace arguments

   (1) Detect models under given path
   (2) Accumulate parameters of detected models
   (3) Normalize parameters
   (4) Insert (3) into model.


.. py:function:: save_numpy_ndarray(*, data: numpy.ndarray, file_path: str)


.. py:function:: numpy_data_type_changer(train_set: numpy.ndarray, num: int) -> numpy.ndarray

   Detect most efficient data type for a given triples
   :param train_set:
   :param num:
   :return:


.. py:function:: save_checkpoint_model(model, path: str) -> None

   Store Pytorch model into disk


.. py:function:: store(trainer, trained_model, model_name: str = 'model', full_storage_path: str = None, save_embeddings_as_csv=False) -> None

   Store trained_model model and save embeddings into csv file.
   :param trainer: an instance of trainer class
   :param full_storage_path: path to save parameters.
   :param model_name: string representation of the name of the model.
   :param trained_model: an instance of BaseKGE see core.models.base_model .
   :param save_embeddings_as_csv: for easy access of embeddings.
   :return:


.. py:function:: add_noisy_triples(train_set: pandas.DataFrame, add_noise_rate: float) -> pandas.DataFrame

   Add randomly constructed triples
   :param train_set:
   :param add_noise_rate:
   :return:


.. py:function:: read_or_load_kg(args, cls)


.. py:function:: intialize_model(args: dict, verbose=0) -> Tuple[object, str]


.. py:function:: load_json(p: str) -> dict


.. py:function:: save_embeddings(embeddings: numpy.ndarray, indexes, path: str) -> None

   Save it as CSV if memory allows.
   :param embeddings:
   :param indexes:
   :param path:
   :return:


.. py:function:: random_prediction(pre_trained_kge)


.. py:function:: deploy_triple_prediction(pre_trained_kge, str_subject, str_predicate, str_object)


.. py:function:: deploy_tail_entity_prediction(pre_trained_kge, str_subject, str_predicate, top_k)


.. py:function:: deploy_head_entity_prediction(pre_trained_kge, str_object, str_predicate, top_k)


.. py:function:: deploy_relation_prediction(pre_trained_kge, str_subject, str_object, top_k)


.. py:function:: vocab_to_parquet(vocab_to_idx, name, path_for_serialization, print_into)


.. py:function:: create_experiment_folder(folder_name='Experiments')


.. py:function:: continual_training_setup_executor(executor) -> None

   storage_path:str A path leading to a parent directory, where a subdirectory containing KGE related data

   full_storage_path:str A path leading to a subdirectory containing KGE related data



.. py:function:: exponential_function(x: numpy.ndarray, lam: float, ascending_order=True) -> torch.FloatTensor


.. py:function:: load_numpy(path) -> numpy.ndarray


.. py:function:: evaluate(entity_to_idx, scores, easy_answers, hard_answers)

   # @TODO: CD: Renamed this function
   Evaluate multi hop query answering on different query types


.. py:function:: download_file(url, destination_folder='.')


.. py:function:: download_files_from_url(base_url: str, destination_folder='.') -> None

   :param base_url:
   :type base_url: e.g. "https://files.dice-research.org/projects/DiceEmbeddings/KINSHIP-Keci-dim128-epoch256-KvsAll"
   :param destination_folder:
   :type destination_folder: e.g. "KINSHIP-Keci-dim128-epoch256-KvsAll"


.. py:function:: download_pretrained_model(url: str) -> str


.. py:class:: DICE_Trainer(args, is_continual_training: bool, storage_path: str, evaluator: Optional[object] = None)


   Implements a training framework for knowledge graph embedding models using [PyTorch Lightning](https://pytorch-lightning.readthedocs.io/en/stable/common/trainer.html),
   supporting [multi-GPU](https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html) and CPU training. This trainer can handle continual training scenarios and supports
   different forms of labeling and evaluation methods.

   :param args: Command line arguments or configurations specifying training parameters and model settings.
   :type args: Namespace
   :param is_continual_training: Flag indicating whether the training session is part of a continual learning process.
   :type is_continual_training: bool
   :param storage_path: Path to the directory where training checkpoints and models are stored.
   :type storage_path: str
   :param evaluator: An evaluation object responsible for model evaluation. This can be any object that implements
                     an `eval` method accepting model predictions and returning evaluation metrics.
   :type evaluator: object, optional

   .. attribute:: report

      A dictionary to store training reports and metrics.

      :type: dict

   .. attribute:: trainer

      The PyTorch Lightning Trainer instance used for model training.

      :type: lightening.Trainer or None

   .. attribute:: form_of_labelling

      The form of labeling used during training, which can be "EntityPrediction", "RelationPrediction", or "Pyke".

      :type: str or None

   .. method:: continual_start()

      Initializes and starts the training process, including model loading and fitting.

   .. method:: initialize_trainer(callbacks: List) -> lightening.Trainer

      Initializes a PyTorch Lightning Trainer instance with the specified callbacks.

   .. method:: initialize_or_load_model()

      Initializes or loads a model for training based on the training configuration.

   .. method:: initialize_dataloader(dataset: torch.utils.data.Dataset) -> torch.utils.data.DataLoader

      Initializes a DataLoader for the given dataset.

   .. method:: initialize_dataset(dataset: KG, form_of_labelling) -> torch.utils.data.Dataset

      Prepares and initializes a dataset for training.

   .. method:: start(knowledge_graph: KG) -> Tuple[BaseKGE, str]

      Starts the training process for a given knowledge graph.

   .. method:: k_fold_cross_validation(dataset) -> Tuple[BaseKGE, str]

      Performs K-fold cross-validation on the dataset and returns the trained model and form of labelling.


   .. py:method:: continual_start()

      Initializes and starts the training process, including model loading and fitting.
      This method is specifically designed for continual training scenarios.

      :returns: * **model** (*BaseKGE*) -- The trained knowledge graph embedding model instance. `BaseKGE` is a placeholder
                  for the actual model class, which should be a subclass of the base model class
                  used in your framework.
                * **form_of_labelling** (*str*) -- The form of labeling used during the training. This can indicate the type of
                  prediction task the model is trained for, such as "EntityPrediction",
                  "RelationPrediction", or other custom labeling forms defined in your implementation.


   .. py:method:: initialize_trainer(callbacks: List) -> lightning.Trainer

      Initializes a PyTorch Lightning Trainer instance.

      :param callbacks: A list of PyTorch Lightning callbacks to be used during training.
      :type callbacks: List

      :returns: The initialized PyTorch Lightning Trainer instance.
      :rtype: pl.Trainer


   .. py:method:: initialize_or_load_model() -> Tuple[dicee.models.base_model.BaseKGE, str]

      Initializes or loads a knowledge graph embedding model based on the training configuration.
      This method decides whether to start training from scratch or to continue training from a
      previously saved model state, depending on the `is_continual_training` attribute.

      :returns: * **model** (*BaseKGE*) -- The model instance that is either initialized from scratch or loaded from a saved state.
                  `BaseKGE` is a generic placeholder for the actual model class, which is a subclass of the
                  base knowledge graph embedding model class used in your implementation.
                * **form_of_labelling** (*str*) -- A string indicating the type of prediction task the model is configured for. Possible values
                  include "EntityPrediction" and "RelationPrediction", which signify whether the model is
                  trained to predict missing entities or relations in a knowledge graph. The actual values
                  depend on the specific tasks supported by your implementation.

      .. rubric:: Notes

      The method uses the `is_continual_training` attribute to determine if the model should be loaded
      from a saved state. If `is_continual_training` is True, the method attempts to load the model and its
      configuration from the specified `storage_path`. If `is_continual_training` is False or the model
      cannot be loaded, a new model instance is initialized.

      This method also sets the `form_of_labelling` attribute based on the model's configuration, which
      is used to inform downstream training and evaluation processes about the type of prediction task.


   .. py:method:: initialize_dataloader(dataset: torch.utils.data.Dataset) -> torch.utils.data.DataLoader

      Initializes and returns a PyTorch DataLoader object for the given dataset.

      This DataLoader is configured based on the training arguments provided,
      including batch size, shuffle status, and the number of workers.

      :param dataset: The dataset to be loaded into the DataLoader. This dataset should already
                      be processed and ready for training or evaluation.
      :type dataset: torch.utils.data.Dataset

      :returns: A DataLoader instance ready for training or evaluation, configured with the
                appropriate batch size, shuffle setting, and number of workers.
      :rtype: torch.utils.data.DataLoader


   .. py:method:: initialize_dataset(dataset: dicee.knowledge_graph.KG, form_of_labelling: str) -> torch.utils.data.Dataset

      Initializes and returns a dataset suitable for training or evaluation, based on the
      knowledge graph data and the specified form of labelling.

      :param dataset: The knowledge graph data used to construct the dataset. This should include
                      training, validation, and test sets along with any other necessary information
                      like entity and relation mappings.
      :type dataset: KG
      :param form_of_labelling: The form of labelling to be used for the dataset, indicating the prediction
                                task (e.g., "EntityPrediction", "RelationPrediction").
      :type form_of_labelling: str

      :returns: A processed dataset ready for use with a PyTorch DataLoader, tailored to the
                specified form of labelling and containing all necessary data for training
                or evaluation.
      :rtype: torch.utils.data.Dataset


   .. py:method:: start(knowledge_graph: dicee.knowledge_graph.KG) -> Tuple[dicee.models.base_model.BaseKGE, str]

      Starts the training process for the selected model using the provided knowledge graph data.
      The method selects and trains the model based on the configuration specified in the arguments.

      :param knowledge_graph: The knowledge graph data containing entities, relations, and triples, which will be used
                              for training the model.
      :type knowledge_graph: KG

      :returns: A tuple containing the trained model instance and the form of labelling used during
                training. The form of labelling indicates the type of prediction task.
      :rtype: Tuple[BaseKGE, str]


   .. py:method:: k_fold_cross_validation(dataset: dicee.knowledge_graph.KG) -> Tuple[dicee.models.base_model.BaseKGE, str]

      Conducts K-fold cross-validation on the provided dataset to assess the performance
      of the model specified in the training arguments. The process involves partitioning
      the dataset into K distinct subsets, iteratively using one subset for testing and
      the remainder for training. The model's performance is evaluated on each test split
      to compute the Mean Reciprocal Rank (MRR) scores.

      Steps:
      1. The dataset is divided into K train and test splits.
      2. For each split:
      2.1. A trainer and model are initialized based on the provided configuration.
      2.2. The model is trained using the training portion of the split.
      2.3. The MRR score of the trained model is computed using the test portion of the split.
      3. The process aggregates the MRR scores across all splits to report the mean and standard deviation
      of the MRR, providing a comprehensive evaluation of the model's performance.

      :param dataset: The dataset to be used for K-fold cross-validation. This dataset should include
                      the triples (head entity, relation, tail entity) for the entire knowledge graph.
      :type dataset: KG

      :returns: A tuple containing:
                - The trained model instance from the last fold of the cross-validation.
                - The form of labelling used during training, indicating the prediction task
                (e.g., "EntityPrediction", "RelationPrediction").
      :rtype: Tuple[BaseKGE, str]

      .. rubric:: Notes

      The function assumes the presence of a predefined number of folds (K) specified in
      the training arguments. It utilizes PyTorch Lightning for model training and evaluation,
      leveraging GPU acceleration if available. The final output includes the model trained
      on the last fold and a summary of the cross-validation performance metrics.



.. py:class:: KGE(path=None, url=None, construct_ensemble=False, model_name=None, apply_semantic_constraint=False)


   Bases: :py:obj:`dicee.abstracts.BaseInteractiveKGE`

   Knowledge Graph Embedding Class for interactive usage of pre-trained models

   .. py:method:: get_transductive_entity_embeddings(indices: Union[torch.LongTensor, List[str]], as_pytorch=False, as_numpy=False, as_list=True) -> Union[torch.FloatTensor, numpy.ndarray, List[float]]


   .. py:method:: create_vector_database(collection_name: str, distance: str, location: str = 'localhost', port: int = 6333)


   .. py:method:: generate(h='', r='')


   .. py:method:: __str__()

      Return str(self).


   .. py:method:: eval_lp_performance(dataset=List[Tuple[str, str, str]], filtered=True)


   .. py:method:: predict_missing_head_entity(relation: Union[List[str], str], tail_entity: Union[List[str], str], within=None) -> Tuple

      Given a relation and a tail entity, return top k ranked head entity.

      argmax_{e \in E } f(e,r,t), where r \in R, t \in E.

      Parameter
      ---------
      relation:  Union[List[str], str]

      String representation of selected relations.

      tail_entity: Union[List[str], str]

      String representation of selected entities.


      k: int

      Highest ranked k entities.

      Returns: Tuple
      ---------

      Highest K scores and entities


   .. py:method:: predict_missing_relations(head_entity: Union[List[str], str], tail_entity: Union[List[str], str], within=None) -> Tuple

      Given a head entity and a tail entity, return top k ranked relations.

      argmax_{r \in R } f(h,r,t), where h, t \in E.


      Parameter
      ---------
      head_entity: List[str]

      String representation of selected entities.

      tail_entity: List[str]

      String representation of selected entities.


      k: int

      Highest ranked k entities.

      Returns: Tuple
      ---------

      Highest K scores and entities


   .. py:method:: predict_missing_tail_entity(head_entity: Union[List[str], str], relation: Union[List[str], str], within: List[str] = None) -> torch.FloatTensor

      Given a head entity and a relation, return top k ranked entities

      argmax_{e \in E } f(h,r,e), where h \in E and r \in R.


      Parameter
      ---------
      head_entity: List[str]

      String representation of selected entities.

      tail_entity: List[str]

      String representation of selected entities.

      Returns: Tuple
      ---------

      scores


   .. py:method:: predict(*, h: Union[List[str], str] = None, r: Union[List[str], str] = None, t: Union[List[str], str] = None, within=None, logits=True) -> torch.FloatTensor

      :param logits:
      :param h:
      :param r:
      :param t:
      :param within:


   .. py:method:: predict_topk(*, h: List[str] = None, r: List[str] = None, t: List[str] = None, topk: int = 10, within: List[str] = None)

      Predict missing item in a given triple.



      Parameter
      ---------
      head_entity: List[str]

      String representation of selected entities.

      relation: List[str]

      String representation of selected relations.

      tail_entity: List[str]

      String representation of selected entities.


      k: int

      Highest ranked k item.

      Returns: Tuple
      ---------

      Highest K scores and items


   .. py:method:: triple_score(h: Union[List[str], str] = None, r: Union[List[str], str] = None, t: Union[List[str], str] = None, logits=False) -> torch.FloatTensor

      Predict triple score

      Parameter
      ---------
      head_entity: List[str]

      String representation of selected entities.

      relation: List[str]

      String representation of selected relations.

      tail_entity: List[str]

      String representation of selected entities.

      logits: bool

      If logits is True, unnormalized score returned

      Returns: Tuple
      ---------

      pytorch tensor of triple score


   .. py:method:: t_norm(tens_1: torch.Tensor, tens_2: torch.Tensor, tnorm: str = 'min') -> torch.Tensor


   .. py:method:: tensor_t_norm(subquery_scores: torch.FloatTensor, tnorm: str = 'min') -> torch.FloatTensor

      Compute T-norm over [0,1] ^{n   imes d} where n denotes the number of hops and d denotes number of entities


   .. py:method:: t_conorm(tens_1: torch.Tensor, tens_2: torch.Tensor, tconorm: str = 'min') -> torch.Tensor


   .. py:method:: negnorm(tens_1: torch.Tensor, lambda_: float, neg_norm: str = 'standard') -> torch.Tensor


   .. py:method:: return_multi_hop_query_results(aggregated_query_for_all_entities, k: int, only_scores)


   .. py:method:: single_hop_query_answering(query: tuple, only_scores: bool = True, k: int = None)


   .. py:method:: answer_multi_hop_query(query_type: str = None, query: Tuple[Union[str, Tuple[str, str]], Ellipsis] = None, queries: List[Tuple[Union[str, Tuple[str, str]], Ellipsis]] = None, tnorm: str = 'prod', neg_norm: str = 'standard', lambda_: float = 0.0, k: int = 10, only_scores=False) -> List[Tuple[str, torch.Tensor]]

      # @TODO: Refactoring is needed
      # @TODO: Score computation for each query type should be done in a static function

      Find an answer set for EPFO queries including negation and disjunction

      Parameter
      ----------
      query_type: str
      The type of the query, e.g., "2p".

      query: Union[str, Tuple[str, Tuple[str, str]]]
      The query itself, either a string or a nested tuple.

      queries: List of Tuple[Union[str, Tuple[str, str]], ...]

      tnorm: str
      The t-norm operator.

      neg_norm: str
      The negation norm.

      lambda_: float
      lambda parameter for sugeno and yager negation norms

      k: int
      The top-k substitutions for intermediate variables.

      :returns: * *List[Tuple[str, torch.Tensor]]*
                * *Entities and corresponding scores sorted in the descening order of scores*


   .. py:method:: find_missing_triples(confidence: float, entities: List[str] = None, relations: List[str] = None, topk: int = 10, at_most: int = sys.maxsize) -> Set

               Find missing triples

               Iterative over a set of entities E and a set of relation R :
      orall e \in E and
      orall r \in R f(e,r,x)
               Return (e,r,x)
      ot\in G and  f(e,r,x) > confidence

              Parameter
              ---------
              confidence: float

              A threshold for an output of a sigmoid function given a triple.

              topk: int

              Highest ranked k item to select triples with f(e,r,x) > confidence .

              at_most: int

              Stop after finding at_most missing triples

              Returns: Set
              ---------

              {(e,r,x) | f(e,r,x) > confidence \land (e,r,x)
      ot\in G



   .. py:method:: deploy(share: bool = False, top_k: int = 10)


   .. py:method:: train_triples(h: List[str], r: List[str], t: List[str], labels: List[float], iteration=2, optimizer=None)


   .. py:method:: train_k_vs_all(h, r, iteration=1, lr=0.001)

      Train k vs all
      :param head_entity:
      :param relation:
      :param iteration:
      :param lr:
      :return:


   .. py:method:: train(kg, lr=0.1, epoch=10, batch_size=32, neg_sample_ratio=10, num_workers=1) -> None

      Retrained a pretrain model on an input KG via negative sampling.



.. py:class:: Execute(args, continuous_training=False)


   A class for Training, Retraining and Evaluation a model.

   (1) Loading & Preprocessing & Serializing input data.
   (2) Training & Validation & Testing
   (3) Storing all necessary info

   .. py:method:: read_or_load_kg()


   .. py:method:: read_preprocess_index_serialize_data() -> None

      Read & Preprocess & Index & Serialize Input Data

      (1) Read or load the data from disk into memory.
      (2) Store the statistics of the data.

      Parameter
      ----------

      :rtype: None


   .. py:method:: load_indexed_data() -> None

      Load the indexed data from disk into memory

      Parameter
      ----------

      :rtype: None


   .. py:method:: save_trained_model() -> None

      Save a knowledge graph embedding model

      (1) Send model to eval mode and cpu.
      (2) Store the memory footprint of the model.
      (3) Save the model into disk.
      (4) Update the stats of KG again ?

      Parameter
      ----------

      :rtype: None


   .. py:method:: end(form_of_labelling: str) -> dict

      End training

      (1) Store trained model.
      (2) Report runtimes.
      (3) Eval model if required.

      Parameter
      ---------

      :rtype: A dict containing information about the training and/or evaluation


   .. py:method:: write_report() -> None

      Report training related information in a report.json file


   .. py:method:: start() -> dict

      Start training

      # (1) Loading the Data
      # (2) Create an evaluator object.
      # (3) Create a trainer object.
      # (4) Start the training

      Parameter
      ---------

      :rtype: A dict containing information about the training and/or evaluation



.. py:function:: mapping_from_first_two_cols_to_third(train_set_idx)


.. py:function:: timeit(func)


.. py:function:: load_pickle(file_path=str)


.. py:function:: reload_dataset(path: str, form_of_labelling, scoring_technique, neg_ratio, label_smoothing_rate)

   Reload the files from disk to construct the Pytorch dataset


.. py:function:: construct_dataset(*, train_set: Union[numpy.ndarray, list], valid_set=None, test_set=None, ordered_bpe_entities=None, train_target_indices=None, target_dim: int = None, entity_to_idx: dict, relation_to_idx: dict, form_of_labelling: str, scoring_technique: str, neg_ratio: int, label_smoothing_rate: float, byte_pair_encoding=None, block_size: int = None) -> torch.utils.data.Dataset


.. py:class:: BPE_NegativeSamplingDataset(train_set: torch.LongTensor, ordered_shaped_bpe_entities: torch.LongTensor, neg_ratio: int)


   Bases: :py:obj:`torch.utils.data.Dataset`

   An abstract class representing a :class:`Dataset`.

   All datasets that represent a map from keys to data samples should subclass
   it. All subclasses should overwrite :meth:`__getitem__`, supporting fetching a
   data sample for a given key. Subclasses could also optionally overwrite
   :meth:`__len__`, which is expected to return the size of the dataset by many
   :class:`~torch.utils.data.Sampler` implementations and the default options
   of :class:`~torch.utils.data.DataLoader`. Subclasses could also
   optionally implement :meth:`__getitems__`, for speedup batched samples
   loading. This method accepts list of indices of samples of batch and returns
   list of samples.

   .. note::
     :class:`~torch.utils.data.DataLoader` by default constructs an index
     sampler that yields integral indices.  To make it work with a map-style
     dataset with non-integral indices/keys, a custom sampler must be provided.

   .. py:method:: __len__()


   .. py:method:: __getitem__(idx)


   .. py:method:: collate_fn(batch_shaped_bpe_triples: List[Tuple[torch.Tensor, torch.Tensor]])



.. py:class:: MultiLabelDataset(train_set: torch.LongTensor, train_indices_target: torch.LongTensor, target_dim: int, torch_ordered_shaped_bpe_entities: torch.LongTensor)


   Bases: :py:obj:`torch.utils.data.Dataset`

   An abstract class representing a :class:`Dataset`.

   All datasets that represent a map from keys to data samples should subclass
   it. All subclasses should overwrite :meth:`__getitem__`, supporting fetching a
   data sample for a given key. Subclasses could also optionally overwrite
   :meth:`__len__`, which is expected to return the size of the dataset by many
   :class:`~torch.utils.data.Sampler` implementations and the default options
   of :class:`~torch.utils.data.DataLoader`. Subclasses could also
   optionally implement :meth:`__getitems__`, for speedup batched samples
   loading. This method accepts list of indices of samples of batch and returns
   list of samples.

   .. note::
     :class:`~torch.utils.data.DataLoader` by default constructs an index
     sampler that yields integral indices.  To make it work with a map-style
     dataset with non-integral indices/keys, a custom sampler must be provided.

   .. py:method:: __len__()


   .. py:method:: __getitem__(idx)



.. py:class:: MultiClassClassificationDataset(subword_units: numpy.ndarray, block_size: int = 8)


   Bases: :py:obj:`torch.utils.data.Dataset`

   Dataset for the 1vsALL training strategy

   :param train_set_idx: Indexed triples for the training.
   :param entity_idxs: mapping.
   :param relation_idxs: mapping.
   :param form: ?
   :param num_workers: int for https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader

   :rtype: torch.utils.data.Dataset

   .. py:method:: __len__()


   .. py:method:: __getitem__(idx)



.. py:class:: OnevsAllDataset(train_set_idx: numpy.ndarray, entity_idxs)


   Bases: :py:obj:`torch.utils.data.Dataset`

   Dataset for the 1vsALL training strategy

   :param train_set_idx: Indexed triples for the training.
   :param entity_idxs: mapping.
   :param relation_idxs: mapping.
   :param form: ?
   :param num_workers: int for https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader

   :rtype: torch.utils.data.Dataset

   .. py:method:: __len__()


   .. py:method:: __getitem__(idx)



.. py:class:: KvsAll(train_set_idx: numpy.ndarray, entity_idxs, relation_idxs, form, store=None, label_smoothing_rate: float = 0.0)


   Bases: :py:obj:`torch.utils.data.Dataset`

   Creates a dataset for KvsAll training by inheriting from torch.utils.data.Dataset.
       Let D denote a dataset for KvsAll training and be defined as D:= {(x,y)_i}_i ^N, where
       x: (h,r) is an unique tuple of an entity h \in E and a relation r \in R that has been seed in the input graph.
       y: denotes a multi-label vector \in [0,1]^{|E|} is a binary label.
   orall y_i =1 s.t. (h r E_i) \in KG

       .. note::
           TODO

       Parameters
       ----------
       train_set_idx : numpy.ndarray
           n by 3 array representing n triples

       entity_idxs : dictonary
           string representation of an entity to its integer id

       relation_idxs : dictonary
           string representation of a relation to its integer id

       Returns
       -------
       self : torch.utils.data.Dataset

       See Also
       --------

       Notes
       -----

       Examples
       --------
       >>> a = KvsAll()
       >>> a
       ? array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])


   .. py:method:: __len__()


   .. py:method:: __getitem__(idx)



.. py:class:: AllvsAll(train_set_idx: numpy.ndarray, entity_idxs, relation_idxs, label_smoothing_rate=0.0)


   Bases: :py:obj:`torch.utils.data.Dataset`

   Creates a dataset for AllvsAll training by inheriting from torch.utils.data.Dataset.
       Let D denote a dataset for AllvsAll training and be defined as D:= {(x,y)_i}_i ^N, where
       x: (h,r) is a possible unique tuple of an entity h \in E and a relation r \in R. Hence N = |E| x |R|
       y: denotes a multi-label vector \in [0,1]^{|E|} is a binary label.
   orall y_i =1 s.t. (h r E_i) \in KG

       .. note::
           AllvsAll extends KvsAll via none existing (h,r). Hence, it adds data points that are labelled without 1s,
            only with 0s.

       Parameters
       ----------
       train_set_idx : numpy.ndarray
           n by 3 array representing n triples

       entity_idxs : dictonary
           string representation of an entity to its integer id

       relation_idxs : dictonary
           string representation of a relation to its integer id

       Returns
       -------
       self : torch.utils.data.Dataset

       See Also
       --------

       Notes
       -----

       Examples
       --------
       >>> a = AllvsAll()
       >>> a
       ? array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])


   .. py:method:: __len__()


   .. py:method:: __getitem__(idx)



.. py:class:: KvsSampleDataset(train_set: numpy.ndarray, num_entities, num_relations, neg_sample_ratio: int = None, label_smoothing_rate: float = 0.0)


   Bases: :py:obj:`torch.utils.data.Dataset`

       KvsSample a Dataset:
           D:= {(x,y)_i}_i ^N, where
               . x:(h,r) is a unique h \in E and a relation r \in R and
               . y \in [0,1]^{|E|} is a binary label.
   orall y_i =1 s.t. (h r E_i) \in KG
              At each mini-batch construction, we subsample(y), hence n
               |new_y| << |E|
               new_y contains all 1's if sum(y)< neg_sample ratio
               new_y contains
          Parameters
          ----------
          train_set_idx
              Indexed triples for the training.
          entity_idxs
              mapping.
          relation_idxs
              mapping.
          form
              ?
          store
               ?
          label_smoothing_rate
              ?
          Returns
          -------
          torch.utils.data.Dataset


   .. py:method:: __len__()


   .. py:method:: __getitem__(idx)



.. py:class:: NegSampleDataset(train_set: numpy.ndarray, num_entities: int, num_relations: int, neg_sample_ratio: int = 1)


   Bases: :py:obj:`torch.utils.data.Dataset`

   An abstract class representing a :class:`Dataset`.

   All datasets that represent a map from keys to data samples should subclass
   it. All subclasses should overwrite :meth:`__getitem__`, supporting fetching a
   data sample for a given key. Subclasses could also optionally overwrite
   :meth:`__len__`, which is expected to return the size of the dataset by many
   :class:`~torch.utils.data.Sampler` implementations and the default options
   of :class:`~torch.utils.data.DataLoader`. Subclasses could also
   optionally implement :meth:`__getitems__`, for speedup batched samples
   loading. This method accepts list of indices of samples of batch and returns
   list of samples.

   .. note::
     :class:`~torch.utils.data.DataLoader` by default constructs an index
     sampler that yields integral indices.  To make it work with a map-style
     dataset with non-integral indices/keys, a custom sampler must be provided.

   .. py:method:: __len__()


   .. py:method:: __getitem__(idx)



.. py:class:: TriplePredictionDataset(train_set: numpy.ndarray, num_entities: int, num_relations: int, neg_sample_ratio: int = 1, label_smoothing_rate: float = 0.0)


   Bases: :py:obj:`torch.utils.data.Dataset`

       Triple Dataset

           D:= {(x)_i}_i ^N, where
               . x:(h,r, t) \in KG is a unique h \in E and a relation r \in R and
               . collact_fn => Generates negative triples

           collect_fn:
   orall (h,r,t) \in G obtain, create negative triples{(h,r,x),(,r,t),(h,m,t)}

           y:labels are represented in torch.float16
          Parameters
          ----------
          train_set_idx
              Indexed triples for the training.
          entity_idxs
              mapping.
          relation_idxs
              mapping.
          form
              ?
          store
               ?
          label_smoothing_rate


          collate_fn: batch:List[torch.IntTensor]
          Returns
          -------
          torch.utils.data.Dataset


   .. py:method:: __len__()


   .. py:method:: __getitem__(idx)


   .. py:method:: collate_fn(batch: List[torch.Tensor])



.. py:class:: CVDataModule(train_set_idx: numpy.ndarray, num_entities, num_relations, neg_sample_ratio, batch_size, num_workers)


   Bases: :py:obj:`pytorch_lightning.LightningDataModule`

   Create a Dataset for cross validation

   :param train_set_idx: Indexed triples for the training.
   :param num_entities: entity to index mapping.
   :param num_relations: relation to index mapping.
   :param batch_size: int
   :param form: ?
   :param num_workers: int for https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader

   :rtype: ?

   .. py:method:: train_dataloader() -> torch.utils.data.DataLoader

      An iterable or collection of iterables specifying training samples.

      For more information about multiple dataloaders, see this :ref:`section <multiple-dataloaders>`.

      The dataloader you return will not be reloaded unless you set
      :paramref:`~pytorch_lightning.trainer.trainer.Trainer.reload_dataloaders_every_n_epochs` to
      a positive integer.

      For data processing use the following pattern:

          - download in :meth:`prepare_data`
          - process and split in :meth:`setup`

      However, the above are only necessary for distributed processing.

      .. warning:: do not assign state in prepare_data

      - :meth:`~pytorch_lightning.trainer.trainer.Trainer.fit`
      - :meth:`prepare_data`
      - :meth:`setup`

      .. note::

         Lightning tries to add the correct sampler for distributed and arbitrary hardware.
         There is no need to set it yourself.


   .. py:method:: setup(*args, **kwargs)

      Called at the beginning of fit (train + validate), validate, test, or predict. This is a good hook when you
      need to build models dynamically or adjust something about them. This hook is called on every process when
      using DDP.

      :param stage: either ``'fit'``, ``'validate'``, ``'test'``, or ``'predict'``

      Example::

          class LitModel(...):
              def __init__(self):
                  self.l1 = None

              def prepare_data(self):
                  download_data()
                  tokenize()

                  # don't do this
                  self.something = else

              def setup(self, stage):
                  data = load_data(...)
                  self.l1 = nn.Linear(28, data.num_classes)



   .. py:method:: transfer_batch_to_device(*args, **kwargs)

      Override this hook if your :class:`~torch.utils.data.DataLoader` returns tensors wrapped in a custom data
      structure.

      The data types listed below (and any arbitrary nesting of them) are supported out of the box:

      - :class:`torch.Tensor` or anything that implements `.to(...)`
      - :class:`list`
      - :class:`dict`
      - :class:`tuple`

      For anything else, you need to define how the data is moved to the target device (CPU, GPU, TPU, ...).

      .. note::

         This hook should only transfer the data and not modify it, nor should it move the data to
         any other device than the one passed in as argument (unless you know what you are doing).
         To check the current state of execution of this hook you can use
         ``self.trainer.training/testing/validating/predicting`` so that you can
         add different logic as per your requirement.

      :param batch: A batch of data that needs to be transferred to a new device.
      :param device: The target device as defined in PyTorch.
      :param dataloader_idx: The index of the dataloader to which the batch belongs.

      :returns: A reference to the data on the new device.

      Example::

          def transfer_batch_to_device(self, batch, device, dataloader_idx):
              if isinstance(batch, CustomBatch):
                  # move all tensors in your custom data structure to the device
                  batch.samples = batch.samples.to(device)
                  batch.targets = batch.targets.to(device)
              elif dataloader_idx == 0:
                  # skip device transfer for the first dataloader or anything you wish
                  pass
              else:
                  batch = super().transfer_batch_to_device(batch, device, dataloader_idx)
              return batch

      :raises MisconfigurationException: If using IPUs, ``Trainer(accelerator='ipu')``.

      .. seealso::

         - :meth:`move_data_to_device`
         - :meth:`apply_to_collection`


   .. py:method:: prepare_data(*args, **kwargs)

      Use this to download and prepare data. Downloading and saving data with multiple processes (distributed
      settings) will result in corrupted data. Lightning ensures this method is called only within a single process,
      so you can safely add your downloading logic within.

      .. warning:: DO NOT set state to the model (use ``setup`` instead)
          since this is NOT called on every device

      Example::

          def prepare_data(self):
              # good
              download_data()
              tokenize()
              etc()

              # bad
              self.split = data_split
              self.some_state = some_other_state()

      In a distributed environment, ``prepare_data`` can be called in two ways
      (using :ref:`prepare_data_per_node<common/lightning_module:prepare_data_per_node>`)

      1. Once per node. This is the default and is only called on LOCAL_RANK=0.
      2. Once in total. Only called on GLOBAL_RANK=0.

      Example::

          # DEFAULT
          # called once per node on LOCAL_RANK=0 of that node
          class LitDataModule(LightningDataModule):
              def __init__(self):
                  super().__init__()
                  self.prepare_data_per_node = True


          # call on GLOBAL_RANK=0 (great for shared file systems)
          class LitDataModule(LightningDataModule):
              def __init__(self):
                  super().__init__()
                  self.prepare_data_per_node = False

      This is called before requesting the dataloaders:

      .. code-block:: python

          model.prepare_data()
          initialize_distributed()
          model.setup(stage)
          model.train_dataloader()
          model.val_dataloader()
          model.test_dataloader()
          model.predict_dataloader()




.. py:class:: QueryGenerator(train_path, val_path: str, test_path: str, ent2id: Dict = None, rel2id: Dict = None, seed: int = 1, gen_valid: bool = False, gen_test: bool = True)


   .. py:method:: list2tuple(list_data)


   .. py:method:: tuple2list(x: Union[List, Tuple]) -> Union[List, Tuple]

      Convert a nested tuple to a nested list.


   .. py:method:: set_global_seed(seed: int)

      Set seed


   .. py:method:: construct_graph(paths: List[str]) -> Tuple[Dict, Dict]

      Construct graph from triples
      Returns dicts with incoming and outgoing edges


   .. py:method:: fill_query(query_structure: List[Union[str, List]], ent_in: Dict, ent_out: Dict, answer: int) -> bool

      Private method for fill_query logic.


   .. py:method:: achieve_answer(query: List[Union[str, List]], ent_in: Dict, ent_out: Dict) -> set

      Private method for achieve_answer logic.
      @TODO: Document the code


   .. py:method:: write_links(ent_out, small_ent_out)


   .. py:method:: ground_queries(query_structure: List[Union[str, List]], ent_in: Dict, ent_out: Dict, small_ent_in: Dict, small_ent_out: Dict, gen_num: int, query_name: str)

      Generating queries and achieving answers


   .. py:method:: unmap(query_type, queries, tp_answers, fp_answers, fn_answers)


   .. py:method:: unmap_query(query_structure, query, id2ent, id2rel)


   .. py:method:: generate_queries(query_struct: List, gen_num: int, query_type: str)

      Passing incoming and outgoing edges to ground queries depending on mode [train valid or text]
      and getting queries and answers in return
      @ TODO: create a class for each single query struct


   .. py:method:: save_queries(query_type: str, gen_num: int, save_path: str)

      


   .. py:method:: load_queries(path)
      :abstractmethod:


   .. py:method:: get_queries(query_type: str, gen_num: int)


   .. py:method:: save_queries_and_answers(path: str, data: List[Tuple[str, Tuple[collections.defaultdict]]]) -> None
      :staticmethod:

      Save Queries into Disk


   .. py:method:: load_queries_and_answers(path: str) -> List[Tuple[str, Tuple[collections.defaultdict]]]
      :staticmethod:

      Load Queries from Disk to Memory



.. py:data:: __version__
   :value: '0.1.4'

   

