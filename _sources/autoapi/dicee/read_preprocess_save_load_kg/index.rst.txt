:py:mod:`dicee.read_preprocess_save_load_kg`
============================================

.. py:module:: dicee.read_preprocess_save_load_kg


Submodules
----------
.. toctree::
   :titlesonly:
   :maxdepth: 1

   preprocess/index.rst
   read_from_disk/index.rst
   save_load_disk/index.rst
   util/index.rst


Package Contents
----------------

Classes
~~~~~~~

.. autoapisummary::

   dicee.read_preprocess_save_load_kg.PreprocessKG
   dicee.read_preprocess_save_load_kg.LoadSaveToDisk
   dicee.read_preprocess_save_load_kg.ReadFromDisk




.. py:class:: PreprocessKG(kg)


   Preprocess the data in memory for a knowledge graph.

   This class handles the preprocessing of the knowledge graph data which includes
   reading the data, adding noise or reciprocal triples, constructing vocabularies,
   and indexing datasets based on the backend being used.

   .. attribute:: kg

      An instance representing the knowledge graph.

      :type: object

   .. method:: start() -> None

      Preprocess train, valid, and test datasets stored in the knowledge graph instance.


   .. method:: preprocess_with_byte_pair_encoding() -> None

      Preprocess the datasets using byte-pair encoding.


   .. method:: preprocess_with_pandas() -> None

      Preprocess the datasets using pandas.


   .. method:: preprocess_with_polars() -> None

      Preprocess the datasets using polars.


   .. method:: sequential_vocabulary_construction() -> None

      Construct integer indexing for entities and relations.


   .. py:method:: start() -> None

      Preprocess train, valid, and test datasets stored in the knowledge graph instance.

      This method applies the appropriate preprocessing technique based on the backend
      specified in the knowledge graph instance.

      :param None:

      :rtype: None

      :raises KeyError: If the specified backend is not supported.


   .. py:method:: preprocess_with_byte_pair_encoding() -> None

      Preprocess the datasets using byte-pair encoding (BPE).

      This method applies byte-pair encoding to the raw training, validation, and test sets of
      the knowledge graph. It transforms string representations of entities and relations into
      sequences of subword tokens. The method also handles padding of these sequences and
      constructs the necessary mappings for entities and relations.

      :param None:

      :rtype: None

      .. rubric:: Notes

      - Byte-pair encoding is used to handle the out-of-vocabulary problem in natural language
      processing by splitting words into more frequently occurring subword units.
      - This method modifies the knowledge graph instance in place by setting various attributes
      related to the byte-pair encoding such as padded sequences, mappings, and the maximum
      length of subword tokens.
      - The method assumes that the raw datasets are available as Pandas DataFrames within the
      knowledge graph instance.
      - If the 'add_reciprical' flag is set in the knowledge graph instance, reciprocal triples are
      added to the datasets.
      - After encoding and padding, the method also constructs mappings from the subword token
      sequences to their corresponding integer indices.


   .. py:method:: preprocess_with_byte_pair_encoding_with_padding() -> None


   .. py:method:: preprocess_with_pandas() -> None

      Preprocess train, valid, and test datasets stored in the knowledge graph instance using pandas.

      This method involves adding reciprocal or noisy triples, constructing vocabularies for entities and relations,
      and indexing the datasets. The preprocessing is performed using the pandas library, which facilitates the handling
      and transformation of the data.

      :param None:

      :rtype: None

      .. rubric:: Notes

      - The method begins by optionally adding reciprocal or noisy triples to the raw training, validation, and test sets.
      - Sequential vocabulary construction is performed to create a bijection mapping of entities and relations to integer indices.
      - The datasets (train, valid, test) are then indexed based on these mappings.
      - The method modifies the knowledge graph instance in place by setting various attributes such as the indexed datasets,
      the number of entities, and the number of relations.
      - The method assumes that the raw datasets are available as pandas DataFrames within the knowledge graph instance.
      - This preprocessing is crucial for converting the raw string-based datasets into a numerical format suitable for
      training machine learning models.


   .. py:method:: preprocess_with_polars() -> None

      Preprocess train, valid, and test datasets stored in the knowledge graph instance using Polars.

      This method involves preprocessing the datasets with the Polars library, which is designed for efficient data
      manipulation and indexing. The process includes adding reciprocal triples, indexing entities and relations,
      and transforming the datasets from string-based to integer-based formats.

      :param None:

      :rtype: None

      .. rubric:: Notes

      - The method begins by adding reciprocal triples to the raw datasets if the 'add_reciprical' flag is set
      in the knowledge graph instance.
      - It then constructs a bijection mapping from entities and relations to integer indices, using the unique
      entities and relations found in the concatenated datasets.
      - The datasets (train, valid, test) are indexed based on these mappings and converted to NumPy arrays.
      - The method updates the knowledge graph instance by setting attributes such as the number of entities,
      the number of relations, and the indexed datasets.
      - Polars is used for its performance advantages in handling large datasets and its efficient data manipulation capabilities.
      - This preprocessing step is crucial for converting the raw string-based datasets into a numerical format suitable
      for training machine learning models.


   .. py:method:: sequential_vocabulary_construction() -> None

      Construct sequential vocabularies for entities and relations in the knowledge graph.

      This method processes the raw training, validation, and test sets to create sequential mappings (bijection)
      of entities and relations to integer indices. These mappings are essential for converting the string-based
      representations of entities and relations to numerical formats that can be processed by machine learning models.

      :param None:

      :rtype: None

      .. rubric:: Notes

      - The method first concatenates the raw datasets and then creates unique lists of all entities and relations.
      - It then assigns a unique integer index to each entity and relation, creating two dictionaries:
      'entity_to_idx' and 'relation_to_idx'.
      - These dictionaries are used to index entities and relations in the knowledge graph.
      - The method updates the knowledge graph instance by setting attributes such as 'entity_to_idx',
      'relation_to_idx', 'num_entities', and 'num_relations'.
      - This method is a crucial preprocessing step for transforming knowledge graph data into a format suitable
      for training and evaluating machine learning models.
      - The method assumes that the raw datasets are available as Pandas DataFrames within the knowledge graph instance.


   .. py:method:: dept_remove_triples_from_train_with_condition()

      Remove specific triples from the training set based on a predefined condition.

      This method filters out triples from the raw training dataset of the knowledge graph based on
      a condition, such as the frequency of entities or relations. This is often used to refine the
      training data, for instance, by removing infrequent entities or relations that may not be
      significant for the model's training.

      :param None:

      :rtype: None

      .. rubric:: Notes

      - The method specifically targets the removal of triples that contain entities or relations
      occurring below a certain frequency threshold.
      - The frequency threshold is determined by the 'min_freq_for_vocab' attribute of the knowledge graph instance.
      - The method updates the knowledge graph instance by modifying the 'raw_train_set' attribute,
      which holds the raw training dataset.
      - This preprocessing step is crucial for ensuring the quality of the training data and can impact
      the performance and generalization ability of the resulting machine learning models.
      - The method assumes that the raw training dataset is available as a Pandas DataFrame within the
      knowledge graph instance.



.. py:class:: LoadSaveToDisk(kg)


   Handle the saving and loading of a knowledge graph to and from disk.

   This class provides functionality to serialize and deserialize the components of a knowledge graph,
   such as entity and relation indices, datasets, and byte-pair encoding mappings, to and from disk storage.

   .. attribute:: kg

      An instance of the knowledge graph to be saved or loaded.

      :type: object

   .. method:: save() -> None

      Save the knowledge graph components to disk.


   .. method:: load() -> None

      Load the knowledge graph components from disk.


   .. py:method:: save()

      Save the knowledge graph components to disk.

      This method serializes various components of the knowledge graph such as entity and relation indices,
      datasets, and byte-pair encoding mappings, and saves them to the specified file paths in the knowledge
      graph instance. The method handles different data types and structures based on the configuration of
      the knowledge graph.

      :param None:

      :rtype: None

      :raises AssertionError: If the path for serialization is not set or other required conditions are not met.

      .. rubric:: Notes

      - The method checks if the 'path_for_serialization' attribute is set in the knowledge graph instance.
      - Depending on the configuration (e.g., whether byte-pair encoding is used), different components are saved.
      - The method uses custom functions like 'save_pickle' and 'save_numpy_ndarray' for serialization.


   .. py:method:: load()

      Load the knowledge graph components from disk.

      This method deserializes various components of the knowledge graph such as entity and relation indices,
      datasets, and byte-pair encoding mappings from the specified file paths in the knowledge graph instance.
      The method reconstructs the knowledge graph instance with the loaded data.

      :param None:

      :rtype: None

      :raises AssertionError: If the path for deserialization is not set or other required conditions are not met.

      .. rubric:: Notes

      - The method checks if the 'path_for_deserialization' attribute is set in the knowledge graph instance.
      - The method updates the knowledge graph instance with the loaded components.
      - The method uses custom functions like 'load_pickle' and 'load_numpy_ndarray' for deserialization.
      - If evaluation models are used, additional components like vocabularies and constraints are also loaded.



.. py:class:: ReadFromDisk(kg)


   Read the data from disk into memory.

   This class is responsible for loading a knowledge graph from various sources such as
   disk files, triple stores, or SPARQL endpoints, and then making it available in memory
   for further processing.

   .. attribute:: kg

      An instance representing the knowledge graph.

      :type: object

   .. method:: start() -> None

      Read a knowledge graph from disk into memory.

   .. method:: add_noisy_triples_into_training() -> None

      Add noisy triples into the training set of the knowledge graph.


   .. py:method:: start() -> None

      Read a knowledge graph from disk into memory.

      This method reads the knowledge graph data from the specified source (disk, triple store,
      or SPARQL endpoint) and loads it into memory. The data is made available in the
      train_set, test_set, and valid_set attributes of the knowledge graph instance.

      :param None:

      :rtype: None

      :raises RuntimeError: If the data source is invalid or not specified correctly.


   .. py:method:: add_noisy_triples_into_training() -> None

      Add noisy triples into the training set of the knowledge graph.

      This method injects a specified proportion of noisy triples into the training set.
      Noisy triples are randomly generated by shuffling the entities and relations in the
      knowledge graph. The purpose of adding noisy triples is often to test the robustness
      of the model or to augment the training data.

      :param None:

      :rtype: None

      .. rubric:: Notes

      The number of noisy triples added is determined by the 'add_noise_rate' attribute
      of the knowledge graph. The method ensures that the total number of triples (original
      plus noisy) in the training set matches the expected count after adding the noisy triples.



