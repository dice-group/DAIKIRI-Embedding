dicee.trainer.dice_trainer
==========================

.. py:module:: dicee.trainer.dice_trainer


Classes
-------

.. autoapisummary::

   dicee.trainer.dice_trainer.BaseKGE
   dicee.trainer.dice_trainer.ASWA
   dicee.trainer.dice_trainer.Eval
   dicee.trainer.dice_trainer.KronE
   dicee.trainer.dice_trainer.PrintCallback
   dicee.trainer.dice_trainer.AccumulateEpochLossCallback
   dicee.trainer.dice_trainer.Perturb
   dicee.trainer.dice_trainer.TorchTrainer
   dicee.trainer.dice_trainer.TorchDDPTrainer
   dicee.trainer.dice_trainer.KG
   dicee.trainer.dice_trainer.DICE_Trainer


Functions
---------

.. autoapisummary::

   dicee.trainer.dice_trainer.select_model
   dicee.trainer.dice_trainer.construct_dataset
   dicee.trainer.dice_trainer.reload_dataset
   dicee.trainer.dice_trainer.timeit
   dicee.trainer.dice_trainer.initialize_trainer
   dicee.trainer.dice_trainer.get_callbacks


Module Contents
---------------

.. py:class:: BaseKGE(args: dict)

   Bases: :py:obj:`BaseKGELightning`


   Base class for all neural network modules.

   Your models should also subclass this class.

   Modules can also contain other Modules, allowing to nest them in
   a tree structure. You can assign the submodules as regular attributes::

       import torch.nn as nn
       import torch.nn.functional as F

       class Model(nn.Module):
           def __init__(self):
               super().__init__()
               self.conv1 = nn.Conv2d(1, 20, 5)
               self.conv2 = nn.Conv2d(20, 20, 5)

           def forward(self, x):
               x = F.relu(self.conv1(x))
               return F.relu(self.conv2(x))

   Submodules assigned in this way will be registered, and will have their
   parameters converted too when you call :meth:`to`, etc.

   .. note::
       As per the example above, an ``__init__()`` call to the parent class
       must be made before assignment on the child.

   :ivar training: Boolean represents whether this module is in training or
                   evaluation mode.
   :vartype training: bool


   .. py:method:: forward_byte_pair_encoded_k_vs_all(x: torch.LongTensor)

      :param x:
      :type x: B x 2 x T



   .. py:method:: forward_byte_pair_encoded_triple(x: Tuple[torch.LongTensor, torch.LongTensor])

      byte pair encoded neural link predictors

      :param -------:



   .. py:method:: init_params_with_sanity_checking()


   .. py:method:: forward(x: Union[torch.LongTensor, Tuple[torch.LongTensor, torch.LongTensor]], y_idx: torch.LongTensor = None)

      :param x:
      :param y_idx:
      :param ordered_bpe_entities:



   .. py:method:: forward_triples(x: torch.LongTensor) -> torch.Tensor

      :param x:



   .. py:method:: forward_k_vs_all(*args, **kwargs)


   .. py:method:: forward_k_vs_sample(*args, **kwargs)


   .. py:method:: get_triple_representation(idx_hrt)


   .. py:method:: get_head_relation_representation(indexed_triple)


   .. py:method:: get_sentence_representation(x: torch.LongTensor)

      :param x shape (b:
      :param 3:
      :param t):



   .. py:method:: get_bpe_head_and_relation_representation(x: torch.LongTensor) -> Tuple[torch.FloatTensor, torch.FloatTensor]

      :param x:
      :type x: B x 2 x T



   .. py:method:: get_embeddings() -> Tuple[numpy.ndarray, numpy.ndarray]


.. py:function:: select_model(args: dict, is_continual_training: bool = None, storage_path: str = None)

.. py:class:: ASWA(num_epochs, path)

   Bases: :py:obj:`dicee.abstracts.AbstractCallback`


   Adaptive stochastic weight averaging
   ASWE keeps track of the validation performance and update s the ensemble model accordingly.


   .. py:method:: on_fit_end(trainer, model)

      Call at the end of the training.

      Parameter
      ---------
      trainer:

      model:

      :rtype: None



   .. py:method:: compute_mrr(trainer, model) -> float
      :staticmethod:



   .. py:method:: get_aswa_state_dict(model)


   .. py:method:: decide(running_model_state_dict, ensemble_state_dict, val_running_model, mrr_updated_ensemble_model)

      Perform Hard Update, software or rejection

      :param running_model_state_dict:
      :param ensemble_state_dict:
      :param val_running_model:
      :param mrr_updated_ensemble_model:



   .. py:method:: on_train_epoch_end(trainer, model)

      Call at the end of each epoch during training.

      Parameter
      ---------
      trainer:

      model:

      :rtype: None



.. py:class:: Eval(path, epoch_ratio: int = None)

   Bases: :py:obj:`dicee.abstracts.AbstractCallback`


   Abstract class for Callback class for knowledge graph embedding models


   Parameter
   ---------



   .. py:method:: on_fit_start(trainer, model)

      Call at the beginning of the training.

      Parameter
      ---------
      trainer:

      model:

      :rtype: None



   .. py:method:: on_fit_end(trainer, model)

      Call at the end of the training.

      Parameter
      ---------
      trainer:

      model:

      :rtype: None



   .. py:method:: on_train_epoch_end(trainer, model)

      Call at the end of each epoch during training.

      Parameter
      ---------
      trainer:

      model:

      :rtype: None



   .. py:method:: on_train_batch_end(*args, **kwargs)

      Call at the end of each mini-batch during the training.


      Parameter
      ---------
      trainer:

      model:

      :rtype: None



.. py:class:: KronE

   Bases: :py:obj:`dicee.abstracts.AbstractCallback`


   Abstract class for Callback class for knowledge graph embedding models


   Parameter
   ---------



   .. py:method:: batch_kronecker_product(a, b)
      :staticmethod:


      Kronecker product of matrices a and b with leading batch dimensions.
      Batch dimensions are broadcast. The number of them mush
      :type a: torch.Tensor
      :type b: torch.Tensor
      :rtype: torch.Tensor



   .. py:method:: get_kronecker_triple_representation(indexed_triple: torch.LongTensor)

      Get kronecker embeddings



   .. py:method:: on_fit_start(trainer, model)

      Call at the beginning of the training.

      Parameter
      ---------
      trainer:

      model:

      :rtype: None



.. py:class:: PrintCallback

   Bases: :py:obj:`dicee.abstracts.AbstractCallback`


   Abstract class for Callback class for knowledge graph embedding models


   Parameter
   ---------



   .. py:method:: on_fit_start(trainer, pl_module)

      Call at the beginning of the training.

      Parameter
      ---------
      trainer:

      model:

      :rtype: None



   .. py:method:: on_fit_end(trainer, pl_module)

      Call at the end of the training.

      Parameter
      ---------
      trainer:

      model:

      :rtype: None



   .. py:method:: on_train_batch_end(*args, **kwargs)

      Call at the end of each mini-batch during the training.


      Parameter
      ---------
      trainer:

      model:

      :rtype: None



   .. py:method:: on_train_epoch_end(*args, **kwargs)

      Call at the end of each epoch during training.

      Parameter
      ---------
      trainer:

      model:

      :rtype: None



.. py:class:: AccumulateEpochLossCallback(path: str)

   Bases: :py:obj:`dicee.abstracts.AbstractCallback`


   Abstract class for Callback class for knowledge graph embedding models


   Parameter
   ---------



   .. py:method:: on_fit_end(trainer, model) -> None

      Store epoch loss


      Parameter
      ---------
      trainer:

      model:

      :rtype: None



.. py:class:: Perturb(level: str = 'input', ratio: float = 0.0, method: str = None, scaler: float = None, frequency=None)

   Bases: :py:obj:`dicee.abstracts.AbstractCallback`


   A callback for a three-Level Perturbation

   Input Perturbation: During training an input x is perturbed by randomly replacing its element.
   In the context of knowledge graph embedding models, x can denote a triple, a tuple of an entity and a relation,
   or a tuple of two entities.
   A perturbation means that a component of x is randomly replaced by an entity or a relation.

   Parameter Perturbation:

   Output Perturbation:


   .. py:method:: on_train_batch_start(trainer, model, batch, batch_idx)

      Called when the train batch begins.



.. py:function:: construct_dataset(*, train_set: Union[numpy.ndarray, list], valid_set=None, test_set=None, ordered_bpe_entities=None, train_target_indices=None, target_dim: int = None, entity_to_idx: dict, relation_to_idx: dict, form_of_labelling: str, scoring_technique: str, neg_ratio: int, label_smoothing_rate: float, byte_pair_encoding=None, block_size: int = None) -> torch.utils.data.Dataset

.. py:function:: reload_dataset(path: str, form_of_labelling, scoring_technique, neg_ratio, label_smoothing_rate)

   Reload the files from disk to construct the Pytorch dataset


.. py:class:: TorchTrainer(args, callbacks)

   Bases: :py:obj:`dicee.abstracts.AbstractTrainer`


    TorchTrainer for using single GPU or multi CPUs on a single node

    Arguments
   ----------
   args: ?

   callbacks: list of Abstract callback instances



   .. py:method:: fit(*args, train_dataloaders, **kwargs) -> None

       Training starts

       Arguments
      ----------
      args:tuple
      (BASEKGE,)
      kwargs:Tuple
          empty dictionary
      :rtype: batch loss (float)



   .. py:method:: forward_backward_update(x_batch: torch.Tensor, y_batch: torch.Tensor) -> torch.Tensor

       Compute forward, loss, backward, and parameter update

       Arguments
      ----------
      x_batch:(torch.Tensor) mini-batch inputs
      y_batch:(torch.Tensor) mini-batch outputs

      :rtype: batch loss (float)



   .. py:method:: extract_input_outputs_set_device(batch: list) -> Tuple

       Construct inputs and outputs from a batch of inputs with outputs From a batch of inputs and put

       Arguments
      ----------
      batch: (list) mini-batch inputs on CPU

      :rtype: (tuple) mini-batch on select device



.. py:class:: TorchDDPTrainer(args, callbacks)

   Bases: :py:obj:`dicee.abstracts.AbstractTrainer`


    A Trainer based on torch.nn.parallel.DistributedDataParallel

    Arguments
   ----------
   train_set_idx
       Indexed triples for the training.
   entity_idxs
       mapping.
   relation_idxs
       mapping.
   form
       ?
   store
        ?
   label_smoothing_rate
        Using hard targets (0,1) drives weights to infinity.
        An outlier produces enormous gradients.

   :rtype: torch.utils.data.Dataset


   .. py:method:: fit(*args, **kwargs)

      Train model



.. py:function:: timeit(func)

.. py:class:: KG(dataset_dir: str = None, byte_pair_encoding: bool = False, padding: bool = False, add_noise_rate: float = None, sparql_endpoint: str = None, path_single_kg: str = None, path_for_deserialization: str = None, add_reciprical: bool = None, eval_model: str = None, read_only_few: int = None, sample_triples_ratio: float = None, path_for_serialization: str = None, entity_to_idx=None, relation_to_idx=None, backend=None, training_technique: str = None)

   Knowledge Graph


   .. py:property:: entities_str
      :type: List



   .. py:property:: relations_str
      :type: List



   .. py:method:: func_triple_to_bpe_representation(triple: List[str])


.. py:function:: initialize_trainer(args, callbacks)

.. py:function:: get_callbacks(args)

.. py:class:: DICE_Trainer(args, is_continual_training, storage_path, evaluator=None)

   DICE_Trainer implement
    1- Pytorch Lightning trainer (https://pytorch-lightning.readthedocs.io/en/stable/common/trainer.html)
    2- Multi-GPU Trainer(https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html)
    3- CPU Trainer

    Parameter
    ---------
    args

    is_continual_training:bool

    storage_path:str

    evaluator:

    Returns
    -------
    report:dict



   .. py:method:: continual_start()

      (1) Initialize training.
      (2) Load model
      (3) Load trainer
      (3) Fit model

      Parameter
      ---------

      :returns: * *model*
                * **form_of_labelling** (*str*)



   .. py:method:: initialize_trainer(callbacks: List) -> lightning.Trainer

      Initialize Trainer from input arguments



   .. py:method:: initialize_or_load_model()


   .. py:method:: initialize_dataloader(dataset: torch.utils.data.Dataset) -> torch.utils.data.DataLoader


   .. py:method:: initialize_dataset(dataset: dicee.knowledge_graph.KG, form_of_labelling) -> torch.utils.data.Dataset


   .. py:method:: start(knowledge_graph: dicee.knowledge_graph.KG) -> Tuple[dicee.models.base_model.BaseKGE, str]

      Train selected model via the selected training strategy



   .. py:method:: k_fold_cross_validation(dataset) -> Tuple[dicee.models.base_model.BaseKGE, str]

      Perform K-fold Cross-Validation

      1. Obtain K train and test splits.
      2. For each split,
          2.1 initialize trainer and model
          2.2. Train model with configuration provided in args.
          2.3. Compute the mean reciprocal rank (MRR) score of the model on the test respective split.
      3. Report the mean and average MRR .

      :param self:
      :param dataset:
      :return: model



