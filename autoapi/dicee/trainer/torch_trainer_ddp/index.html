<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../../../../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>dicee.trainer.torch_trainer_ddp &mdash; DICE Embeddings 0.1.3.2 documentation</title>
      <link rel="stylesheet" type="text/css" href="../../../../_static/pygments.css?v=8775fe07" />
      <link rel="stylesheet" type="text/css" href="../../../../_static/css/theme.css?v=19f00094" />
      <link rel="stylesheet" type="text/css" href="../../../../_static/graphviz.css?v=eafc0fe6" />
      <link rel="stylesheet" type="text/css" href="../../../../_static/theme.css?v=ea877efc" />
      <link rel="stylesheet" type="text/css" href="../../../../_static/theme_tweak.css?v=f0ad19f3" />
      <link rel="stylesheet" type="text/css" href="../../../../_static/pygments.css?v=8775fe07" />

  
  <!--[if lt IE 9]>
    <script src="../../../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script src="../../../../_static/jquery.js?v=5d32c60e"></script>
        <script src="../../../../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
        <script src="../../../../_static/documentation_options.js?v=c6726a90"></script>
        <script src="../../../../_static/doctools.js?v=888ff710"></script>
        <script src="../../../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../search.html" />
    <link rel="next" title="dicee.abstracts" href="../../abstracts/index.html" />
    <link rel="prev" title="dicee.trainer.torch_trainer" href="../torch_trainer/index.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../../../../index.html" class="icon icon-home">
            DICE Embeddings
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../../../../usage/main.html">Dicee Manual</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../usage/main.html#installation">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../usage/main.html#download-knowledge-graphs">Download Knowledge Graphs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../usage/main.html#knowledge-graph-embedding-models">Knowledge Graph Embedding Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../usage/main.html#how-to-train">How to Train</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../usage/main.html#creating-an-embedding-vector-database">Creating an Embedding Vector Database</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../usage/main.html#answering-complex-queries">Answering Complex Queries</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../usage/main.html#predicting-missing-links">Predicting Missing Links</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../usage/main.html#downloading-pretrained-models">Downloading Pretrained Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../usage/main.html#how-to-deploy">How to Deploy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../usage/main.html#docker">Docker</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../usage/main.html#how-to-cite">How to cite</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../../index.html"><code class="xref py py-mod docutils literal notranslate"><span class="pre">dicee</span></code></a><ul class="current">
<li class="toctree-l2 current"><a class="reference internal" href="../../index.html#subpackages">Subpackages</a><ul class="current">
<li class="toctree-l3"><a class="reference internal" href="../../models/index.html"><code class="xref py py-mod docutils literal notranslate"><span class="pre">dicee.models</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../read_preprocess_save_load_kg/index.html"><code class="xref py py-mod docutils literal notranslate"><span class="pre">dicee.read_preprocess_save_load_kg</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../scripts/index.html"><code class="xref py py-mod docutils literal notranslate"><span class="pre">dicee.scripts</span></code></a></li>
<li class="toctree-l3 current"><a class="reference internal" href="../index.html"><code class="xref py py-mod docutils literal notranslate"><span class="pre">dicee.trainer</span></code></a><ul class="current">
<li class="toctree-l4 current"><a class="reference internal" href="../index.html#submodules">Submodules</a><ul class="current">
<li class="toctree-l5"><a class="reference internal" href="../dice_trainer/index.html"><code class="xref py py-mod docutils literal notranslate"><span class="pre">dicee.trainer.dice_trainer</span></code></a></li>
<li class="toctree-l5"><a class="reference internal" href="../torch_trainer/index.html"><code class="xref py py-mod docutils literal notranslate"><span class="pre">dicee.trainer.torch_trainer</span></code></a></li>
<li class="toctree-l5 current"><a class="current reference internal" href="#"><code class="xref py py-mod docutils literal notranslate"><span class="pre">dicee.trainer.torch_trainer_ddp</span></code></a><ul>
<li class="toctree-l6"><a class="reference internal" href="#module-contents">Module Contents</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l4"><a class="reference internal" href="../index.html#package-contents">Package Contents</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../index.html#submodules">Submodules</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../index.html#package-contents">Package Contents</a></li>
</ul>
</li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../../index.html">DICE Embeddings</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../../../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="../../index.html"><code class="xref py py-mod docutils literal notranslate"><span class="pre">dicee</span></code></a></li>
          <li class="breadcrumb-item"><a href="../index.html"><code class="xref py py-mod docutils literal notranslate"><span class="pre">dicee.trainer</span></code></a></li>
      <li class="breadcrumb-item active"><code class="xref py py-mod docutils literal notranslate"><span class="pre">dicee.trainer.torch_trainer_ddp</span></code></li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../../../_sources/autoapi/dicee/trainer/torch_trainer_ddp/index.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="module-dicee.trainer.torch_trainer_ddp">
<span id="dicee-trainer-torch-trainer-ddp"></span><h1><a class="reference internal" href="#module-dicee.trainer.torch_trainer_ddp" title="dicee.trainer.torch_trainer_ddp"><code class="xref py py-mod docutils literal notranslate"><span class="pre">dicee.trainer.torch_trainer_ddp</span></code></a><a class="headerlink" href="#module-dicee.trainer.torch_trainer_ddp" title="Link to this heading"></a></h1>
<section id="module-contents">
<h2>Module Contents<a class="headerlink" href="#module-contents" title="Link to this heading"></a></h2>
<section id="classes">
<h3>Classes<a class="headerlink" href="#classes" title="Link to this heading"></a></h3>
<table class="autosummary longtable docutils align-default">
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#dicee.trainer.torch_trainer_ddp.TorchDDPTrainer" title="dicee.trainer.torch_trainer_ddp.TorchDDPTrainer"><code class="xref py py-obj docutils literal notranslate"><span class="pre">TorchDDPTrainer</span></code></a></p></td>
<td><p>A Trainer class that leverages PyTorch's DistributedDataParallel (DDP) for distributed training across</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#dicee.trainer.torch_trainer_ddp.NodeTrainer" title="dicee.trainer.torch_trainer_ddp.NodeTrainer"><code class="xref py py-obj docutils literal notranslate"><span class="pre">NodeTrainer</span></code></a></p></td>
<td><p>Manages the training process of a PyTorch model on a single node in a distributed training setup using</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#dicee.trainer.torch_trainer_ddp.DDPTrainer" title="dicee.trainer.torch_trainer_ddp.DDPTrainer"><code class="xref py py-obj docutils literal notranslate"><span class="pre">DDPTrainer</span></code></a></p></td>
<td><p>Distributed Data Parallel (DDP) Trainer for PyTorch models. Orchestrates the model training across multiple GPUs</p></td>
</tr>
</tbody>
</table>
</section>
<section id="functions">
<h3>Functions<a class="headerlink" href="#functions" title="Link to this heading"></a></h3>
<table class="autosummary longtable docutils align-default">
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#dicee.trainer.torch_trainer_ddp.print_peak_memory" title="dicee.trainer.torch_trainer_ddp.print_peak_memory"><code class="xref py py-obj docutils literal notranslate"><span class="pre">print_peak_memory</span></code></a>(→ None)</p></td>
<td><p>Prints the peak memory usage for the specified device during the execution.</p></td>
</tr>
</tbody>
</table>
<dl class="py function">
<dt class="sig sig-object py" id="dicee.trainer.torch_trainer_ddp.print_peak_memory">
<span class="sig-prename descclassname"><span class="pre">dicee.trainer.torch_trainer_ddp.</span></span><span class="sig-name descname"><span class="pre">print_peak_memory</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">prefix</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="headerlink" href="#dicee.trainer.torch_trainer_ddp.print_peak_memory" title="Link to this definition"></a></dt>
<dd><p>Prints the peak memory usage for the specified device during the execution.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>prefix</strong> (<em>str</em>) – A prefix string to include in the print statement for context or identification
of the memory usage check point.</p></li>
<li><p><strong>device</strong> (<em>int</em>) – The device index for which to check the peak memory usage. This is typically
used for CUDA devices. For example, <cite>device=0</cite> refers to the first CUDA device.</p></li>
</ul>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p>None</p>
</dd>
</dl>
<p class="rubric">Notes</p>
<p>This function is specifically useful for monitoring the peak memory usage of GPU
devices in CUDA context. The memory usage is reported in megabytes (MB). This can
help in debugging memory issues or for optimizing memory usage in deep learning models.
It requires PyTorch’s CUDA utilities to be available and will print the peak allocated
memory on the specified CUDA device. If the device is not a CUDA device or if PyTorch
is not compiled with CUDA support, this function will not display memory usage.</p>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="dicee.trainer.torch_trainer_ddp.TorchDDPTrainer">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">dicee.trainer.torch_trainer_ddp.</span></span><span class="sig-name descname"><span class="pre">TorchDDPTrainer</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">callbacks</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">lightning.Callback</span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#dicee.trainer.torch_trainer_ddp.TorchDDPTrainer" title="Link to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="../../abstracts/index.html#dicee.abstracts.AbstractTrainer" title="dicee.abstracts.AbstractTrainer"><code class="xref py py-obj docutils literal notranslate"><span class="pre">dicee.abstracts.AbstractTrainer</span></code></a></p>
<p>A Trainer class that leverages PyTorch’s DistributedDataParallel (DDP) for distributed training across
multiple GPUs. This trainer is designed for training models in a distributed fashion using multiple
GPUs either on a single machine or across multiple nodes.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>args</strong> (<em>argparse.Namespace</em>) – The command-line arguments namespace, containing training hyperparameters and configurations.</p></li>
<li><p><strong>callbacks</strong> (<em>List</em><em>[</em><em>lightening.Callback</em><em>]</em>) – A list of PyTorch Lightning Callbacks to be called during the training process.</p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="dicee.trainer.torch_trainer_ddp.TorchDDPTrainer.train_set_idx">
<span class="sig-name descname"><span class="pre">train_set_idx</span></span><a class="headerlink" href="#dicee.trainer.torch_trainer_ddp.TorchDDPTrainer.train_set_idx" title="Link to this definition"></a></dt>
<dd><p>An array of indexed triples for training the model.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>np.ndarray</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="dicee.trainer.torch_trainer_ddp.TorchDDPTrainer.entity_idxs">
<span class="sig-name descname"><span class="pre">entity_idxs</span></span><a class="headerlink" href="#dicee.trainer.torch_trainer_ddp.TorchDDPTrainer.entity_idxs" title="Link to this definition"></a></dt>
<dd><p>A dictionary mapping entity names to their corresponding indexes.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>Dict[str, int]</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="dicee.trainer.torch_trainer_ddp.TorchDDPTrainer.relation_idxs">
<span class="sig-name descname"><span class="pre">relation_idxs</span></span><a class="headerlink" href="#dicee.trainer.torch_trainer_ddp.TorchDDPTrainer.relation_idxs" title="Link to this definition"></a></dt>
<dd><p>A dictionary mapping relation names to their corresponding indexes.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>Dict[str, int]</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="dicee.trainer.torch_trainer_ddp.TorchDDPTrainer.form">
<span class="sig-name descname"><span class="pre">form</span></span><a class="headerlink" href="#dicee.trainer.torch_trainer_ddp.TorchDDPTrainer.form" title="Link to this definition"></a></dt>
<dd><p>The form of training to be used. This parameter specifies how the training data is presented
to the model, e.g., ‘EntityPrediction’, ‘RelationPrediction’.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>str</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="dicee.trainer.torch_trainer_ddp.TorchDDPTrainer.store">
<span class="sig-name descname"><span class="pre">store</span></span><a class="headerlink" href="#dicee.trainer.torch_trainer_ddp.TorchDDPTrainer.store" title="Link to this definition"></a></dt>
<dd><p>The path to where the trained model and other artifacts are stored.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>str</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="dicee.trainer.torch_trainer_ddp.TorchDDPTrainer.label_smoothing_rate">
<span class="sig-name descname"><span class="pre">label_smoothing_rate</span></span><a class="headerlink" href="#dicee.trainer.torch_trainer_ddp.TorchDDPTrainer.label_smoothing_rate" title="Link to this definition"></a></dt>
<dd><p>The rate of label smoothing to apply to the loss function. Using label smoothing helps in
regularizing the model and preventing overfitting by softening the hard targets.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">fit(self,</span> <span class="pre">\*args,</span> <span class="pre">\*\*kwargs):</span></span></dt>
<dd><p>Trains the model using distributed data parallelism. This method initializes the distributed
process group, creates a distributed data loader, and starts the training process using a
NodeTrainer instance. It handles the setup and teardown of the distributed training environment.</p>
</dd></dl>

<p class="rubric">Notes</p>
<ul class="simple">
<li><p>This trainer requires the PyTorch library and is designed to work with GPUs.</p></li>
<li><p>Proper setup of the distributed environment variables (e.g., WORLD_SIZE, RANK, LOCAL_RANK) is
necessary before using this trainer.</p></li>
<li><p>The ‘nccl’ backend is used for GPU-based distributed training.</p></li>
<li><p>It’s important to ensure that the same number of batches is available across all participating
processes to avoid hanging issues.</p></li>
</ul>
<dl class="py method">
<dt class="sig sig-object py" id="dicee.trainer.torch_trainer_ddp.TorchDDPTrainer.fit">
<span class="sig-name descname"><span class="pre">fit</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#dicee.trainer.torch_trainer_ddp.TorchDDPTrainer.fit" title="Link to this definition"></a></dt>
<dd><p>Trains the model using Distributed Data Parallel (DDP). This method initializes the
distributed environment, creates a distributed sampler for the DataLoader, and starts
the training process.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>*args</strong> (<em>Model</em>) – The model to be trained. Passed as a positional argument.</p></li>
<li><p><strong>**kwargs</strong> (<em>dict</em>) – <p>Additional keyword arguments, including:
- train_dataloaders: DataLoader</p>
<blockquote>
<div><p>The DataLoader for the training dataset. Must contain a ‘dataset’ attribute.</p>
</div></blockquote>
</p></li>
</ul>
</dd>
<dt class="field-even">Raises<span class="colon">:</span></dt>
<dd class="field-even"><p><strong>AssertionError</strong> – If the number of arguments is not equal to 1 (i.e., the model is not provided).</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>None</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="dicee.trainer.torch_trainer_ddp.NodeTrainer">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">dicee.trainer.torch_trainer_ddp.</span></span><span class="sig-name descname"><span class="pre">NodeTrainer</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">trainer</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">model</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">torch.nn.Module</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">train_dataset_loader</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">torch.utils.data.DataLoader</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">optimizer</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">torch.optim.Optimizer</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">callbacks</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_epochs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#dicee.trainer.torch_trainer_ddp.NodeTrainer" title="Link to this definition"></a></dt>
<dd><p>Manages the training process of a PyTorch model on a single node in a distributed training setup using
DistributedDataParallel (DDP). This class orchestrates the training process across multiple GPUs on the node,
handling batch processing, loss computation, and optimizer steps.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>trainer</strong> (<a class="reference internal" href="../../abstracts/index.html#dicee.abstracts.AbstractTrainer" title="dicee.abstracts.AbstractTrainer"><em>AbstractTrainer</em></a>) – The higher-level trainer instance managing the overall training process.</p></li>
<li><p><strong>model</strong> (<em>torch.nn.Module</em>) – The PyTorch model to be trained.</p></li>
<li><p><strong>train_dataset_loader</strong> (<em>DataLoader</em>) – The DataLoader providing access to the training data, properly batched and shuffled.</p></li>
<li><p><strong>optimizer</strong> (<em>torch.optim.Optimizer</em>) – The optimizer used for updating model parameters.</p></li>
<li><p><strong>callbacks</strong> (<em>list</em>) – A list of callbacks to be executed during training, such as model checkpointing.</p></li>
<li><p><strong>num_epochs</strong> (<em>int</em>) – The total number of epochs to train the model.</p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="dicee.trainer.torch_trainer_ddp.NodeTrainer.local_rank">
<span class="sig-name descname"><span class="pre">local_rank</span></span><a class="headerlink" href="#dicee.trainer.torch_trainer_ddp.NodeTrainer.local_rank" title="Link to this definition"></a></dt>
<dd><p>The rank of the GPU on the current node, used for GPU-specific operations.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="dicee.trainer.torch_trainer_ddp.NodeTrainer.global_rank">
<span class="sig-name descname"><span class="pre">global_rank</span></span><a class="headerlink" href="#dicee.trainer.torch_trainer_ddp.NodeTrainer.global_rank" title="Link to this definition"></a></dt>
<dd><p>The global rank of the process in the distributed training setup.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="dicee.trainer.torch_trainer_ddp.NodeTrainer.loss_func">
<span class="sig-name descname"><span class="pre">loss_func</span></span><a class="headerlink" href="#dicee.trainer.torch_trainer_ddp.NodeTrainer.loss_func" title="Link to this definition"></a></dt>
<dd><p>The loss function used to compute the difference between the model predictions and targets.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>callable</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="dicee.trainer.torch_trainer_ddp.NodeTrainer.loss_history">
<span class="sig-name descname"><span class="pre">loss_history</span></span><a class="headerlink" href="#dicee.trainer.torch_trainer_ddp.NodeTrainer.loss_history" title="Link to this definition"></a></dt>
<dd><p>A list to record the history of loss values over epochs.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>list</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">_run_batch(self,</span> <span class="pre">source,</span> <span class="pre">targets):</span></span></dt>
<dd><p>Processes a single batch of data, performing a forward pass, loss computation, and an optimizer step.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">extract_input_outputs(self,</span> <span class="pre">z):</span></span></dt>
<dd><p>Extracts and sends input data and targets to the appropriate device.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">_run_epoch(self,</span> <span class="pre">epoch):</span></span></dt>
<dd><p>Performs a single pass over the training dataset, returning the average loss for the epoch.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">train(self):</span></span></dt>
<dd><p>Executes the training process, iterating over epochs and managing DDP-specific configurations.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="dicee.trainer.torch_trainer_ddp.NodeTrainer.extract_input_outputs">
<span class="sig-name descname"><span class="pre">extract_input_outputs</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">z</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">list</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">tuple</span></span></span><a class="headerlink" href="#dicee.trainer.torch_trainer_ddp.NodeTrainer.extract_input_outputs" title="Link to this definition"></a></dt>
<dd><p>Processes the batch data, ensuring it is on the correct device.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>z</strong> (<em>list</em>) – The batch data, which can vary in structure depending on the training setup.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The processed input and output data, ready for model training.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>tuple</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="dicee.trainer.torch_trainer_ddp.NodeTrainer.train">
<span class="sig-name descname"><span class="pre">train</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="headerlink" href="#dicee.trainer.torch_trainer_ddp.NodeTrainer.train" title="Link to this definition"></a></dt>
<dd><p>The main training loop. Iterates over all epochs, processing each batch of data.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>None</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="dicee.trainer.torch_trainer_ddp.DDPTrainer">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">dicee.trainer.torch_trainer_ddp.</span></span><span class="sig-name descname"><span class="pre">DDPTrainer</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">torch.nn.Module</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">train_dataset_loader</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">torch.utils.data.DataLoader</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">optimizer</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">torch.optim.Optimizer</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">gpu_id</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">callbacks</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_epochs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#dicee.trainer.torch_trainer_ddp.DDPTrainer" title="Link to this definition"></a></dt>
<dd><p>Distributed Data Parallel (DDP) Trainer for PyTorch models. Orchestrates the model training across multiple GPUs
by wrapping the model with PyTorch’s DDP. It manages the training loop, loss computation, and optimization steps.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>model</strong> (<em>torch.nn.Module</em>) – The model to be trained in a distributed manner.</p></li>
<li><p><strong>train_dataset_loader</strong> (<em>DataLoader</em>) – DataLoader providing access to the training data, properly batched and shuffled.</p></li>
<li><p><strong>optimizer</strong> (<em>torch.optim.Optimizer</em>) – The optimizer to be used for updating the model’s parameters.</p></li>
<li><p><strong>gpu_id</strong> (<em>int</em>) – The GPU identifier where the model is to be placed.</p></li>
<li><p><strong>callbacks</strong> (<em>List</em><em>[</em><em>Callable</em><em>]</em>) – A list of callback functions to be called during training.</p></li>
<li><p><strong>num_epochs</strong> (<em>int</em>) – The number of epochs for which the model will be trained.</p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="dicee.trainer.torch_trainer_ddp.DDPTrainer.loss_history">
<span class="sig-name descname"><span class="pre">loss_history</span></span><a class="headerlink" href="#dicee.trainer.torch_trainer_ddp.DDPTrainer.loss_history" title="Link to this definition"></a></dt>
<dd><p>Records the history of loss values over training epochs.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>list</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="dicee.trainer.torch_trainer_ddp.DDPTrainer._run_batch">
<span class="sig-name descname"><span class="pre">_run_batch</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">source</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">torch.Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">targets</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">torch.Tensor</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">float:</span></span></span><a class="headerlink" href="#dicee.trainer.torch_trainer_ddp.DDPTrainer._run_batch" title="Link to this definition"></a></dt>
<dd><p>Executes a forward pass, computes the loss, performs a backward pass, and updates the model parameters for
a single batch of data.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="dicee.trainer.torch_trainer_ddp.DDPTrainer.extract_input_outputs">
<span class="sig-name descname"><span class="pre">extract_input_outputs</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">z</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tuple[torch.Tensor,</span> <span class="pre">torch.Tensor]:</span></span></span><a class="headerlink" href="#dicee.trainer.torch_trainer_ddp.DDPTrainer.extract_input_outputs" title="Link to this definition"></a></dt>
<dd><p>Processes the batch data, ensuring it is on the correct device.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="dicee.trainer.torch_trainer_ddp.DDPTrainer._run_epoch">
<span class="sig-name descname"><span class="pre">_run_epoch</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">epoch</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">float:</span></span></span><a class="headerlink" href="#dicee.trainer.torch_trainer_ddp.DDPTrainer._run_epoch" title="Link to this definition"></a></dt>
<dd><p>Completes one full pass over the entire dataset and computes the average loss for the epoch.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="dicee.trainer.torch_trainer_ddp.DDPTrainer.train">
<span class="sig-name descname"><span class="pre">train</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None:</span></span></span><a class="headerlink" href="#dicee.trainer.torch_trainer_ddp.DDPTrainer.train" title="Link to this definition"></a></dt>
<dd><p>Starts the training process, iterating through epochs and managing the distributed training operations.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="id0">
<span class="sig-name descname"><span class="pre">extract_input_outputs</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">z</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#id0" title="Link to this definition"></a></dt>
<dd><p>Extracts and moves input and target tensors to the correct device.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>z</strong> (<em>List</em><em>[</em><em>torch.Tensor</em><em>]</em>) – A batch of data from the DataLoader.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>Inputs and targets, moved to the correct device.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>Tuple[torch.Tensor, torch.Tensor]</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="id1">
<span class="sig-name descname"><span class="pre">train</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="headerlink" href="#id1" title="Link to this definition"></a></dt>
<dd><p>Trains the model across specified epochs and GPUs using DDP.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>None</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="../torch_trainer/index.html" class="btn btn-neutral float-left" title="dicee.trainer.torch_trainer" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="../../abstracts/index.html" class="btn btn-neutral float-right" title="dicee.abstracts" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2023, Caglar Demir.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>