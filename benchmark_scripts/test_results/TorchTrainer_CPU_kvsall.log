============================= test session starts ==============================
platform linux -- Python 3.9.12, pytest-6.2.5, py-1.11.0, pluggy-1.0.0 -- /upb/users/r/renzhong/profiles/unix/cs/.conda/envs/pykeen/bin/python
cachedir: .pytest_cache
rootdir: /local/upb/users/r/renzhong/profiles/unix/cs/Dicee/dice-embeddings
plugins: anyio-3.6.2
collecting ... collected 41 items

tests/test_regression_pykeen.py::test_model[Pykeen_DistMult] PASSED      [  2%]
tests/test_regression_pykeen.py::test_model[Pykeen_TuckER] PASSED        [  4%]
tests/test_regression_pykeen.py::test_model[Pykeen_UM] PASSED            [  7%]
tests/test_regression_pykeen.py::test_model[Pykeen_TransR] PASSED        [  9%]
tests/test_regression_pykeen.py::test_model[Pykeen_TransH] PASSED        [ 12%]
tests/test_regression_pykeen.py::test_model[Pykeen_TransF] PASSED        [ 14%]
tests/test_regression_pykeen.py::test_model[Pykeen_TransE] PASSED        [ 17%]
tests/test_regression_pykeen.py::test_model[Pykeen_TransD] PASSED        [ 19%]
tests/test_regression_pykeen.py::test_model[Pykeen_TorusE] PASSED        [ 21%]
tests/test_regression_pykeen.py::test_model[Pykeen_SimplE] PASSED        [ 24%]
tests/test_regression_pykeen.py::test_model[Pykeen_SE] PASSED            [ 26%]
tests/test_regression_pykeen.py::test_model[Pykeen_RESCAL] PASSED        [ 29%]
tests/test_regression_pykeen.py::test_model[Pykeen_RotatE] PASSED        [ 31%]
tests/test_regression_pykeen.py::test_model[Pykeen_QuatE] PASSED         [ 34%]
tests/test_regression_pykeen.py::test_model[Pykeen_PairRE] PASSED        [ 36%]
tests/test_regression_pykeen.py::test_model[Pykeen_ProjE] PASSED         [ 39%]
tests/test_regression_pykeen.py::test_model[Pykeen_NTN] PASSED           [ 41%]
tests/test_regression_pykeen.py::test_model[Pykeen_NodePiece] PASSED     [ 43%]
tests/test_regression_pykeen.py::test_model[Pykeen_MuRE] PASSED          [ 46%]
tests/test_regression_pykeen.py::test_model[Pykeen_KG2E] PASSED          [ 48%]
tests/test_regression_pykeen.py::test_model[Pykeen_InductiveNodePiece] FAILED [ 51%]
tests/test_regression_pykeen.py::test_model[Pykeen_InductiveNodePieceGNN] FAILED [ 53%]
tests/test_regression_pykeen.py::test_model[Pykeen_HolE] PASSED          [ 56%]
tests/test_regression_pykeen.py::test_model[Pykeen_FixedModel] PASSED    [ 58%]
tests/test_regression_pykeen.py::test_model[Pykeen_ERMLPE] PASSED        [ 60%]
tests/test_regression_pykeen.py::test_model[Pykeen_DistMA] PASSED        [ 63%]
tests/test_regression_pykeen.py::test_model[Pykeen_CrossE] PASSED        [ 65%]
tests/test_regression_pykeen.py::test_model[Pykeen_CooccurrenceFilteredModel] PASSED [ 68%]
tests/test_regression_pykeen.py::test_model[Pykeen_ConvKB] PASSED        [ 70%]
tests/test_regression_pykeen.py::test_model[Pykeen_ConvE] PASSED         [ 73%]
tests/test_regression_pykeen.py::test_model[Pykeen_ComplExLiteral] FAILED [ 75%]
tests/test_regression_pykeen.py::test_model[Pykeen_ComplEx] PASSED       [ 78%]
tests/test_regression_pykeen.py::test_model[Pykeen_CompGCN] FAILED       [ 80%]
tests/test_regression_pykeen.py::test_model[Pykeen_CP] PASSED            [ 82%]
tests/test_regression_pykeen.py::test_model[Pykeen_BoxE] PASSED          [ 85%]
tests/test_regression_pykeen.py::test_model[Pykeen_AutoSF] PASSED        [ 87%]
tests/test_regression_pykeen.py::test_model[Pykeen_DistMultLiteral] FAILED [ 90%]
tests/test_regression_pykeen.py::test_pykeenInteraction[Pykeen_TripleREInteraction] PASSED [ 92%]
tests/test_regression_pykeen.py::test_pykeenInteraction[Pykeen_TransformerInteraction] PASSED [ 95%]
tests/test_regression_pykeen.py::test_pykeenInteraction[Pykeen_MultiLinearTuckerInteraction] PASSED [ 97%]
tests/test_regression_pykeen.py::test_pykeenInteraction[Pykeen_LineaREInteraction] PASSED [100%]

=================================== FAILURES ===================================
____________________ test_model[Pykeen_InductiveNodePiece] _____________________

model_name = 'Pykeen_InductiveNodePiece'

    @pytest.mark.filterwarnings("ignore::UserWarning")
    @pytest.mark.parametrize(
        "model_name",
        [
            "Pykeen_DistMult",
            "Pykeen_TuckER",
            "Pykeen_UM",
            "Pykeen_TransR",
            "Pykeen_TransH",
            "Pykeen_TransF",
            "Pykeen_TransE",
            "Pykeen_TransD",
            "Pykeen_TorusE",
            "Pykeen_SimplE",
            "Pykeen_SE",
            "Pykeen_RESCAL",
            "Pykeen_RotatE",
            "Pykeen_QuatE",
            "Pykeen_PairRE",
            "Pykeen_ProjE",
            "Pykeen_NTN",
            "Pykeen_NodePiece",
            "Pykeen_MuRE",
            "Pykeen_KG2E",
            "Pykeen_InductiveNodePiece",
            "Pykeen_InductiveNodePieceGNN",
            "Pykeen_HolE",
            "Pykeen_FixedModel",
            "Pykeen_ERMLPE",
            "Pykeen_DistMA",
            "Pykeen_CrossE",
            "Pykeen_CooccurrenceFilteredModel",
            "Pykeen_ConvKB",  # this one is really slow
            "Pykeen_ConvE",
            "Pykeen_ComplExLiteral",
            "Pykeen_ComplEx",
            "Pykeen_CompGCN",
            "Pykeen_CP",
            "Pykeen_BoxE",
            "Pykeen_AutoSF",
            "Pykeen_DistMultLiteral",
        ],
    )
    def test_model(model_name):
        args = template(model_name)
        # config = {
        #     "epoch":args.num_epochs,"lr":args.lr,"embedding_dim":args.embedding_dim
        # }
        # dataset = args.path_dataset_folder.split('/')[1]
        # wandb.setup(wandb.Settings(program="test_pykeen_model.py", program_relpath="test_pykeen_model.py"))
        # wandb.init(project="dice_demo",config=config,name=f'{args.model}-{dataset}')
>       Execute(args).start()

tests/test_regression_pykeen.py:108: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
dicee/executer.py:212: in start
    self.trained_model, form_of_labelling = self.trainer.start(dataset=self.dataset)
dicee/trainer/dice_trainer.py:238: in start
    model, form_of_labelling = self.initialize_or_load_model(dataset)
dicee/static_funcs.py:50: in timeit_wrapper
    result = func(*args, **kwargs)
dicee/trainer/dice_trainer.py:185: in initialize_or_load_model
    model, form_of_labelling = select_model(
dicee/static_funcs.py:97: in select_model
    intialize_model(args, dataset)
dicee/static_funcs.py:525: in intialize_model
    model = get_pykeen_model(model_name, args, dataset)
dicee/static_funcs.py:505: in get_pykeen_model
    model = MyLCWALitModule(
dicee/models/pykeen_LCWALitModule.py:11: in __init__
    super().__init__(**kwargs)
/upb/users/r/renzhong/profiles/unix/cs/.conda/envs/pykeen/lib/python3.9/site-packages/pykeen/contrib/lightning.py:112: in __init__
    self.model = model_resolver.make(model, model_kwargs, triples_factory=self.dataset.training)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <class_resolver.api.ClassResolver object at 0x7fa476742fa0>
query = 'InductiveNodePiece'
pos_kwargs = {'embedding_dim': 64, 'loss': 'BCEWithLogitsLoss'}
kwargs = {'triples_factory': TriplesFactory(num_entities=14, num_relations=110, create_inverse_triples=False, num_triples=3184)}
cls = <class 'pykeen.models.inductive.inductive_nodepiece.InductiveNodePiece'>

    def make(
        self,
        query: HintOrType[X],
        pos_kwargs: Optional[Mapping[str, Any]] = None,
        **kwargs,
    ) -> X:
        """Instantiate a class with optional kwargs."""
        if query is None or isinstance(query, (str, type)):
            cls: Type[X] = self.lookup(query)
            try:
                return cls(**(pos_kwargs or {}), **kwargs)  # type: ignore
            except TypeError as e:
                if "required keyword-only argument" in e.args[0]:
>                   raise KeywordArgumentError(cls, e.args[0]) from None
E                   class_resolver.api.KeywordArgumentError: InductiveNodePiece: __init__() missing 1 required keyword-only argument: 'inference_factory'

/upb/users/r/renzhong/profiles/unix/cs/.conda/envs/pykeen/lib/python3.9/site-packages/class_resolver/api.py:207: KeywordArgumentError
----------------------------- Captured stdout call -----------------------------
Start time:2023-07-05 19:49:11.072548
*** Read or Load Knowledge Graph  ***
*** Reading KGs/Nations/test.txt with Pandas ***
Reading with pandas.read_csv with sep ** s+ ** ...
Took 0.0015 seconds | Current Memory Usage  2468.1 in MB
*** Reading KGs/Nations/train.txt with Pandas ***
Reading with pandas.read_csv with sep ** s+ ** ...
Took 0.0016 seconds | Current Memory Usage  2468.1 in MB
Unrecognized data KGs/Nations/literals.txt
*** Reading KGs/Nations/valid.txt with Pandas ***
Reading with pandas.read_csv with sep ** s+ ** ...
Took 0.0013 seconds | Current Memory Usage  2468.1 in MB
[3.1 / 14] Add reciprocal triples to train, validation, and test sets, e.g. KG:= {(s,p,o)} union {(o,p_inverse,s)}
Done !


Concatenating data to obtain index...
Done !

Creating a mapping from entities to integer indexes...
Done !

Done ! 0.003 seconds

Done !

Done !

Took 0.0102 seconds | Current Memory Usage  2468.1 in MB
Data Type conversion...
Submit er-vocab, re-vocab, and ee-vocab via  ProcessPoolExecutor...
Preprocessing took: 0.082 seconds

------------------- Description of Dataset KGs/Nations -------------------
Number of entities:14
Number of relations:110
Number of triples on train set:3184
Number of triples on valid set:398
Number of triples on test set:402
Entity Index:0.00000 in GB
Relation Index:0.00000 in GB
Train set :0.00001 in GB

# of CPUs:64 | # of GPUs:1 | # of CPUs for dataloader:1
NVIDIA GeForce RTX 3090
------------------- Train -------------------
Initializing TorchTrainer CPU Trainer...	Took 0.0004 seconds | Current Memory Usage  2468.1 in MB
Initializing Model...	Initializing the selected model... False
___________________ test_model[Pykeen_InductiveNodePieceGNN] ___________________

model_name = 'Pykeen_InductiveNodePieceGNN'

    @pytest.mark.filterwarnings("ignore::UserWarning")
    @pytest.mark.parametrize(
        "model_name",
        [
            "Pykeen_DistMult",
            "Pykeen_TuckER",
            "Pykeen_UM",
            "Pykeen_TransR",
            "Pykeen_TransH",
            "Pykeen_TransF",
            "Pykeen_TransE",
            "Pykeen_TransD",
            "Pykeen_TorusE",
            "Pykeen_SimplE",
            "Pykeen_SE",
            "Pykeen_RESCAL",
            "Pykeen_RotatE",
            "Pykeen_QuatE",
            "Pykeen_PairRE",
            "Pykeen_ProjE",
            "Pykeen_NTN",
            "Pykeen_NodePiece",
            "Pykeen_MuRE",
            "Pykeen_KG2E",
            "Pykeen_InductiveNodePiece",
            "Pykeen_InductiveNodePieceGNN",
            "Pykeen_HolE",
            "Pykeen_FixedModel",
            "Pykeen_ERMLPE",
            "Pykeen_DistMA",
            "Pykeen_CrossE",
            "Pykeen_CooccurrenceFilteredModel",
            "Pykeen_ConvKB",  # this one is really slow
            "Pykeen_ConvE",
            "Pykeen_ComplExLiteral",
            "Pykeen_ComplEx",
            "Pykeen_CompGCN",
            "Pykeen_CP",
            "Pykeen_BoxE",
            "Pykeen_AutoSF",
            "Pykeen_DistMultLiteral",
        ],
    )
    def test_model(model_name):
        args = template(model_name)
        # config = {
        #     "epoch":args.num_epochs,"lr":args.lr,"embedding_dim":args.embedding_dim
        # }
        # dataset = args.path_dataset_folder.split('/')[1]
        # wandb.setup(wandb.Settings(program="test_pykeen_model.py", program_relpath="test_pykeen_model.py"))
        # wandb.init(project="dice_demo",config=config,name=f'{args.model}-{dataset}')
>       Execute(args).start()

tests/test_regression_pykeen.py:108: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
dicee/executer.py:212: in start
    self.trained_model, form_of_labelling = self.trainer.start(dataset=self.dataset)
dicee/trainer/dice_trainer.py:238: in start
    model, form_of_labelling = self.initialize_or_load_model(dataset)
dicee/static_funcs.py:50: in timeit_wrapper
    result = func(*args, **kwargs)
dicee/trainer/dice_trainer.py:185: in initialize_or_load_model
    model, form_of_labelling = select_model(
dicee/static_funcs.py:97: in select_model
    intialize_model(args, dataset)
dicee/static_funcs.py:525: in intialize_model
    model = get_pykeen_model(model_name, args, dataset)
dicee/static_funcs.py:505: in get_pykeen_model
    model = MyLCWALitModule(
dicee/models/pykeen_LCWALitModule.py:11: in __init__
    super().__init__(**kwargs)
/upb/users/r/renzhong/profiles/unix/cs/.conda/envs/pykeen/lib/python3.9/site-packages/pykeen/contrib/lightning.py:112: in __init__
    self.model = model_resolver.make(model, model_kwargs, triples_factory=self.dataset.training)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <class_resolver.api.ClassResolver object at 0x7fa476742fa0>
query = 'InductiveNodePieceGNN'
pos_kwargs = {'embedding_dim': 64, 'loss': 'BCEWithLogitsLoss'}
kwargs = {'triples_factory': TriplesFactory(num_entities=14, num_relations=110, create_inverse_triples=False, num_triples=3184)}
cls = <class 'pykeen.models.inductive.inductive_nodepiece_gnn.InductiveNodePieceGNN'>

    def make(
        self,
        query: HintOrType[X],
        pos_kwargs: Optional[Mapping[str, Any]] = None,
        **kwargs,
    ) -> X:
        """Instantiate a class with optional kwargs."""
        if query is None or isinstance(query, (str, type)):
            cls: Type[X] = self.lookup(query)
            try:
                return cls(**(pos_kwargs or {}), **kwargs)  # type: ignore
            except TypeError as e:
                if "required keyword-only argument" in e.args[0]:
>                   raise KeywordArgumentError(cls, e.args[0]) from None
E                   class_resolver.api.KeywordArgumentError: InductiveNodePieceGNN: __init__() missing 1 required keyword-only argument: 'inference_factory'

/upb/users/r/renzhong/profiles/unix/cs/.conda/envs/pykeen/lib/python3.9/site-packages/class_resolver/api.py:207: KeywordArgumentError
----------------------------- Captured stdout call -----------------------------
Start time:2023-07-05 19:49:11.713333
*** Read or Load Knowledge Graph  ***
*** Reading KGs/Nations/test.txt with Pandas ***
Reading with pandas.read_csv with sep ** s+ ** ...
Took 0.0020 seconds | Current Memory Usage  2467.5 in MB
*** Reading KGs/Nations/train.txt with Pandas ***
Reading with pandas.read_csv with sep ** s+ ** ...
Took 0.0016 seconds | Current Memory Usage  2467.5 in MB
Unrecognized data KGs/Nations/literals.txt
*** Reading KGs/Nations/valid.txt with Pandas ***
Reading with pandas.read_csv with sep ** s+ ** ...
Took 0.0011 seconds | Current Memory Usage  2467.5 in MB
[3.1 / 14] Add reciprocal triples to train, validation, and test sets, e.g. KG:= {(s,p,o)} union {(o,p_inverse,s)}
Done !


Concatenating data to obtain index...
Done !

Creating a mapping from entities to integer indexes...
Done !

Done ! 0.003 seconds

Done !

Done !

Took 0.0100 seconds | Current Memory Usage  2467.5 in MB
Data Type conversion...
Submit er-vocab, re-vocab, and ee-vocab via  ProcessPoolExecutor...
Preprocessing took: 0.098 seconds

------------------- Description of Dataset KGs/Nations -------------------
Number of entities:14
Number of relations:110
Number of triples on train set:3184
Number of triples on valid set:398
Number of triples on test set:402
Entity Index:0.00000 in GB
Relation Index:0.00000 in GB
Train set :0.00001 in GB

# of CPUs:64 | # of GPUs:1 | # of CPUs for dataloader:1
NVIDIA GeForce RTX 3090
------------------- Train -------------------
Initializing TorchTrainer CPU Trainer...	Took 0.0014 seconds | Current Memory Usage  2467.5 in MB
Initializing Model...	Initializing the selected model... False
______________________ test_model[Pykeen_ComplExLiteral] _______________________

model_name = 'Pykeen_ComplExLiteral'

    @pytest.mark.filterwarnings("ignore::UserWarning")
    @pytest.mark.parametrize(
        "model_name",
        [
            "Pykeen_DistMult",
            "Pykeen_TuckER",
            "Pykeen_UM",
            "Pykeen_TransR",
            "Pykeen_TransH",
            "Pykeen_TransF",
            "Pykeen_TransE",
            "Pykeen_TransD",
            "Pykeen_TorusE",
            "Pykeen_SimplE",
            "Pykeen_SE",
            "Pykeen_RESCAL",
            "Pykeen_RotatE",
            "Pykeen_QuatE",
            "Pykeen_PairRE",
            "Pykeen_ProjE",
            "Pykeen_NTN",
            "Pykeen_NodePiece",
            "Pykeen_MuRE",
            "Pykeen_KG2E",
            "Pykeen_InductiveNodePiece",
            "Pykeen_InductiveNodePieceGNN",
            "Pykeen_HolE",
            "Pykeen_FixedModel",
            "Pykeen_ERMLPE",
            "Pykeen_DistMA",
            "Pykeen_CrossE",
            "Pykeen_CooccurrenceFilteredModel",
            "Pykeen_ConvKB",  # this one is really slow
            "Pykeen_ConvE",
            "Pykeen_ComplExLiteral",
            "Pykeen_ComplEx",
            "Pykeen_CompGCN",
            "Pykeen_CP",
            "Pykeen_BoxE",
            "Pykeen_AutoSF",
            "Pykeen_DistMultLiteral",
        ],
    )
    def test_model(model_name):
        args = template(model_name)
        # config = {
        #     "epoch":args.num_epochs,"lr":args.lr,"embedding_dim":args.embedding_dim
        # }
        # dataset = args.path_dataset_folder.split('/')[1]
        # wandb.setup(wandb.Settings(program="test_pykeen_model.py", program_relpath="test_pykeen_model.py"))
        # wandb.init(project="dice_demo",config=config,name=f'{args.model}-{dataset}')
>       Execute(args).start()

tests/test_regression_pykeen.py:108: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
dicee/executer.py:215: in start
    return self.end(start_time, form_of_labelling)
dicee/executer.py:152: in end
    self.save_trained_model()
dicee/static_funcs.py:50: in timeit_wrapper
    result = func(*args, **kwargs)
dicee/executer.py:119: in save_trained_model
    store(trainer=self.trainer,
dicee/static_funcs.py:318: in store
    save_embeddings(
dicee/static_funcs.py:615: in save_embeddings
    df = pd.DataFrame(embeddings, index=_indexes)
/upb/users/r/renzhong/profiles/unix/cs/.conda/envs/pykeen/lib/python3.9/site-packages/pandas/core/frame.py:720: in __init__
    mgr = ndarray_to_mgr(
/upb/users/r/renzhong/profiles/unix/cs/.conda/envs/pykeen/lib/python3.9/site-packages/pandas/core/internals/construction.py:349: in ndarray_to_mgr
    _check_values_indices_shape_match(values, index, columns)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

values = array([[-0.12480122-0.00338577j, -0.05872995+0.13953967j,
        -0.04187514+0.07684474j, ...,  0.07967759+0.16677085...74j  , ..., -0.03296027+0.1299885j ,
        -0.10907213-0.00619368j,  0.238706  +0.11586554j]],
      dtype=complex64)
index = Index(['militaryalliance', 'intergovorgs3', 'relbooktranslations',
       'timesincewar', 'negativebehavior', 'relinte...erse', 'warning_inverse', 'lostterritory_inverse',
       'severdiplomatic_inverse'],
      dtype='object', length=110)
columns = RangeIndex(start=0, stop=64, step=1)

    def _check_values_indices_shape_match(
        values: np.ndarray, index: Index, columns: Index
    ) -> None:
        """
        Check that the shape implied by our axes matches the actual shape of the
        data.
        """
        if values.shape[1] != len(columns) or values.shape[0] != len(index):
            # Could let this raise in Block constructor, but we get a more
            #  helpful exception message this way.
            if values.shape[0] == 0:
                raise ValueError("Empty data passed with indices specified.")
    
            passed = values.shape
            implied = (len(index), len(columns))
>           raise ValueError(f"Shape of passed values is {passed}, indices imply {implied}")
E           ValueError: Shape of passed values is (55, 64), indices imply (110, 64)

/upb/users/r/renzhong/profiles/unix/cs/.conda/envs/pykeen/lib/python3.9/site-packages/pandas/core/internals/construction.py:420: ValueError
----------------------------- Captured stdout call -----------------------------
Start time:2023-07-05 19:49:16.349469
*** Read or Load Knowledge Graph  ***
*** Reading KGs/Nations/test.txt with Pandas ***
Reading with pandas.read_csv with sep ** s+ ** ...
Took 0.0015 seconds | Current Memory Usage  3331.4 in MB
*** Reading KGs/Nations/train.txt with Pandas ***
Reading with pandas.read_csv with sep ** s+ ** ...
Took 0.0018 seconds | Current Memory Usage  3331.4 in MB
Unrecognized data KGs/Nations/literals.txt
*** Reading KGs/Nations/valid.txt with Pandas ***
Reading with pandas.read_csv with sep ** s+ ** ...
Took 0.0011 seconds | Current Memory Usage  3331.4 in MB
[3.1 / 14] Add reciprocal triples to train, validation, and test sets, e.g. KG:= {(s,p,o)} union {(o,p_inverse,s)}
Done !


Concatenating data to obtain index...
Done !

Creating a mapping from entities to integer indexes...
Done !

Done ! 0.003 seconds

Done !

Done !

Took 0.0102 seconds | Current Memory Usage  3331.4 in MB
Data Type conversion...
Submit er-vocab, re-vocab, and ee-vocab via  ProcessPoolExecutor...
Preprocessing took: 0.096 seconds

------------------- Description of Dataset KGs/Nations -------------------
Number of entities:14
Number of relations:110
Number of triples on train set:3184
Number of triples on valid set:398
Number of triples on test set:402
Entity Index:0.00000 in GB
Relation Index:0.00000 in GB
Train set :0.00001 in GB

# of CPUs:64 | # of GPUs:1 | # of CPUs for dataloader:1
NVIDIA GeForce RTX 3090
------------------- Train -------------------
Initializing TorchTrainer CPU Trainer...	Took 0.0033 seconds | Current Memory Usage  3331.4 in MB
Initializing Model...	Initializing the selected model... False
Took 0.0154 seconds | Current Memory Usage  3331.7 in MB
MyLCWALitModule(
  (model): ComplExLiteral(
    (loss): BCEWithLogitsLoss()
    (interaction): ComplExInteraction()
    (entity_representations): ModuleList(
      (0): CombinedRepresentation(
        (base): ModuleList(
          (0): Embedding(
            (_embeddings): Embedding(14, 128)
          )
          (1): Embedding(
            (_embeddings): Embedding(14, 2)
          )
        )
        (combination): ComplexSeparatedCombination(
          (real_combination): ConcatProjectionCombination(
            (projection): Sequential(
              (0): Linear(in_features=66, out_features=64, bias=True)
              (1): Dropout(p=0.2, inplace=False)
              (2): Tanh()
            )
          )
          (imag_combination): ConcatProjectionCombination(
            (projection): Sequential(
              (0): Linear(in_features=66, out_features=64, bias=True)
              (1): Dropout(p=0.2, inplace=False)
              (2): Tanh()
            )
          )
        )
      )
    )
    (relation_representations): ModuleList(
      (0): Embedding(
        (_embeddings): Embedding(55, 128)
      )
    )
    (weight_regularizers): ModuleList()
  )
  (loss): BCEWithLogitsLoss()
)
  | Name  | Type              | Params
--------------------------------------------
0 | model | ComplExLiteral    | 17.4 K
1 | loss  | BCEWithLogitsLoss | 0     
--------------------------------------------
17.4 K    Trainable params
28        Non-trainable params
17.4 K    Total params
0.070     Total estimated model params size (MB)
Adam

Training is starting 2023-07-05 19:49:16.469388...
NumOfDataPoints:471 | NumOfEpochs:3 | LearningRate:0.01 | BatchSize:128 | EpochBatchsize:4
Epoch:1 | Batch:1 | Loss:0.8619691729545593 |ForwardBackwardUpdate:0.01secs | Mem. Usage  3333.1MB
Epoch:1 | Batch:2 | Loss:0.7337377071 |ForwardBackwardUpdate:0.01sec | BatchConst.:0.01sec | Mem. Usage  3333.1MB  avail. 16.4 %
Epoch:1 | Batch:3 | Loss:0.6596273780 |ForwardBackwardUpdate:0.01sec | BatchConst.:0.01sec | Mem. Usage  3333.1MB  avail. 16.4 %
Epoch:1 | Batch:4 | Loss:0.6063648462 |ForwardBackwardUpdate:0.01sec | BatchConst.:0.00sec | Mem. Usage  3333.1MB  avail. 16.4 %
Epoch:1 | Loss:0.71542478 | Runtime:0.002 mins
Epoch:2 | Batch:1 | Loss:0.5900872945785522 |ForwardBackwardUpdate:0.01secs | Mem. Usage  3333.1MB
Epoch:2 | Batch:2 | Loss:0.5630889535 |ForwardBackwardUpdate:0.01sec | BatchConst.:0.00sec | Mem. Usage  3333.1MB  avail. 16.4 %
Epoch:2 | Batch:3 | Loss:0.5985515118 |ForwardBackwardUpdate:0.01sec | BatchConst.:0.00sec | Mem. Usage  3333.1MB  avail. 16.4 %
Epoch:2 | Batch:4 | Loss:0.5702178478 |ForwardBackwardUpdate:0.01sec | BatchConst.:0.00sec | Mem. Usage  3333.1MB  avail. 16.4 %
Epoch:2 | Loss:0.58048640 | Runtime:0.001 mins
Epoch:3 | Batch:1 | Loss:0.5954322814941406 |ForwardBackwardUpdate:0.01secs | Mem. Usage  3333.1MB
Epoch:3 | Batch:2 | Loss:0.5773313046 |ForwardBackwardUpdate:0.01sec | BatchConst.:0.00sec | Mem. Usage  3333.1MB  avail. 16.4 %
Epoch:3 | Batch:3 | Loss:0.5221539736 |ForwardBackwardUpdate:0.01sec | BatchConst.:0.00sec | Mem. Usage  3333.1MB  avail. 16.4 %
Epoch:3 | Batch:4 | Loss:0.5812053084 |ForwardBackwardUpdate:0.01sec | BatchConst.:0.00sec | Mem. Usage  3333.1MB  avail. 16.4 %
Epoch:3 | Loss:0.56903072 | Runtime:0.001 mins
Done ! It took 0.211 seconds.

*** Save Trained Model ***
------------------------------ Captured log call -------------------------------
WARNING  pykeen.models.base:base.py:99 No random seed is specified. This may lead to non-reproducible results.
WARNING  pykeen.nn.combination:combination.py:58 No symbolic computation of output shape.
__________________________ test_model[Pykeen_CompGCN] __________________________

model_name = 'Pykeen_CompGCN'

    @pytest.mark.filterwarnings("ignore::UserWarning")
    @pytest.mark.parametrize(
        "model_name",
        [
            "Pykeen_DistMult",
            "Pykeen_TuckER",
            "Pykeen_UM",
            "Pykeen_TransR",
            "Pykeen_TransH",
            "Pykeen_TransF",
            "Pykeen_TransE",
            "Pykeen_TransD",
            "Pykeen_TorusE",
            "Pykeen_SimplE",
            "Pykeen_SE",
            "Pykeen_RESCAL",
            "Pykeen_RotatE",
            "Pykeen_QuatE",
            "Pykeen_PairRE",
            "Pykeen_ProjE",
            "Pykeen_NTN",
            "Pykeen_NodePiece",
            "Pykeen_MuRE",
            "Pykeen_KG2E",
            "Pykeen_InductiveNodePiece",
            "Pykeen_InductiveNodePieceGNN",
            "Pykeen_HolE",
            "Pykeen_FixedModel",
            "Pykeen_ERMLPE",
            "Pykeen_DistMA",
            "Pykeen_CrossE",
            "Pykeen_CooccurrenceFilteredModel",
            "Pykeen_ConvKB",  # this one is really slow
            "Pykeen_ConvE",
            "Pykeen_ComplExLiteral",
            "Pykeen_ComplEx",
            "Pykeen_CompGCN",
            "Pykeen_CP",
            "Pykeen_BoxE",
            "Pykeen_AutoSF",
            "Pykeen_DistMultLiteral",
        ],
    )
    def test_model(model_name):
        args = template(model_name)
        # config = {
        #     "epoch":args.num_epochs,"lr":args.lr,"embedding_dim":args.embedding_dim
        # }
        # dataset = args.path_dataset_folder.split('/')[1]
        # wandb.setup(wandb.Settings(program="test_pykeen_model.py", program_relpath="test_pykeen_model.py"))
        # wandb.init(project="dice_demo",config=config,name=f'{args.model}-{dataset}')
>       Execute(args).start()

tests/test_regression_pykeen.py:108: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
dicee/executer.py:212: in start
    self.trained_model, form_of_labelling = self.trainer.start(dataset=self.dataset)
dicee/trainer/dice_trainer.py:251: in start
    self.trainer.fit(model,train_dataloaders=model.train_dataloaders)
dicee/trainer/torch_trainer.py:144: in fit
    avg_epoch_loss = self._run_epoch(epoch)
dicee/trainer/torch_trainer.py:96: in _run_epoch
    batch_loss = self._run_batch(i, x_batch, y_batch,batch=batch)
dicee/trainer/torch_trainer.py:69: in _run_batch
    return self.compute_forward_loss_backward(x_batch,y_batch,batch=batch).item()
dicee/trainer/torch_trainer.py:192: in compute_forward_loss_backward
    batch_loss.backward()
/upb/users/r/renzhong/profiles/unix/cs/.conda/envs/pykeen/lib/python3.9/site-packages/torch/_tensor.py:487: in backward
    torch.autograd.backward(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor(1.9535, device='cuda:0', grad_fn=<AddBackward0>),)
grad_tensors = None, retain_graph = False, create_graph = False
grad_variables = None, inputs = ()

    def backward(
        tensors: _TensorOrTensors,
        grad_tensors: Optional[_TensorOrTensors] = None,
        retain_graph: Optional[bool] = None,
        create_graph: bool = False,
        grad_variables: Optional[_TensorOrTensors] = None,
        inputs: Optional[_TensorOrTensors] = None,
    ) -> None:
        r"""Computes the sum of gradients of given tensors with respect to graph
        leaves.
    
        The graph is differentiated using the chain rule. If any of ``tensors``
        are non-scalar (i.e. their data has more than one element) and require
        gradient, then the Jacobian-vector product would be computed, in this
        case the function additionally requires specifying ``grad_tensors``.
        It should be a sequence of matching length, that contains the "vector"
        in the Jacobian-vector product, usually the gradient of the differentiated
        function w.r.t. corresponding tensors (``None`` is an acceptable value for
        all tensors that don't need gradient tensors).
    
        This function accumulates gradients in the leaves - you might need to zero
        ``.grad`` attributes or set them to ``None`` before calling it.
        See :ref:`Default gradient layouts<default-grad-layouts>`
        for details on the memory layout of accumulated gradients.
    
        .. note::
            Using this method with ``create_graph=True`` will create a reference cycle
            between the parameter and its gradient which can cause a memory leak.
            We recommend using ``autograd.grad`` when creating the graph to avoid this.
            If you have to use this function, make sure to reset the ``.grad`` fields of your
            parameters to ``None`` after use to break the cycle and avoid the leak.
    
        .. note::
    
            If you run any forward ops, create ``grad_tensors``, and/or call ``backward``
            in a user-specified CUDA stream context, see
            :ref:`Stream semantics of backward passes<bwd-cuda-stream-semantics>`.
    
        .. note::
    
            When ``inputs`` are provided and a given input is not a leaf,
            the current implementation will call its grad_fn (even though it is not strictly needed to get this gradients).
            It is an implementation detail on which the user should not rely.
            See https://github.com/pytorch/pytorch/pull/60521#issuecomment-867061780 for more details.
    
        Args:
            tensors (Sequence[Tensor] or Tensor): Tensors of which the derivative will be
                computed.
            grad_tensors (Sequence[Tensor or None] or Tensor, optional): The "vector" in
                the Jacobian-vector product, usually gradients w.r.t. each element of
                corresponding tensors. None values can be specified for scalar Tensors or
                ones that don't require grad. If a None value would be acceptable for all
                grad_tensors, then this argument is optional.
            retain_graph (bool, optional): If ``False``, the graph used to compute the grad
                will be freed. Note that in nearly all cases setting this option to ``True``
                is not needed and often can be worked around in a much more efficient
                way. Defaults to the value of ``create_graph``.
            create_graph (bool, optional): If ``True``, graph of the derivative will
                be constructed, allowing to compute higher order derivative products.
                Defaults to ``False``.
            inputs (Sequence[Tensor] or Tensor, optional): Inputs w.r.t. which the gradient
                be will accumulated into ``.grad``. All other Tensors will be ignored. If
                not provided, the gradient is accumulated into all the leaf Tensors that
                were used to compute the attr::tensors.
        """
        if torch._C._are_functorch_transforms_active():
            raise RuntimeError(
                "backward() called inside a functorch transform. This is not "
                "supported, please use functorch.grad or functorch.vjp instead "
                "or call backward() outside of functorch transforms.")
    
        if grad_variables is not None:
            warnings.warn("'grad_variables' is deprecated. Use 'grad_tensors' instead.")
            if grad_tensors is None:
                grad_tensors = grad_variables
            else:
                raise RuntimeError("'grad_tensors' and 'grad_variables' (deprecated) "
                                   "arguments both passed to backward(). Please only "
                                   "use 'grad_tensors'.")
        if inputs is not None and len(inputs) == 0:
            raise RuntimeError("'inputs' argument to backward() cannot be empty.")
    
        tensors = (tensors,) if isinstance(tensors, torch.Tensor) else tuple(tensors)
        inputs = (inputs,) if isinstance(inputs, torch.Tensor) else \
            tuple(inputs) if inputs is not None else tuple()
    
        grad_tensors_ = _tensor_or_tensors_to_tuple(grad_tensors, len(tensors))
        grad_tensors_ = _make_grads(tensors, grad_tensors_, is_grads_batched=False)
        if retain_graph is None:
            retain_graph = create_graph
    
        # The reason we repeat same the comment below is that
        # some Python versions print out the first line of a multi-line function
        # calls in the traceback and some print out the last line
>       Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
            tensors, grad_tensors_, retain_graph, create_graph, inputs,
            allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass
E       RuntimeError: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.

/upb/users/r/renzhong/profiles/unix/cs/.conda/envs/pykeen/lib/python3.9/site-packages/torch/autograd/__init__.py:200: RuntimeError
----------------------------- Captured stdout call -----------------------------
Start time:2023-07-05 19:49:17.461480
*** Read or Load Knowledge Graph  ***
*** Reading KGs/Nations/test.txt with Pandas ***
Reading with pandas.read_csv with sep ** s+ ** ...
Took 0.0015 seconds | Current Memory Usage  3341.8 in MB
*** Reading KGs/Nations/train.txt with Pandas ***
Reading with pandas.read_csv with sep ** s+ ** ...
Took 0.0017 seconds | Current Memory Usage  3341.8 in MB
Unrecognized data KGs/Nations/literals.txt
*** Reading KGs/Nations/valid.txt with Pandas ***
Reading with pandas.read_csv with sep ** s+ ** ...
Took 0.0011 seconds | Current Memory Usage  3341.8 in MB
[3.1 / 14] Add reciprocal triples to train, validation, and test sets, e.g. KG:= {(s,p,o)} union {(o,p_inverse,s)}
Done !


Concatenating data to obtain index...
Done !

Creating a mapping from entities to integer indexes...
Done !

Done ! 0.003 seconds

Done !

Done !

Took 0.0103 seconds | Current Memory Usage  3341.8 in MB
Data Type conversion...
Submit er-vocab, re-vocab, and ee-vocab via  ProcessPoolExecutor...
Preprocessing took: 0.098 seconds

------------------- Description of Dataset KGs/Nations -------------------
Number of entities:14
Number of relations:110
Number of triples on train set:3184
Number of triples on valid set:398
Number of triples on test set:402
Entity Index:0.00000 in GB
Relation Index:0.00000 in GB
Train set :0.00001 in GB

# of CPUs:64 | # of GPUs:1 | # of CPUs for dataloader:1
NVIDIA GeForce RTX 3090
------------------- Train -------------------
Initializing TorchTrainer CPU Trainer...	Took 0.0011 seconds | Current Memory Usage  3341.8 in MB
Initializing Model...	Initializing the selected model... False
Took 0.0229 seconds | Current Memory Usage  3342.3 in MB
MyLCWALitModule(
  (model): CompGCN(
    (loss): BCEWithLogitsLoss()
    (interaction): DistMultInteraction()
    (entity_representations): ModuleList(
      (0): SingleCompGCNRepresentation(
        (combined): CombinedCompGCNRepresentations(
          (entity_representations): Embedding(
            (_embeddings): Embedding(14, 64)
          )
          (relation_representations): Embedding(
            (_embeddings): Embedding(220, 64)
          )
          (layers): ModuleList(
            (0): CompGCNLayer(
              (composition): MultiplicationCompositionModule()
              (edge_weighting): SymmetricEdgeWeighting()
              (w_rel): Linear(in_features=64, out_features=64, bias=False)
              (drop): Dropout(p=0.0, inplace=False)
              (bn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (bias): Bias()
              (activation): Identity()
            )
          )
        )
      )
    )
    (relation_representations): ModuleList(
      (0): SingleCompGCNRepresentation(
        (combined): CombinedCompGCNRepresentations(
          (entity_representations): Embedding(
            (_embeddings): Embedding(14, 64)
          )
          (relation_representations): Embedding(
            (_embeddings): Embedding(220, 64)
          )
          (layers): ModuleList(
            (0): CompGCNLayer(
              (composition): MultiplicationCompositionModule()
              (edge_weighting): SymmetricEdgeWeighting()
              (w_rel): Linear(in_features=64, out_features=64, bias=False)
              (drop): Dropout(p=0.0, inplace=False)
              (bn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (bias): Bias()
              (activation): Identity()
            )
          )
        )
      )
    )
    (weight_regularizers): ModuleList()
  )
  (loss): BCEWithLogitsLoss()
)
  | Name  | Type              | Params
--------------------------------------------
0 | model | CompGCN           | 31.6 K
1 | loss  | BCEWithLogitsLoss | 0     
--------------------------------------------
31.6 K    Trainable params
0         Non-trainable params
31.6 K    Total params
0.126     Total estimated model params size (MB)
Adam

Training is starting 2023-07-05 19:49:17.586523...
NumOfDataPoints:1818 | NumOfEpochs:3 | LearningRate:0.01 | BatchSize:128 | EpochBatchsize:15
Epoch:1 | Batch:1 | Loss:1.9319884777069092 |ForwardBackwardUpdate:0.01secs | Mem. Usage  3342.5MB
------------------------------ Captured log call -------------------------------
WARNING  pykeen.models.base:base.py:99 No random seed is specified. This may lead to non-reproducible results.
INFO     pykeen.triples.triples_factory:triples_factory.py:469 Creating inverse triples.
______________________ test_model[Pykeen_DistMultLiteral] ______________________

model_name = 'Pykeen_DistMultLiteral'

    @pytest.mark.filterwarnings("ignore::UserWarning")
    @pytest.mark.parametrize(
        "model_name",
        [
            "Pykeen_DistMult",
            "Pykeen_TuckER",
            "Pykeen_UM",
            "Pykeen_TransR",
            "Pykeen_TransH",
            "Pykeen_TransF",
            "Pykeen_TransE",
            "Pykeen_TransD",
            "Pykeen_TorusE",
            "Pykeen_SimplE",
            "Pykeen_SE",
            "Pykeen_RESCAL",
            "Pykeen_RotatE",
            "Pykeen_QuatE",
            "Pykeen_PairRE",
            "Pykeen_ProjE",
            "Pykeen_NTN",
            "Pykeen_NodePiece",
            "Pykeen_MuRE",
            "Pykeen_KG2E",
            "Pykeen_InductiveNodePiece",
            "Pykeen_InductiveNodePieceGNN",
            "Pykeen_HolE",
            "Pykeen_FixedModel",
            "Pykeen_ERMLPE",
            "Pykeen_DistMA",
            "Pykeen_CrossE",
            "Pykeen_CooccurrenceFilteredModel",
            "Pykeen_ConvKB",  # this one is really slow
            "Pykeen_ConvE",
            "Pykeen_ComplExLiteral",
            "Pykeen_ComplEx",
            "Pykeen_CompGCN",
            "Pykeen_CP",
            "Pykeen_BoxE",
            "Pykeen_AutoSF",
            "Pykeen_DistMultLiteral",
        ],
    )
    def test_model(model_name):
        args = template(model_name)
        # config = {
        #     "epoch":args.num_epochs,"lr":args.lr,"embedding_dim":args.embedding_dim
        # }
        # dataset = args.path_dataset_folder.split('/')[1]
        # wandb.setup(wandb.Settings(program="test_pykeen_model.py", program_relpath="test_pykeen_model.py"))
        # wandb.init(project="dice_demo",config=config,name=f'{args.model}-{dataset}')
>       Execute(args).start()

tests/test_regression_pykeen.py:108: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
dicee/executer.py:215: in start
    return self.end(start_time, form_of_labelling)
dicee/executer.py:152: in end
    self.save_trained_model()
dicee/static_funcs.py:50: in timeit_wrapper
    result = func(*args, **kwargs)
dicee/executer.py:119: in save_trained_model
    store(trainer=self.trainer,
dicee/static_funcs.py:318: in store
    save_embeddings(
dicee/static_funcs.py:615: in save_embeddings
    df = pd.DataFrame(embeddings, index=_indexes)
/upb/users/r/renzhong/profiles/unix/cs/.conda/envs/pykeen/lib/python3.9/site-packages/pandas/core/frame.py:720: in __init__
    mgr = ndarray_to_mgr(
/upb/users/r/renzhong/profiles/unix/cs/.conda/envs/pykeen/lib/python3.9/site-packages/pandas/core/internals/construction.py:349: in ndarray_to_mgr
    _check_values_indices_shape_match(values, index, columns)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

values = array([[ 0.00151454,  0.06112412, -0.020248  , ..., -0.07078241,
         0.02639196,  0.04297231],
       [-0.1444369...6],
       [-0.13835932,  0.0693896 , -0.04588016, ..., -0.0110907 ,
         0.10395623, -0.03839252]], dtype=float32)
index = Index(['militaryalliance', 'intergovorgs3', 'relbooktranslations',
       'timesincewar', 'negativebehavior', 'relinte...erse', 'warning_inverse', 'lostterritory_inverse',
       'severdiplomatic_inverse'],
      dtype='object', length=110)
columns = RangeIndex(start=0, stop=64, step=1)

    def _check_values_indices_shape_match(
        values: np.ndarray, index: Index, columns: Index
    ) -> None:
        """
        Check that the shape implied by our axes matches the actual shape of the
        data.
        """
        if values.shape[1] != len(columns) or values.shape[0] != len(index):
            # Could let this raise in Block constructor, but we get a more
            #  helpful exception message this way.
            if values.shape[0] == 0:
                raise ValueError("Empty data passed with indices specified.")
    
            passed = values.shape
            implied = (len(index), len(columns))
>           raise ValueError(f"Shape of passed values is {passed}, indices imply {implied}")
E           ValueError: Shape of passed values is (55, 64), indices imply (110, 64)

/upb/users/r/renzhong/profiles/unix/cs/.conda/envs/pykeen/lib/python3.9/site-packages/pandas/core/internals/construction.py:420: ValueError
----------------------------- Captured stdout call -----------------------------
Start time:2023-07-05 19:49:30.195400
*** Read or Load Knowledge Graph  ***
*** Reading KGs/Nations/test.txt with Pandas ***
Reading with pandas.read_csv with sep ** s+ ** ...
Took 0.0015 seconds | Current Memory Usage  3383.0 in MB
*** Reading KGs/Nations/train.txt with Pandas ***
Reading with pandas.read_csv with sep ** s+ ** ...
Took 0.0016 seconds | Current Memory Usage  3383.0 in MB
Unrecognized data KGs/Nations/literals.txt
*** Reading KGs/Nations/valid.txt with Pandas ***
Reading with pandas.read_csv with sep ** s+ ** ...
Took 0.0012 seconds | Current Memory Usage  3383.0 in MB
[3.1 / 14] Add reciprocal triples to train, validation, and test sets, e.g. KG:= {(s,p,o)} union {(o,p_inverse,s)}
Done !


Concatenating data to obtain index...
Done !

Creating a mapping from entities to integer indexes...
Done !

Done ! 0.003 seconds

Done !

Done !

Took 0.0101 seconds | Current Memory Usage  3383.0 in MB
Data Type conversion...
Submit er-vocab, re-vocab, and ee-vocab via  ProcessPoolExecutor...
Preprocessing took: 0.101 seconds

------------------- Description of Dataset KGs/Nations -------------------
Number of entities:14
Number of relations:110
Number of triples on train set:3184
Number of triples on valid set:398
Number of triples on test set:402
Entity Index:0.00000 in GB
Relation Index:0.00000 in GB
Train set :0.00001 in GB

# of CPUs:64 | # of GPUs:1 | # of CPUs for dataloader:1
NVIDIA GeForce RTX 3090
------------------- Train -------------------
Initializing TorchTrainer CPU Trainer...	Took 0.0006 seconds | Current Memory Usage  3383.0 in MB
Initializing Model...	Initializing the selected model... False
Took 0.0190 seconds | Current Memory Usage  3383.0 in MB
MyLCWALitModule(
  (model): DistMultLiteral(
    (loss): BCEWithLogitsLoss()
    (interaction): DistMultInteraction()
    (entity_representations): ModuleList(
      (0): CombinedRepresentation(
        (base): ModuleList(
          (0): Embedding(
            (_embeddings): Embedding(14, 64)
          )
          (1): Embedding(
            (_embeddings): Embedding(14, 2)
          )
        )
        (combination): ConcatProjectionCombination(
          (projection): Sequential(
            (0): Linear(in_features=66, out_features=64, bias=True)
            (1): Dropout(p=0.0, inplace=False)
            (2): Identity()
          )
        )
      )
    )
    (relation_representations): ModuleList(
      (0): Embedding(
        (_embeddings): Embedding(55, 64)
      )
    )
    (weight_regularizers): ModuleList()
  )
  (loss): BCEWithLogitsLoss()
)
  | Name  | Type              | Params
--------------------------------------------
0 | model | DistMultLiteral   | 8.7 K 
1 | loss  | BCEWithLogitsLoss | 0     
--------------------------------------------
8.7 K     Trainable params
28        Non-trainable params
8.7 K     Total params
0.035     Total estimated model params size (MB)
Adam

Training is starting 2023-07-05 19:49:30.318836...
NumOfDataPoints:471 | NumOfEpochs:3 | LearningRate:0.01 | BatchSize:128 | EpochBatchsize:4
Epoch:1 | Batch:1 | Loss:201001147564032.0 |ForwardBackwardUpdate:0.01secs | Mem. Usage  3383.0MB
Epoch:1 | Batch:2 | Loss:118245877088256.0000000000 |ForwardBackwardUpdate:0.00sec | BatchConst.:0.01sec | Mem. Usage  3383.0MB  avail. 16.4 %
Epoch:1 | Batch:3 | Loss:111517257170944.0000000000 |ForwardBackwardUpdate:0.00sec | BatchConst.:0.00sec | Mem. Usage  3383.0MB  avail. 16.4 %
Epoch:1 | Batch:4 | Loss:74421557526528.0000000000 |ForwardBackwardUpdate:0.00sec | BatchConst.:0.00sec | Mem. Usage  3383.0MB  avail. 16.4 %
Epoch:1 | Loss:126296459837440.00000000 | Runtime:0.002 mins
Epoch:2 | Batch:1 | Loss:54332250652672.0 |ForwardBackwardUpdate:0.00secs | Mem. Usage  3383.0MB
Epoch:2 | Batch:2 | Loss:63677361291264.0000000000 |ForwardBackwardUpdate:0.00sec | BatchConst.:0.01sec | Mem. Usage  3383.0MB  avail. 16.4 %
Epoch:2 | Batch:3 | Loss:75016561491968.0000000000 |ForwardBackwardUpdate:0.00sec | BatchConst.:0.00sec | Mem. Usage  3383.0MB  avail. 16.4 %
Epoch:2 | Batch:4 | Loss:48878678179840.0000000000 |ForwardBackwardUpdate:0.00sec | BatchConst.:0.00sec | Mem. Usage  3383.0MB  avail. 16.4 %
Epoch:2 | Loss:60476212903936.00000000 | Runtime:0.001 mins
Epoch:3 | Batch:1 | Loss:34806198960128.0 |ForwardBackwardUpdate:0.00secs | Mem. Usage  3383.0MB
Epoch:3 | Batch:2 | Loss:41692497444864.0000000000 |ForwardBackwardUpdate:0.00sec | BatchConst.:0.01sec | Mem. Usage  3383.0MB  avail. 16.4 %
Epoch:3 | Batch:3 | Loss:49202562334720.0000000000 |ForwardBackwardUpdate:0.00sec | BatchConst.:0.00sec | Mem. Usage  3383.0MB  avail. 16.4 %
Epoch:3 | Batch:4 | Loss:41335641866240.0000000000 |ForwardBackwardUpdate:0.00sec | BatchConst.:0.00sec | Mem. Usage  3383.0MB  avail. 16.4 %
Epoch:3 | Loss:41759225151488.00000000 | Runtime:0.001 mins
Done ! It took 0.196 seconds.

*** Save Trained Model ***
------------------------------ Captured log call -------------------------------
WARNING  pykeen.models.base:base.py:99 No random seed is specified. This may lead to non-reproducible results.
WARNING  pykeen.nn.combination:combination.py:58 No symbolic computation of output shape.
=============================== warnings summary ===============================
../../../../../../../../../../upb/users/r/renzhong/profiles/unix/cs/.conda/envs/pykeen/lib/python3.9/site-packages/torch/utils/tensorboard/__init__.py:4
  /upb/users/r/renzhong/profiles/unix/cs/.conda/envs/pykeen/lib/python3.9/site-packages/torch/utils/tensorboard/__init__.py:4: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
    if not hasattr(tensorboard, "__version__") or LooseVersion(

../../../../../../../../../../upb/users/r/renzhong/profiles/unix/cs/.conda/envs/pykeen/lib/python3.9/site-packages/torch/utils/tensorboard/__init__.py:6
  /upb/users/r/renzhong/profiles/unix/cs/.conda/envs/pykeen/lib/python3.9/site-packages/torch/utils/tensorboard/__init__.py:6: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
    ) < LooseVersion("1.15"):

../../../../../../../../../../upb/users/r/renzhong/profiles/unix/cs/.local/lib/python3.9/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:326
  /upb/users/r/renzhong/profiles/unix/cs/.local/lib/python3.9/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:326: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)
    np.bool8: (False, True),

tests/test_regression_pykeen.py::test_model[Pykeen_DistMult]
  /upb/users/r/renzhong/profiles/unix/cs/.conda/envs/pykeen/lib/python3.9/site-packages/torch/distributed/_sharded_tensor/__init__.py:8: DeprecationWarning: torch.distributed._sharded_tensor will be deprecated, use torch.distributed._shard.sharded_tensor instead
    warnings.warn(

tests/test_regression_pykeen.py: 39 warnings
  /local/upb/users/r/renzhong/profiles/unix/cs/Dicee/dice-embeddings/dicee/abstracts.py:53: LightningDeprecationWarning: The `LightningModule.summarize` method is deprecated in v1.5 and will be removed in v1.7. Use `pytorch_lightning.utilities.model_summary.summarize` instead.
    c.on_fit_start(*args, **kwargs)

-- Docs: https://docs.pytest.org/en/stable/warnings.html
=========================== short test summary info ============================
FAILED tests/test_regression_pykeen.py::test_model[Pykeen_InductiveNodePiece]
FAILED tests/test_regression_pykeen.py::test_model[Pykeen_InductiveNodePieceGNN]
FAILED tests/test_regression_pykeen.py::test_model[Pykeen_ComplExLiteral] - V...
FAILED tests/test_regression_pykeen.py::test_model[Pykeen_CompGCN] - RuntimeE...
FAILED tests/test_regression_pykeen.py::test_model[Pykeen_DistMultLiteral] - ...
================== 5 failed, 36 passed, 43 warnings in 43.59s ==================
